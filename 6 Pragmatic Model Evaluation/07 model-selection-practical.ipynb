{"metadata": {"kernelspec": {"display_name": "learn", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 4, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Model Selection Practical Notebook\n", "\n", "In this notebook you will demonstrate what you have learnt in the lesson and tackle the following challenges:\n", "\n", "1. Write your own \"Randomised Selection\" code to select the optimal configuration for your linear model.\n", "2. Modify the `SimpleNeuralNetwork()` code in the lesson notebook such that it runs a classificaiton model."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": [], "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import seaborn as sns\n", "import matplotlib.pylab as plt\n", "\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.datasets import load_iris, load_diabetes, load_breast_cancer\n", "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n", "\n", "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": [], "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Preparing the data\n", "\n", "diabetes = load_diabetes()\n", "X = diabetes['data']\n", "y = diabetes['target']\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q1.** Implement your own Randomised Selection function.\n", "\n", "Recall that randomised selection is a method to determine the optimal configuration for a linear regression model.\n", "\n", "_Hint: you would need a sampling function that gives you potential feature candidates, consider using `np.random.choice`. Remember also that AIC and BIC should only be calculated on the training sample, where you may also wish to calculate MSE on the test sample_"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": [], "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# AIC and BIC function\n", "\n", "def AIC(X, y, lm):\n", "    \"\"\" Compute the AIC score of a linear model\"\"\"\n", "    N = X.shape[0]\n", "    y_pred = lm.predict(X)\n", "    ll = (N/2)*np.log(mean_squared_error(y_pred, y))\n", "    k = len(lm.coef_)\n", "    \n", "    return 2*k + 2*ll\n", "\n", "def BIC(X, y, lm):\n", "    \"\"\" Compute the BIC score of a linear model\"\"\"\n", "    N = X.shape[0]\n", "    y_pred = lm.predict(X)\n", "    ll = (N/2)*np.log(mean_squared_error(y_pred, y))\n", "    k = len(lm.coef_)\n", "    \n", "    return k*np.log(N) + 2*ll"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": [], "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here ...\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q2.** Modify the regression code in `SimpleNeuralNetwork` into a classification model.\n", "\n", "_Hint: Modify the `model` object defined in the `.fit()` method and also change the loss function from mean squared loss to binary cross entropy loss._"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": [], "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Prepare binary classfication data\n", "\n", "cancer = load_breast_cancer()\n", "X, y = cancer['data'], cancer['target'].reshape(-1, 1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": [], "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import torch\n", "\n", "# Regression code to modify\n", "class SimpleNeuralNetwork():\n", "\n", "    def __init__(self, h1, epoch, verbose=False):\n", "        \"\"\"\n", "        args:\n", "            h1: number of nodes in hidden layer 1\n", "            epoch: number of gradient updates\n", "        \"\"\"\n", "        self.epoch = epoch\n", "        self.h1 = h1\n", "        self.verbose = verbose\n", "\n", "    def fit(self, X, y):\n", "        \"\"\"\n", "        args:\n", "            X, y: predictor and target\n", "        \"\"\"\n", "        n, p = X.shape\n", "        inputs = torch.from_numpy(X).float()\n", "        targets = torch.from_numpy(y).float()\n", "\n", "        # Create the neural network model\n", "        model = torch.nn.Sequential(torch.nn.Linear(p, self.h1),\n", "                                    torch.nn.ReLU(),\n", "                                    torch.nn.Linear(self.h1, 1))\n", "        loss_fn = torch.nn.MSELoss()\n", "        opt = torch.optim.SGD(model.parameters(), lr=1e-3)\n", "\n", "        for rd in range(self.epoch):\n", "            pred = model(inputs)\n", "            loss = loss_fn(pred, targets)\n", "\n", "            if rd%300 == 0:\n", "                print(\"loss: %.2f\" %loss)\n", "\n", "            loss.backward()\n", "            opt.step()\n", "            opt.zero_grad()\n", "\n", "        self.model = model\n", "\n", "    def predict(self, x_test):\n", "        inputs = torch.from_numpy(x_test).float()\n", "        return self.model(inputs).data.numpy()\n", "\n", "\n"]}]}