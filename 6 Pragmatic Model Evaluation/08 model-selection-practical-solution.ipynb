{"metadata": {"kernelspec": {"display_name": "learn", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 4, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Model Selection Practical Notebook\n", "\n", "In this notebook you will demonstrate what you have learnt in the lesson and tackle the following challenges:\n", "\n", "1. Write your own \"Randomised Selection\" code to select the optimal configuration for your linear model.\n", "2. Modify the `SimpleNeuralNetwork()` code in the lesson notebook such that it runs a classificaiton model."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": [], "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import seaborn as sns\n", "import matplotlib.pylab as plt\n", "\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.datasets import load_iris, load_diabetes, load_breast_cancer\n", "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n", "\n", "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": [], "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Preparing the data\n", "\n", "diabetes = load_diabetes()\n", "X = diabetes['data']\n", "y = diabetes['target']\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q1.** Implement your own Randomised Selection function.\n", "\n", "Recall that randomised selection is a method to determine the optimal configuration for a linear regression model.\n", "\n", "_Hint: you would need a sampling function that gives you potential feature candidates, consider using `np.random.choice`. Remember also that AIC and BIC should only be calculated on the training sample, where you may also wish to calculate MSE on the test sample_"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": [], "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# AIC and BIC function\n", "\n", "def AIC(X, y, lm):\n", "    \"\"\" Compute the AIC score of a linear model\"\"\"\n", "    N = X.shape[0]\n", "    y_pred = lm.predict(X)\n", "    ll = (N/2)*np.log(mean_squared_error(y_pred, y))\n", "    k = len(lm.coef_)\n", "    \n", "    return 2*k + 2*ll\n", "\n", "def BIC(X, y, lm):\n", "    \"\"\" Compute the BIC score of a linear model\"\"\"\n", "    N = X.shape[0]\n", "    y_pred = lm.predict(X)\n", "    ll = (N/2)*np.log(mean_squared_error(y_pred, y))\n", "    k = len(lm.coef_)\n", "    \n", "    return k*np.log(N) + 2*ll"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": [], "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here ...\n", "\n", "def sample_candidates(num_samples, p):\n", "    \"\"\"\n", "    args:\n", "        num_samples: number of samples to sample, if None, returns all possible combinations.\n", "        p: number of features\n", "    \"\"\"\n", "    if num_samples: \n", "        return np.random.choice([0, 1], size=(num_samples, p)).astype(bool)\n", "    else:\n", "        samples = []\n", "        for i in range(1,2**p):\n", "            samples.append(np.array(list(np.binary_repr(i, width=p)), dtype=int))\n", "        return np.array(samples).astype(bool)\n", "\n", "\n", "def return_metric(X_train, y_train, X_test, y_test, candidate, metric=\"aic\"):\n", "    \"\"\"\n", "    args:\n", "        X_train, y_train, X_test, y_test: The standard train-validation test split of your training data.\n", "        candidate: Candidate set of features for testing - can take either an array of unique column indexes or boolean array as to whether corresponding column should be included\n", "        metric: the metric used to access the goodness of fit. \"aic\", \"bic\" or \"mse\". Note that AIC and BIC are calculated on the training data, whereas MSE is calculated on the test data.\n", "    \n", "    \"\"\"\n", "    \n", "    X_train_sub = X_train[:, candidate]\n", "    X_test_sub = X_test[:, candidate]\n", "    lm = LinearRegression()\n", "    lm.fit(X_train_sub, y_train)\n", "    \n", "    if metric == \"aic\":\n", "        return AIC(X_train_sub, y_train, lm)\n", "    elif metric == \"bic\":\n", "        return BIC(X_train_sub, y_train, lm)\n", "    elif metric == \"mse\":\n", "        return mean_squared_error(y_test, lm.predict(X_test_sub))\n", "\n", "\n", "def random_selection(X_train, y_train, X_test, y_test, metric, num_samples=None):\n", "    \"\"\"\n", "    args:\n", "        X_train, y_train, X_test, y_test: The standard train-validation test split of your training data.\n", "        num_samples: number of samples for the `sample_candidate` function\n", "        metric: the metric used to access the goodness of fit. \"aic\" or \"bic\"\n", "    \n", "    \"\"\"\n", "    samples = sample_candidates(num_samples, X_train.shape[1])\n", "    metric_ls = []\n", "    for candidate in samples:\n", "        metric_ls.append(return_metric(X_train, y_train, X_test, y_test, candidate, metric=metric))\n", "    \n", "    return samples, metric_ls\n", "\n", "# Demo\n", "np.random.seed(42)\n", "for metric in ['aic', 'bic', 'mse']:\n", "    samples_, metric_ls_ = random_selection(X_train, y_train, X_test, y_test, metric, 30)\n", "    best_sample = samples_[np.argmin(metric_ls_)]\n", "    print(f\"The optimal feature configuration according to the {metric.upper()} is: {best_sample.astype(int)}\")\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q2.** Modify the regression code in `SimpleNeuralNetwork` into a classification model.\n", "\n", "_Hint: Modify the `model` object defined in the `.fit()` method and also change the loss function from mean squared loss to binary cross entropy loss._"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": [], "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Prepare binary classfication data\n", "\n", "cancer = load_breast_cancer()\n", "X, y = cancer['data'], cancer['target'].reshape(-1, 1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": [], "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import torch\n", "\n", "# Regression code to modify\n", "class SimpleNeuralNetwork():\n", "\n", "    def __init__(self, h1, epoch, verbose=False):\n", "        \"\"\"\n", "        args:\n", "            h1: number of nodes in hidden layer 1\n", "            epoch: number of gradient updates\n", "        \"\"\"\n", "        self.epoch = epoch\n", "        self.h1 = h1\n", "        self.verbose = verbose\n", "\n", "    def fit(self, X, y):\n", "        \"\"\"\n", "        args:\n", "            X, y: predictor and target\n", "        \"\"\"\n", "        n, p = X.shape\n", "        inputs = torch.from_numpy(X).float()\n", "        targets = torch.from_numpy(y).float()\n", "\n", "        # Create the neural network model\n", "        model = torch.nn.Sequential(torch.nn.Linear(p, self.h1),\n", "                                    torch.nn.ReLU(),\n", "                                    torch.nn.Linear(self.h1, 1))\n", "        loss_fn = torch.nn.MSELoss()\n", "        opt = torch.optim.SGD(model.parameters(), lr=1e-3)\n", "\n", "        for rd in range(self.epoch):\n", "            pred = model(inputs)\n", "            loss = loss_fn(pred, targets)\n", "\n", "            if rd%300 == 0:\n", "                print(\"loss: %.2f\" %loss)\n", "\n", "            loss.backward()\n", "            opt.step()\n", "            opt.zero_grad()\n", "\n", "        self.model = model\n", "\n", "    def predict(self, x_test):\n", "        inputs = torch.from_numpy(x_test).float()\n", "        return self.model(inputs).data.numpy()\n", "\n", "\n", "\n", "class SimpleNeuralNetwork_Classification():\n", "    \n", "    def __init__(self, h1, epoch, verbose=False):\n", "        self.epoch = epoch\n", "        self.h1 = h1\n", "        self.verbose = verbose\n", "    \n", "    def fit(self, X, y):\n", "        n, p = X.shape\n", "        inputs = torch.from_numpy(X).float()\n", "        targets = torch.from_numpy(y).float()\n", "        \n", "        # Create the neural network model\n", "        model = torch.nn.Sequential(torch.nn.Linear(p, self.h1),\n", "                                    torch.nn.ReLU(),\n", "                                    torch.nn.Linear(self.h1, 1),\n", "                                    torch.nn.Sigmoid())\n", "\n", "        loss_fn = torch.nn.BCELoss()\n", "        opt = torch.optim.SGD(model.parameters(), lr=1e-4)\n", "        \n", "        for rd in range(self.epoch):\n", "            pred = model(inputs)\n", "            loss = loss_fn(pred, targets)\n", "                        \n", "            if rd%300 == 0:\n", "                print(\"loss: %.2f\" %loss)\n", "            \n", "            loss.backward()\n", "            opt.step()\n", "            opt.zero_grad()\n", "        \n", "        self.model = model\n", "    \n", "    def predict(self, x_test):\n", "        inputs = torch.from_numpy(x_test).float()\n", "        return self.model(inputs).data.numpy()\n", "\n", "# Test it\n", "snnc = SimpleNeuralNetwork_Classification(10, 1000)\n", "snnc.fit(X, y)\n", "\n", "print(\"Accuracy: %.2f\" %accuracy_score((snnc.predict(X) > 0.5), y))\n", "\n"]}]}