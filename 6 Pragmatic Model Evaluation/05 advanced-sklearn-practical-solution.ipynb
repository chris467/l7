{"cells":[{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["# Modular pipelines with scikit-learn\n","\n","In this notebook you will practice how to make your analyses modular, using `sklearn` tools. You will see how this structure can help build more sophisticated setups.\n","\n","We will work on a classification task, using a credit risk dataset from Taiwan, with the goal of predicting the risk of credit default.\n","\n","[Dataset reference](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)."]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Hide warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import pandas as pd\n","import numpy as np\n","import pickle\n","\n","from sklearn.compose import ColumnTransformer, make_column_transformer\n","from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, FunctionTransformer, PolynomialFeatures\n","from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n","from sklearn.model_selection import train_test_split, GridSearchCV, ShuffleSplit\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import cross_val_score\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# For custom transformers\n","from sklearn.base import TransformerMixin, BaseEstimator"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Load the dataset from `data/default.xls`:"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["data = pd.read_excel(\"data/default.xls\", skiprows=1)\n","data.head()"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Basic pipeline"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### Create data matrices\n","\n","- Create a feature matrix `X` and a target vector `y` from `data`. \n","    * The target to be predicted is given in the last column. \n","    * The feature matrix should consist of the rest of the columns (except for `'ID'`).\n","- Create train/test splits with `train_test_split()`, specifying that the data should be stratified by `y`."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","X = data.iloc[:,1:-1]\n","y = data.iloc[:,-1]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Lets have a look at the unique values in the target to check whether the dataset is imbalanced:"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["print(\"Unique values in y: {}\".format(np.unique(y)))\n","print(\"Number of 1s in y: {}/{}\".format(sum(y), len(y)))"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["X_train.shape, X_test.shape"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### Fit model and classify\n","Let's use `.fit()` and find the score of a Decision Tree classifier:"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["logreg = DecisionTreeClassifier()\n","logreg.fit(X_train, y_train)\n","\n","print(\"Score: \", logreg.score(X_test, y_test))\n","\n","y_pred = logreg.predict(X_test)\n","\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### Scikit-learn pipeline"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Now use the pipeline function of scikit-learn to arrive at the same results in three lines of code. Create a simple Decision Tree pipeline with `make_pipeline()` and test its predictions with `cross_val_score()`. Use a single split from now on, by setting `cv` to `ShuffleSplit(n_splits=1)`."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","pipeline = make_pipeline(DecisionTreeClassifier())\n","split = ShuffleSplit(n_splits=1)\n","cross_val_score(pipeline, X, y, cv=split)\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Preprocessing pipeline"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Our initial analysis overlooked several aspects that may hinder the performance of a predictive model. One is that the features should be transformed to an appropriate input type for a linear model. Here are the features we want to build:\n","* `SEX`, `EDUCATION`, `MARRIAGE`: One-hot encoded features for categorical columns.\n","* `PAY_...`, `BILL_AMT...`, `PAY_AMT...`: these features correspond to bills and payments made in the previous months. They're highly correlated so we will first scale them and then apply PCA.\n","* `AGE`, `LIMIT_BAL`: these are numerical values. We will use `FeatureUnion` to create log and polynomial versions of them.\n","* `Unpaid_by_user` / `avg_unpaid`: the ratio amount unpaid by the user divided by the average unpaid bill last month for all users. This should give us an idea of how bad at paying bills the user is.\n","\n","We'll go through each of them step-by-step, leveraging scikit-learn transformers.\n","\n","#### Column one-hot transformation\n","\n","We will create one-hot encoded features for the categorical columns `SEX`, `EDUCATION`, `MARRIAGE`. The standard process would be to create the new columns and add them to a new data matrix, and for each such preprocessing step we would have to create a new data set variation. Instead, we can create a transformation to be used in a pipeline.\n","\n","Create a one-hot transformer for the categorical variables, using `make_column_transformer()` and the `OneHotEncoder()`. The `ColumnTransformer` allows you to apply different transformations in parallel to different columns. Remember to set `remainder='passthrough'` to retain all other columns. \n","\n","Create a pipeline with the one-hot preprocessor and a Decision Tree. Test the model predictions with `cross_val_score()`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","cat_cols = [\"SEX\", \"EDUCATION\", \"MARRIAGE\"]\n","\n","one_hot_processor = make_column_transformer(\n","    (OneHotEncoder(sparse=False, handle_unknown='ignore'), cat_cols),\n","    remainder='passthrough')\n","# handle_unknown is for when there is something in the test set that's not in the training set, when you're using ohe\n","# the value hasn't been seen before, it ignores it rather than erroring out -- so 0,0,0,0\n","pipeline = make_pipeline(one_hot_processor, DecisionTreeClassifier())\n","cross_val_score(pipeline, X, y, cv=split)\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### Reducing correlated features with Scaler and PCA"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["We will scale the payment columns and reduce their dimensionality with PCA.\n","\n","Create a sequential transformer that first scales the data and then applies PCA.\n","\n","Choose a sensible value for `n_components`, which sets the number of components to keep (note that `n_components` can be specified as a `float` saying what proportion of variance is retained)."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","payment_transformer = Pipeline([\n","    (\"scaler\", MinMaxScaler()),\n","    (\"pca\", PCA(n_components=.8))\n","])\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Great, let's keep this transformer for later when we'll group all the pieces together."]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### Adding handcrafted features with `FeatureUnion`\n","\n","`FeatureUnion` can be applied on the data to add features (for example: interaction features or transformed features). In this dataset we want to create new features by combining and transforming `AGE` and `LIMIT_BAL`.\n","\n","Create a transformer that adds the log features with `FunctionTransformer()`, and a polynomial pairwise transformation (check the `PolynomialFeatures` documentation). For the latter transformer, specify `include_bias=False` and `interaction_only=True`.\n","\n","Test the transformation by calling `.fit_transform()` on `X_train`, columns `AGE` and `LIMIT_BAL`. How many new columns does this transformation create?"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","numerical_transformer = FeatureUnion([\n","    ('log', FunctionTransformer(np.log)),\n","    ('polynomial', PolynomialFeatures(2, include_bias=False, \n","                                      interaction_only=True))\n","])\n","\n","numerical_transformer.fit_transform(X_train[[\"AGE\", \"LIMIT_BAL\"]]).shape\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Note that if you have two features `a` and `b`, then `PolynomialFeatures(2, include_bias=False, interaction_only=True)` includes features `a`, `b` and `a*b`. If you remove `interaction_only`, it would include `a`, `b`, `a*b`, `a**2`, `b**2`. "]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### Custom Transformer\n","\n","We'll create a custom transformer that indicates if a user is above or below average for the unpaid bill last month: `unpaid_by_user > avg_unpaid`.\n","\n","For this we need to memorise the average amount unpaid by all users in the `.fit()` step, and then apply the transformation for any user in the `.transform()` step.\n","\n","We've defined the template for the class you need to implement below. Note that it extends `TransformerMixin` and `BaseEstimator`; that's required to define a new `Transformer` in the right format for `sklearn`.\n","\n","Follow the instructions below. You can run the code in the next cell to test your implementation.\n","\n","1. Implement the `.fit()` method:\n","  * This method takes `X` and `y` (`y` is optional). \n","  * It needs to compute `avg_unpaid_6`, `avg_unpaid_5`, and `avg_unpaid_4` - the amount unpaid on average for all users in the given training set for month 4, 5, 6. For example `avg_unpaid_6` is the mean of `BILL_AMT6` - `PAY_AMT6`.\n","  * To be compatible with other `sklearn` tools, `.fit()` needs to return `self`.\n","\n","\n","2. Implement the `.transform()` method:\n","  * This method takes `X` as input and needs to return a new DataFrame with columns `Unpaid_ratio_6`, `Unpaid_ratio_5`, and `Unpaid_ratio_4`, containing the same number of rows as `X`.\n","  * Each value indicates if the amount unpaid by user (`BILL_AMT` - `PAY_AMT`) is higher than the average amount saved in `.fit()`."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["class UnpaidTransformer(TransformerMixin, BaseEstimator):\n","    \n","    def fit(self, X, y=None):\n","        # Your code here...\n","        self.avg_unpaid_6 = (X[\"BILL_AMT6\"] - X[\"PAY_AMT6\"]).mean()\n","        self.avg_unpaid_5 = (X[\"BILL_AMT5\"] - X[\"PAY_AMT5\"]).mean()\n","        self.avg_unpaid_4 = (X[\"BILL_AMT4\"] - X[\"PAY_AMT4\"]).mean()\n","        \n","        return self\n","        pass\n","\n","    def transform(self, X, y=None):\n","        # Your code here...\n","        X_6 = (X[\"BILL_AMT6\"] - X[\"PAY_AMT6\"]) > self.avg_unpaid_6\n","        X_5 = (X[\"BILL_AMT5\"] - X[\"PAY_AMT5\"]) > self.avg_unpaid_5\n","        X_4 = (X[\"BILL_AMT4\"] - X[\"PAY_AMT4\"]) > self.avg_unpaid_4\n","\n","        X_new = pd.DataFrame({\"Unpaid_ratio_6\": X_6, \"Unpaid_ratio_5\": X_5, \"Unpaid_ratio_4\": X_4})\n","        return X_new\n","        pass"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Instantiate the transformer\n","unpaid_transfomer = UnpaidTransformer()\n","\n","# Test .fit()\n","unpaid_transfomer.fit(X_train)\n","print(unpaid_transfomer.avg_unpaid_6)\n","\n","# Test .transform()\n","unpaid_transfomer.transform(X_train).head()"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### Build a joint preprocessor"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Now that we have all the transformers we need, we can create a single object using `ColumnTransformer`. First we need to define a list of columns that will be processed separately:"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["payment_cols = [col for col in X_train.columns if col.startswith(\"PAY_\") or col.startswith(\"BILL_\")]\n","\n","num_cols = [\"AGE\", \"LIMIT_BAL\"]\n","\n","# Features needed to compute the unpaid features:\n","unpaid_cols = [\"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\"]\n","cat_cols = [\"SEX\", \"EDUCATION\", \"MARRIAGE\"]"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Now let's create a `ColumnTransfomer` object, which takes a list of tuples as input. Each tuple needs to have the following format: (`name`, `transformer`, `columns`)."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","preprocessor = ColumnTransformer([(\"payments\", payment_transformer, payment_cols), \n","                                  (\"numerical\", numerical_transformer, num_cols),\n","                                  (\"unpaid\", unpaid_transfomer, unpaid_cols),\n","                                  (\"categorical\", OneHotEncoder(), cat_cols)])\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Now we have a preprocessor object that we can reuse for any new data. The next step is to create a predictive model with it."]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Optimizing models\n","\n","In order to evaluate different predictive algorithms, we can create several pipeline objects, each combining our preprocessor together with a different algorithm. \n","\n","Create new pipeline objects (`dtc` and `rfc`) with our joint preprocessor for the Decision Tree and the Random Forest classifiers. Test them with `cross_val_score()`."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","dtc = make_pipeline(preprocessor, DecisionTreeClassifier())\n","print(cross_val_score(dtc,X,y,cv=split))\n","\n","rfc = make_pipeline(preprocessor, RandomForestClassifier()) \n","print(cross_val_score(rfc,X,y,cv=split))\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### Using grid search with a pipeline\n","\n","One major advantage of using a pipeline is that now we have both the processing **and** predictive modeling in one object, which can be trained and optimized jointly. It means that on every fold it is trained on, we will refit the model and also the transformations, which ensures we do not have data leakage and makes the evaluation more accurate. \n","\n","When tuning our model we can tune the hyperparameters of the model, and also some processing parameters, such as the number of PCA components. This would be much harder with a regular sequential preprocessing-then-training scenario.\n","\n","Below we use grid search to tune the `max_depth` and `min_samples_split` of the Random Forest model, together with the `n_components` in PCA.\n","\n","Note: to refer to a parameter, you need to use the name of the step, followed by `__`, then the parameter name. Since the PCA model is deep inside our pipeline, the name is a bit more complicated. Using `.named_steps` allows us to see what's inside our pipeline model:"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["rfc.named_steps"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Here PCA is in `columntransformer`, then `payments` (the Pipeline), then `pca` (one can use `.get_params()` to make sure we got the name of the parameter right). \n","\n","Use `GridSearchCV()` to find the parameters with the best accuracy. Print these parameters and the corresponding score, with `.best_params_` and `.best_score_`."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","params = {'columntransformer__payments__pca__n_components': [0.5, 0.98],\n","          'randomforestclassifier__max_depth': [5, 30],\n","          'randomforestclassifier__min_samples_split': [2, 5]}\n","\n","gcv = GridSearchCV(rfc, param_grid=params, n_jobs=-1, cv=split)\n","gcv.fit(X_train, y_train)\n","\n","print(gcv.best_params_)\n","print(gcv.best_score_)\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### Recover and fit the model with the best hyper-parameters\n","\n","You can recover the best hyper-parameters of the grid-search with `.set_params()`."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","rfc.set_params(**gcv.best_params_);\n","rfc.fit(X_train, y_train);\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Saving pre-trained models"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Now save the best pre-trained model to be used later, using the `pickle` library. Write your model to `\"data/rf_model.pickle\"`."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","# Open a file in write binary mode\n","with open(\"data/rf_model.pickle\", \"wb\") as f:\n","    pickle.dump(rfc, f)\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Now load your model and print its score again, to check that you have successfully saved it."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["with open(\"data/rf_model.pickle\", \"rb\") as f:\n","    rfc_pickle = pickle.load(f)\n","cross_val_score(rfc_pickle, X, y, cv=split)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Pickle is really useful to save trained models, but keep in mind that it does not save dependencies. To be able to load your model, you need to import the code it relies on: external libraries and our own class definitions. Because of this, it is common to get errors when trying to load a model trained with an old version of a library."]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### Reproducible experiments\n","\n","To make sure your model was not just lucky to get good results for a given training session, it is important to know how to make reproducible experiments. Apart from having the same code and data of course, to be reproducible, one needs to have same random number generator state. \n","\n","Run three instances of training your pipeline and testing it with `cross_val_score()`. Before two of them set the random seed to `1234`, with `np.random.seed()`. Do the results match exactly?"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","np.random.seed(); \n","rfc1 = make_pipeline(preprocessor, RandomForestClassifier()) \n","cross_val_score(rfc1, X, y, cv=split)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","np.random.seed(1234)\n","rfc2 = make_pipeline(preprocessor, RandomForestClassifier()) \n","cross_val_score(rfc2, X, y, cv=split)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","np.random.seed(1234)\n","rfc3 = make_pipeline(preprocessor, RandomForestClassifier()) \n","cross_val_score(rfc3, X, y, cv=split)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7"}},"nbformat":4,"nbformat_minor":4}
