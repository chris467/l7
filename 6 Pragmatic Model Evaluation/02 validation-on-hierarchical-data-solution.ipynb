{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Validation on Hierarchical Data\n", "\n", "You are familiar with cross-validation and holdout-validation, but when should each of them be used? and what happens if our data has a complex structure?\n", "\n", "In this notebook, we will demonstrate some examples to help answer these questions.\n", "\n", "## Holdout validation\n", "\n", "During holdout validation, we select a holdout sample at the beginning of the modelling process, before we have done any data processing or model training. This is how we test the performance of a model on a situation it has not seen before. If possible, it is good to use data which has also been collected separately, for example a separate year of data, or data from a seperate geographical sample. \n", "\n", "This helps ensure the relations in the model are *general* to the population and not *specific* to your dataset. \n", "\n", "It is vital that the validation data data remains untouched till testing!\n", "\n", "Let's use the travel mode dataset to investigate scoring methods. In this dataset there isn't an easy way of selecting data which was collected sepearately, as we do not information such as the trip dates for instance. \n", "\n", "We will therefore have to rely on some sort of random sampling.\n", "\n", "Let's load the data and the libraries we need."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn import metrics, preprocessing, model_selection\n", "\n", "import matplotlib.pyplot as plt\n", "\n", "%matplotlib inline"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["df = pd.read_csv('./data/travel_mode.csv')\n", "bool_map = {'no': 0, 'yes': 1}\n", "education_map = {'lower': 0, 'middle': 1, 'higher': 2}\n", "income_map = {'less20': 0, '20to40': 1, 'more40': 2}\n", "mode_map = {'car': 0, 'walk': 1, 'bike': 2, 'pt': 3}\n", "\n", "df.mode_main = df.mode_main.map(mode_map)\n", "df.male = df.male.map(bool_map)\n", "df.license = df.license.map(bool_map)\n", "df.weekend = df.weekend.map(bool_map)\n", "df.education = df.education.map(education_map)\n", "df.income = df.income.map(income_map)\n", "\n", "df = pd.get_dummies(df)\n", "\n", "#drop native column so n-1 catgories\n", "df.drop('ethnicity_native', axis=1, inplace=True)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's have a look at the `household_id` column, along with some other columns."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["df.head(20)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["So we can clearly see that people from the same household seem to be making multiple trips, and they will be highly correlated, in terms of explanitary variables (e.g. green, the percentage of green space in the vicinity around the household), and the target variable (e.g. mode choice). \n", "\n", "For instance, four trips are made a 24 year old male in household 3460, all walking, and all with identical explanitary variables, except distance and density.\n", "\n", "This data is therefore *hierarchical*.\n", "\n", "### Implications of hierarchical data\n", "\n", "So what is the relevance of the hierarchical nature of the data?\n", "\n", "When we sample data randomly to form a test set, each individual row has an equal probability of being selected, which means there is a high chance of rows (trips) made by the same household appearing in both the test and train dataset. This is bad, as we can then overfit the model to noise in that households data - i.e. unique features of the household/individual, and not general relations which indicate someone is likely to take one mode over another.\n", "\n", "Let's dig a little deeper to find out more."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["print('There are {} trips made by {} households in the data'.format(len(df), max(df.household_id)))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["So it looks like we have around 5 trips made by each household! For sure that will introduce correlations between test and train data which are specific to the household, and not general.\n", "\n", "First, let's generate some results for a model that we know will overfit, a decision tree with no maximum depth, using our original random sampling. First lets generate train and test folds. We will add the suffix `_r` for random."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["y = df.mode_main\n", "hh = df.household_id\n", "X = df.drop(['mode_main', 'household_id'], axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["X_train_r, X_test_r, y_train_r, y_test_r = model_selection.train_test_split(X, y)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Then we can create our candidate model. We'll use standard parameters, except we wont restrict the tree depth!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["clf_r = RandomForestClassifier(n_estimators=50, max_features=3, max_depth=None, n_jobs=-1)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The next cell might be slow!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["clf_r.fit(X_train_r, y_train_r)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Next, caluclate the discrete and probabilistic classifications for the classifier, and use them to calculate the predicted log loss and accuracy."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# generate y_pred and y_probs\n", "y_pred_r = clf_r.predict(X_test_r)\n", "y_probs_r = clf_r.predict_proba(X_test_r)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# and the accuracy score\n", "metrics.accuracy_score(y_test_r, y_pred_r)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# and the log loss\n", "metrics.log_loss(y_test_r, y_probs_r)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Great, we have some baseline scores to compare against! By not restricing the tree depth at all, we are almost definitely overfitting our model to the noise from the hierarchical data. We can see our metrics have improved hugely from yesterday, just by not restricting the tree depth. \n", "\n", "We are essenitally predicting the training data!\n", "\n", "### Grouped sampling\n", "But how do we deal with hierarchical data when sampling test data?\n", "\n", "Scikit learn can help! We can use `GroupShuffleSplit` from the `model_selection` module, see the documentation [here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupShuffleSplit.html).\n", "\n", "Remember, we only need one split for holdout-validation, so be sure to set the approriate parameters. Also stick to a 70:30 split.\n", "\n", "Also, when using split, make sure to use the household series (stored as `hh`) as the groups.\n", "\n", "*HINT* `split` returns a generator, so you will need to get the result out of the generator by iterating over it. Remember, you can use [`next`](https://docs.python.org/2/library/functions.html#next) to simplify things!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["gss = model_selection.GroupShuffleSplit(n_splits=1, train_size=.7, test_size=.3, \n", "                                        random_state=42)\n", "train_idx, test_idx = next(gss.split(X, y, groups=hh))\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can then use the indices from the split to generate the train and test data (yes it is a little longwinded!) We can use the suffix `_g` for grouped."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["X_train_g, X_test_g, y_train_g, y_test_g = X.iloc[train_idx], X.iloc[test_idx], y[train_idx], y[test_idx]"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Now lets fit the same classifier to our correctly sampled data..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["clf_g = RandomForestClassifier(n_estimators=50, max_features=3, max_depth=None, n_jobs=-1)\n", "clf_g.fit(X_train_g, y_train_g)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["y_pred_g = clf_g.predict(X_test_g)\n", "y_probs_g = clf_g.predict_proba(X_test_g)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["metrics.accuracy_score(y_test_g, y_pred_g)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["metrics.log_loss(y_test_g, y_probs_g)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Wow! That's a pretty big drop in our metrics - accuracy has dropped to from 83% to 68%! Goes to show it is **very** important to deal with hierarchical data properly."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Cross-validation\n", "\n", "Cross validation is when we can estimate model performance on a dataset of fixed size. It is very useful for instance when trying to select model parameters.\n", "\n", "It is important to note, that the validation we do to select model parameters/feature engineering, needs to be completely sepearate to the holdout-validation to test the model. For instance, if we separate data into a testing and a holdout training set, we should then only do model selction using the training set. Otherwise, we will be selecting parameters which allow us to fit to our testing data. This is known as *data leakage*.\n", "\n", "So how do we do cross-validation with our hiearchical data? This time, we can use [GroupKFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html )!\n", "\n", "We will use GroupKFold to do a simple parameter search on max-depth for the forest.\n", "\n", "Firstly, we need to get hold of the *groups* (`household_id`) for the train data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["gss = model_selection.GroupShuffleSplit(n_splits=1, train_size=.7, test_size=.3, \n", "                                        random_state=42)\n", "train_idx, test_idx = next(gss.split(X, y, groups=hh))\n", "X_train_g, X_test_g, y_train_g, y_test_g = X.iloc[train_idx], X.iloc[test_idx], y[train_idx], y[test_idx]\n", "hh_train_g, hh_test_g = hh.iloc[train_idx], hh.iloc[test_idx]"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Next we can use `GroupKFold` with `GridSearchCV`, documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n", "\n", "*HINT* Use the `cv` parameter within `GridSearchCV` to do a grouped K-fold CV.\n", "\n", "Let's leave the other parameters the same, and just investigate the `max_depth` parameter. We can try values of 2,5 and 10, as a crude search."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["clf_cv = RandomForestClassifier(n_estimators=50, max_features=3, n_jobs=-1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["params = {'max_depth': [2, 5, 10]}\n", "# use GroupKFold with 3 splits and GridSearchCV to search max_depth valus of 2, 6 and 12.\n", "\n", "gcv = model_selection.GroupKFold(n_splits=3).split(X_train_g, y_train_g, groups=hh_train_g)\n", "gs = model_selection.GridSearchCV(clf_cv, param_grid=params, scoring=['accuracy', 'neg_log_loss'],\n", "                                  n_jobs=1, cv=gcv, refit='neg_log_loss', verbose=3)\n", "gs.fit(X_train_g, y_train_g)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["So it seems the `max_depth` of 12 worked best out of the values we tried. Let's see how well this classifier works on our validation data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["clf_cv = gs.best_estimator_"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["clf_cv_pred = gs.predict(X_test_g)\n", "clf_cv_probs = gs.predict_proba(X_test_g)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["metrics.accuracy_score(y_test_g, clf_cv_pred)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["metrics.log_loss(y_test_g, clf_cv_probs)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Finally, we can investigate which features were most important in the two different classifiers. The following cells plot bar charts of the feature importances for each classifier."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["features = X.columns\n", "importances = clf_r.feature_importances_\n", "indices = np.argsort(importances)\n", "\n", "plt.title('Feature Importances')\n", "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n", "plt.yticks(range(len(indices)), features[indices])\n", "plt.xlabel('Relative Importance')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["features = X.columns\n", "importances = clf_cv.feature_importances_\n", "indices = np.argsort(importances)\n", "\n", "plt.title('Feature Importances')\n", "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n", "plt.yticks(range(len(indices)), features[indices])\n", "plt.xlabel('Relative Importance')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["So it seems the first classifier, with the random sampling, was overfitting to the `float` values, `density`, `green`, and `diversity`. This makes sense, as each tree could just memorise the mode associated with each unique value, and repeat it for the test data. The classifier using grouped sampling puts much more emphasis on car ownership and trip distance."]}]}