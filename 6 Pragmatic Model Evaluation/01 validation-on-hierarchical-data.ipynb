{"cells":[{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["# Validation on Hierarchical Data\n","\n","You are familiar with cross-validation and holdout-validation, but when should each of them be used? and what happens if our data has a complex structure?\n","\n","In this notebook, we will demonstrate some examples to help answer these questions.\n","\n","## Holdout validation\n","\n","During holdout validation, we select a holdout sample at the beginning of the modelling process, before we have done any data processing or model training. This is how we test the performance of a model on a situation it has not seen before. If possible, it is good to use data which has also been collected separately, for example a separate year of data, or data from a seperate geographical sample. \n","\n","This helps ensure the relations in the model are *general* to the population and not *specific* to your dataset. \n","\n","It is vital that the validation data data remains untouched till testing!\n","\n","Let's use the travel mode dataset to investigate scoring methods. In this dataset there isn't an easy way of selecting data which was collected sepearately, as we do not information such as the trip dates for instance. \n","\n","We will therefore have to rely on some sort of random sampling.\n","\n","Let's load the data and the libraries we need."]},{"cell_type":"code","execution_count":1,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import metrics, preprocessing, model_selection\n","\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":3,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["df = pd.read_csv('./data/travel_mode.csv')\n","bool_map = {'no': 0, 'yes': 1}\n","education_map = {'lower': 0, 'middle': 1, 'higher': 2}\n","income_map = {'less20': 0, '20to40': 1, 'more40': 2}\n","mode_map = {'car': 0, 'walk': 1, 'bike': 2, 'pt': 3}\n","\n","df.mode_main = df.mode_main.map(mode_map)\n","df.male = df.male.map(bool_map)\n","df.license = df.license.map(bool_map)\n","df.weekend = df.weekend.map(bool_map)\n","df.education = df.education.map(education_map)\n","df.income = df.income.map(income_map)\n","\n","df = pd.get_dummies(df)\n","\n","#drop native column so n-1 catgories\n","df.drop('ethnicity_native', axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Let's have a look at the `household_id` column, along with some other columns."]},{"cell_type":"code","execution_count":4,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>household_id</th>\n","      <th>distance</th>\n","      <th>density</th>\n","      <th>age</th>\n","      <th>male</th>\n","      <th>education</th>\n","      <th>income</th>\n","      <th>cars</th>\n","      <th>license</th>\n","      <th>bicycles</th>\n","      <th>weekend</th>\n","      <th>diversity</th>\n","      <th>green</th>\n","      <th>temp</th>\n","      <th>precip</th>\n","      <th>wind</th>\n","      <th>mode_main</th>\n","      <th>ethnicity_nonwestern</th>\n","      <th>ethnicity_western</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7875</td>\n","      <td>1.0</td>\n","      <td>1.26259</td>\n","      <td>84</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1.24604</td>\n","      <td>26.881233</td>\n","      <td>0.1</td>\n","      <td>0.10</td>\n","      <td>3.0</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7875</td>\n","      <td>10.0</td>\n","      <td>1.26259</td>\n","      <td>84</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1.24604</td>\n","      <td>26.881233</td>\n","      <td>0.1</td>\n","      <td>0.10</td>\n","      <td>3.0</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>14449</td>\n","      <td>3.0</td>\n","      <td>1.76264</td>\n","      <td>27</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1.53959</td>\n","      <td>36.045955</td>\n","      <td>-3.4</td>\n","      <td>0.05</td>\n","      <td>1.8</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>14449</td>\n","      <td>3.0</td>\n","      <td>1.76264</td>\n","      <td>27</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1.53959</td>\n","      <td>36.045955</td>\n","      <td>-3.4</td>\n","      <td>0.05</td>\n","      <td>1.8</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>14449</td>\n","      <td>61.5</td>\n","      <td>1.76264</td>\n","      <td>27</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1.53959</td>\n","      <td>36.045955</td>\n","      <td>-3.4</td>\n","      <td>0.05</td>\n","      <td>1.8</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>14449</td>\n","      <td>61.5</td>\n","      <td>0.88025</td>\n","      <td>27</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1.53959</td>\n","      <td>36.045955</td>\n","      <td>-3.4</td>\n","      <td>0.05</td>\n","      <td>1.8</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>19452</td>\n","      <td>0.5</td>\n","      <td>1.83997</td>\n","      <td>47</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1.72325</td>\n","      <td>36.698977</td>\n","      <td>-2.2</td>\n","      <td>0.05</td>\n","      <td>2.5</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>19452</td>\n","      <td>0.5</td>\n","      <td>2.15367</td>\n","      <td>47</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1.72325</td>\n","      <td>36.698977</td>\n","      <td>-2.2</td>\n","      <td>0.05</td>\n","      <td>2.5</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>10039</td>\n","      <td>0.4</td>\n","      <td>1.18075</td>\n","      <td>31</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1.33844</td>\n","      <td>82.758365</td>\n","      <td>-1.0</td>\n","      <td>0.00</td>\n","      <td>2.5</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10706</td>\n","      <td>5.0</td>\n","      <td>2.17375</td>\n","      <td>65</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1.36730</td>\n","      <td>33.290612</td>\n","      <td>0.2</td>\n","      <td>2.20</td>\n","      <td>1.8</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10706</td>\n","      <td>5.0</td>\n","      <td>1.25468</td>\n","      <td>65</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1.36730</td>\n","      <td>33.290612</td>\n","      <td>0.2</td>\n","      <td>2.20</td>\n","      <td>1.8</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>10706</td>\n","      <td>5.0</td>\n","      <td>3.09575</td>\n","      <td>65</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1.36730</td>\n","      <td>33.290612</td>\n","      <td>0.2</td>\n","      <td>2.20</td>\n","      <td>1.8</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12465</td>\n","      <td>20.0</td>\n","      <td>0.70105</td>\n","      <td>47</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>1.45511</td>\n","      <td>85.867899</td>\n","      <td>-0.8</td>\n","      <td>0.05</td>\n","      <td>2.7</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>12465</td>\n","      <td>20.0</td>\n","      <td>1.50813</td>\n","      <td>47</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>1.45511</td>\n","      <td>85.867899</td>\n","      <td>-0.8</td>\n","      <td>0.05</td>\n","      <td>2.7</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>3460</td>\n","      <td>1.0</td>\n","      <td>2.61713</td>\n","      <td>24</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1.02295</td>\n","      <td>22.163588</td>\n","      <td>-1.0</td>\n","      <td>0.00</td>\n","      <td>2.5</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>3460</td>\n","      <td>0.5</td>\n","      <td>2.61713</td>\n","      <td>24</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1.02295</td>\n","      <td>22.163588</td>\n","      <td>-1.0</td>\n","      <td>0.00</td>\n","      <td>2.5</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>3460</td>\n","      <td>0.5</td>\n","      <td>5.26579</td>\n","      <td>24</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1.02295</td>\n","      <td>22.163588</td>\n","      <td>-1.0</td>\n","      <td>0.00</td>\n","      <td>2.5</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>3460</td>\n","      <td>0.5</td>\n","      <td>5.26579</td>\n","      <td>24</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1.02295</td>\n","      <td>22.163588</td>\n","      <td>-1.0</td>\n","      <td>0.00</td>\n","      <td>2.5</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>11644</td>\n","      <td>25.2</td>\n","      <td>2.79098</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1.41009</td>\n","      <td>38.285714</td>\n","      <td>-0.5</td>\n","      <td>0.00</td>\n","      <td>2.8</td>\n","      <td>3</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>11644</td>\n","      <td>25.2</td>\n","      <td>1.47212</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1.41009</td>\n","      <td>38.285714</td>\n","      <td>-0.5</td>\n","      <td>0.00</td>\n","      <td>2.8</td>\n","      <td>3</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    household_id  distance  density  age  male  education  income  cars  \\\n","0           7875       1.0  1.26259   84     0          0       0     0   \n","1           7875      10.0  1.26259   84     0          0       0     0   \n","2          14449       3.0  1.76264   27     1          1       1     1   \n","3          14449       3.0  1.76264   27     1          1       1     1   \n","4          14449      61.5  1.76264   27     1          1       1     1   \n","5          14449      61.5  0.88025   27     1          1       1     1   \n","6          19452       0.5  1.83997   47     1          0       0     0   \n","7          19452       0.5  2.15367   47     1          0       0     0   \n","8          10039       0.4  1.18075   31     1          1       1     1   \n","9          10706       5.0  2.17375   65     0          0       0     1   \n","10         10706       5.0  1.25468   65     0          0       0     1   \n","11         10706       5.0  3.09575   65     0          0       0     1   \n","12         12465      20.0  0.70105   47     1          2       2     2   \n","13         12465      20.0  1.50813   47     1          2       2     2   \n","14          3460       1.0  2.61713   24     0          1       1     1   \n","15          3460       0.5  2.61713   24     0          1       1     1   \n","16          3460       0.5  5.26579   24     0          1       1     1   \n","17          3460       0.5  5.26579   24     0          1       1     1   \n","18         11644      25.2  2.79098   29     0          2       2     0   \n","19         11644      25.2  1.47212   29     0          2       2     0   \n","\n","    license  bicycles  weekend  diversity      green  temp  precip  wind  \\\n","0         1         1        1    1.24604  26.881233   0.1    0.10   3.0   \n","1         1         1        1    1.24604  26.881233   0.1    0.10   3.0   \n","2         1         2        1    1.53959  36.045955  -3.4    0.05   1.8   \n","3         1         2        1    1.53959  36.045955  -3.4    0.05   1.8   \n","4         1         2        1    1.53959  36.045955  -3.4    0.05   1.8   \n","5         1         2        1    1.53959  36.045955  -3.4    0.05   1.8   \n","6         1         3        1    1.72325  36.698977  -2.2    0.05   2.5   \n","7         1         3        1    1.72325  36.698977  -2.2    0.05   2.5   \n","8         1         2        1    1.33844  82.758365  -1.0    0.00   2.5   \n","9         1         2        1    1.36730  33.290612   0.2    2.20   1.8   \n","10        1         2        1    1.36730  33.290612   0.2    2.20   1.8   \n","11        1         2        1    1.36730  33.290612   0.2    2.20   1.8   \n","12        1         7        1    1.45511  85.867899  -0.8    0.05   2.7   \n","13        1         7        1    1.45511  85.867899  -0.8    0.05   2.7   \n","14        1         3        1    1.02295  22.163588  -1.0    0.00   2.5   \n","15        1         3        1    1.02295  22.163588  -1.0    0.00   2.5   \n","16        1         3        1    1.02295  22.163588  -1.0    0.00   2.5   \n","17        1         3        1    1.02295  22.163588  -1.0    0.00   2.5   \n","18        0         3        1    1.41009  38.285714  -0.5    0.00   2.8   \n","19        0         3        1    1.41009  38.285714  -0.5    0.00   2.8   \n","\n","    mode_main  ethnicity_nonwestern  ethnicity_western  \n","0           1                 False              False  \n","1           1                 False              False  \n","2           0                 False               True  \n","3           0                 False               True  \n","4           0                 False               True  \n","5           0                 False               True  \n","6           1                 False              False  \n","7           1                 False              False  \n","8           1                 False              False  \n","9           0                 False              False  \n","10          0                 False              False  \n","11          0                 False              False  \n","12          0                 False              False  \n","13          0                 False              False  \n","14          1                 False              False  \n","15          1                 False              False  \n","16          1                 False              False  \n","17          1                 False              False  \n","18          3                 False               True  \n","19          3                 False               True  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df.head(20)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["So we can clearly see that people from the same household seem to be making multiple trips, and they will be highly correlated, in terms of explanitary variables (e.g. green, the percentage of green space in the vicinity around the household), and the target variable (e.g. mode choice). \n","\n","For instance, four trips are made a 24 year old male in household 3460, all walking, and all with identical explanitary variables, except distance and density.\n","\n","This data is therefore *hierarchical*.\n","\n","### Implications of hierarchical data\n","\n","So what is the relevance of the hierarchical nature of the data?\n","\n","When we sample data randomly to form a test set, each individual row has an equal probability of being selected, which means there is a high chance of rows (trips) made by the same household appearing in both the test and train dataset. This is bad, as we can then overfit the model to noise in that households data - i.e. unique features of the household/individual, and not general relations which indicate someone is likely to take one mode over another.\n","\n","Let's dig a little deeper to find out more."]},{"cell_type":"code","execution_count":5,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 230608 trips made by 45036 households in the data\n"]}],"source":["print('There are {} trips made by {} households in the data'.format(len(df), max(df.household_id)))"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["So it looks like we have around 5 trips made by each household! For sure that will introduce correlations between test and train data which are specific to the household, and not general.\n","\n","First, let's generate some results for a model that we know will overfit, a decision tree with no maximum depth, using our original random sampling. First lets generate train and test folds. We will add the suffix `_r` for random."]},{"cell_type":"code","execution_count":6,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["y = df.mode_main\n","hh = df.household_id\n","X = df.drop(['mode_main', 'household_id'], axis=1)"]},{"cell_type":"code","execution_count":7,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["X_train_r, X_test_r, y_train_r, y_test_r = model_selection.train_test_split(X, y)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Then we can create our candidate model. We'll use standard parameters, except we wont restrict the tree depth!"]},{"cell_type":"code","execution_count":8,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["clf_r = RandomForestClassifier(n_estimators=50, max_features=3, max_depth=None, n_jobs=-1)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["The next cell might be slow!"]},{"cell_type":"code","execution_count":9,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_features=3, n_estimators=50, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_features=3, n_estimators=50, n_jobs=-1)</pre></div></div></div></div></div>"],"text/plain":["RandomForestClassifier(max_features=3, n_estimators=50, n_jobs=-1)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["clf_r.fit(X_train_r, y_train_r)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Next, caluclate the discrete and probabilistic classifications for the classifier, and use them to calculate the predicted log loss and accuracy."]},{"cell_type":"code","execution_count":10,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# generate y_pred and y_probs\n","y_pred_r = clf_r.predict(X_test_r)\n","y_probs_r = clf_r.predict_proba(X_test_r)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[{"data":{"text/plain":["0.8258169707902588"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# and the accuracy score\n","metrics.accuracy_score(y_test_r, y_pred_r)"]},{"cell_type":"code","execution_count":13,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[{"data":{"text/plain":["0.6127672653174836"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# and the log loss\n","metrics.log_loss(y_test_r, y_probs_r)\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Great, we have some baseline scores to compare against! By not restricing the tree depth at all, we are almost definitely overfitting our model to the noise from the hierarchical data. We can see our metrics have improved hugely from yesterday, just by not restricting the tree depth. \n","\n","We are essenitally predicting the training data!\n","\n","### Grouped sampling\n","But how do we deal with hierarchical data when sampling test data?\n","\n","Scikit learn can help! We can use `GroupShuffleSplit` from the `model_selection` module, see the documentation [here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupShuffleSplit.html).\n","\n","Remember, we only need one split for holdout-validation, so be sure to set the approriate parameters. Also stick to a 70:30 split.\n","\n","Also, when using split, make sure to use the household series (stored as `hh`) as the groups.\n","\n","*HINT* `split` returns a generator, so you will need to get the result out of the generator by iterating over it. Remember, you can use [`next`](https://docs.python.org/2/library/functions.html#next) to simplify things!"]},{"cell_type":"code","execution_count":16,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["from sklearn.model_selection import GroupShuffleSplit\n","gss = GroupShuffleSplit(n_splits=1, train_size= 0.7, random_state= 42)\n","\n","train_idx, test_idx = next(gss.split(X, y, groups=hh))"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["We can then use the indices from the split to generate the train and test data (yes it is a little longwinded!) We can use the suffix `_g` for grouped."]},{"cell_type":"code","execution_count":17,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["X_train_g, X_test_g, y_train_g, y_test_g = X.iloc[train_idx], X.iloc[test_idx], y[train_idx], y[test_idx]"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Now lets fit the same classifier to our correctly sampled data..."]},{"cell_type":"code","execution_count":18,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[{"data":{"text/html":["<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_features=3, n_estimators=50, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_features=3, n_estimators=50, n_jobs=-1)</pre></div></div></div></div></div>"],"text/plain":["RandomForestClassifier(max_features=3, n_estimators=50, n_jobs=-1)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["clf_g = RandomForestClassifier(n_estimators=50, max_features=3, max_depth=None, n_jobs=-1)\n","clf_g.fit(X_train_g, y_train_g)"]},{"cell_type":"code","execution_count":19,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["y_pred_g = clf_g.predict(X_test_g)\n","y_probs_g = clf_g.predict_proba(X_test_g)"]},{"cell_type":"code","execution_count":20,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[{"data":{"text/plain":["0.6777958936326778"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["metrics.accuracy_score(y_test_g, y_pred_g)"]},{"cell_type":"code","execution_count":21,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[{"data":{"text/plain":["0.9007184775169212"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["metrics.log_loss(y_test_g, y_probs_g)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Wow! That's a pretty big drop in our metrics - accuracy has dropped to from 83% to 68%! Goes to show it is **very** important to deal with hierarchical data properly."]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Cross-validation\n","\n","Cross validation is when we can estimate model performance on a dataset of fixed size. It is very useful for instance when trying to select model parameters.\n","\n","It is important to note, that the validation we do to select model parameters/feature engineering, needs to be completely sepearate to the holdout-validation to test the model. For instance, if we separate data into a testing and a holdout training set, we should then only do model selction using the training set. Otherwise, we will be selecting parameters which allow us to fit to our testing data. This is known as *data leakage*.\n","\n","So how do we do cross-validation with our hiearchical data? This time, we can use [GroupKFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html )!\n","\n","We will use GroupKFold to do a simple parameter search on max-depth for the forest.\n","\n","Firstly, we need to get hold of the *groups* (`household_id`) for the train data."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["gss = model_selection.GroupShuffleSplit(n_splits=1, train_size=.7, test_size=.3, \n","                                        random_state=42)\n","train_idx, test_idx = next(gss.split(X, y, groups=hh))\n","X_train_g, X_test_g, y_train_g, y_test_g = X.iloc[train_idx], X.iloc[test_idx], y[train_idx], y[test_idx]\n","hh_train_g, hh_test_g = hh.iloc[train_idx], hh.iloc[test_idx]"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Next we can use `GroupKFold` with `GridSearchCV`, documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n","\n","*HINT* Use the `cv` parameter within `GridSearchCV` to do a grouped K-fold CV.\n","\n","Let's leave the other parameters the same, and just investigate the `max_depth` parameter. We can try values of 2,5 and 10, as a crude search."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["clf_cv = RandomForestClassifier(n_estimators=50, max_features=3, n_jobs=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["params = {'max_depth': [2, 5, 10]}\n","# use GroupKFold with 3 splits and GridSearchCV to search max_depth valus of 2, 6 and 12.\n","\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["So it seems the `max_depth` of 12 worked best out of the values we tried. Let's see how well this classifier works on our validation data."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["clf_cv = gs.best_estimator_"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["clf_cv_pred = gs.predict(X_test_g)\n","clf_cv_probs = gs.predict_proba(X_test_g)"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["metrics.accuracy_score(y_test_g, clf_cv_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["metrics.log_loss(y_test_g, clf_cv_probs)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Finally, we can investigate which features were most important in the two different classifiers. The following cells plot bar charts of the feature importances for each classifier."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["features = X.columns\n","importances = clf_r.feature_importances_\n","indices = np.argsort(importances)\n","\n","plt.title('Feature Importances')\n","plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n","plt.yticks(range(len(indices)), features[indices])\n","plt.xlabel('Relative Importance')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["features = X.columns\n","importances = clf_cv.feature_importances_\n","indices = np.argsort(importances)\n","\n","plt.title('Feature Importances')\n","plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n","plt.yticks(range(len(indices)), features[indices])\n","plt.xlabel('Relative Importance')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["So it seems the first classifier, with the random sampling, was overfitting to the `float` values, `density`, `green`, and `diversity`. This makes sense, as each tree could just memorise the mode associated with each unique value, and repeat it for the test data. The classifier using grouped sampling puts much more emphasis on car ownership and trip distance."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":2}
