{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Evaluating Models\n", "\n", "Main goals:\n", "\n", "* Select appropriate metrics for evaluating different models, given different data\n", "* Explore visualisation methods for evaluation\n", "\n", "We will work with `sklearn` and the usual data science libraries, which we import now:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import pandas as pd\n", "from matplotlib import pyplot as plt\n", "import seaborn as sns\n", "import numpy as np"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Task 1 : Is this fish ready?\n", "\n", "Imagine you have a small fish farm, where you breed a variety of fish. You manually inspect the fish to see when they are ready for sale. This involves picking up a fish, looking at it very closely and making a decision based on your many years of training and experience in fish appraisal.\n", "\n", "The sad fact is, you hate fish and it's a fairly time-consuming task. Could you just automatically weigh the fish and use that as a proxy for your skills? Surely it can't be that simple?"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Load the data\n", "\n", "You've collected data on the last ~150 fish you evaluated. You recorded the species, weight, some physical measurements, how much you think the fish is worth, and whether you think it is ready for sale."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["data = pd.read_csv('data/fish.csv')\n", "\n", "data.head()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Exploring the task\n", "\n", "You are hoping to predict the binary outcome recorded in the `Ready` column, using only the `Weight` column.\n", "\n", "* Using `pandas`, we find out the distribution of the outcomes. We represent these:\n", "    * Numerically (with the `.value_counts()` method of a DataFrame column)\n", "    * Visually (the output of `.value_counts()` has a `.plot()` method)\n", "\n", "* Is this a classification or a regression task?\n", "* What issues do you notice with the data?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["print(data['Ready'].value_counts())\n", "\n", "data['Ready'].value_counts().plot(kind='bar');\n", "\n", "# Your thoughts here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We will use a simple Naive Bayes model for predicting `Ready` from `Weight`. \n", "\n", "The data will be split into two sets: 75% for training the model and the remaining 25% for evaluating it.\n", "\n", "Splitting the data this way gives an idea how well the model generalises to unseen data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.naive_bayes import GaussianNB\n", "from sklearn.model_selection import train_test_split\n", "\n", "x = data[['Weight']]\n", "y = data['Ready']\n", "\n", "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n", "\n", "model = GaussianNB()\n", "model.fit(x_train, y_train)\n", "\n", "predictions = model.predict(x_test)\n", "\n", "print(predictions)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Base functions for evaluating the classifier\n", "\n", "Most metrics for classifiers are some combination of true/false negatives/positives.\n", "\n", "Implement functions for these, which take in two lists (truth and prediction) containing True and False booleans."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def get_tp(ground_truth, predictions):\n", "    # True positive: both ground truth and prediction are True\n", "    tp = 0\n", "    # Your code here...\n", "    return tp\n", "\n", "def get_tn(ground_truth, predictions):\n", "    # True negative: both ground truth and prediction are False\n", "    tn = 0\n", "    # Your code here...\n", "    return tn\n", "\n", "def get_fp(ground_truth, predictions):\n", "    # False positive: ground is False but prediction is True\n", "    fp = 0\n", "    # Your code here...\n", "    return fp\n", "\n", "def get_fn(ground_truth, predictions):\n", "    # False negative: ground is True but prediction is False\n", "    fn = 0\n", "    # Your code here...\n", "    return fn\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Compound functions for evaluating the classifier\n", "\n", "Some evaluation measures are just combinations of the output of the above functions:\n", "\n", "* Accuracy: $\\frac{TP + TN}{TP+FP+TN+FN}$\n", "\n", "* Precision: $\\frac{TP}{TP+FP}$\n", "\n", "* Recall: $\\frac{TP}{TP+FN}$\n", "\n", "And F1 is just a combination of the output of *those* functions:\n", "\n", "* F1 Score = $2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$\n", "\n", "Implement these now."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def get_accuracy(tp, fp, tn, fn):\n", "    # Your code here...\n", "    return accuracy\n", "\n", "def get_precision(tp, fp):\n", "    # Your code here...\n", "    return precision\n", "\n", "def get_recall(tp, fn):\n", "    # Your code here...\n", "    return recall\n", "\n", "def get_f1(precision, recall):\n", "    # Your code here...\n", "    return f1"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Use the eight functions now, on the `y_test` from the dataset and the `predictions` the model made."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["How would you summarise these results to someone who wasn't familiar with these measures?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your thoughts here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Double check your results against those calculated by functions in `sklearn.metrics`.\n", "\n", "If they aren't the same, then check you have implemented the four base functions accurately and that you haven't accidentally typed \"fp\" instead of \"tp\" somewhere!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n", "\n", "print(precision_score(y_test, predictions))\n", "print(accuracy_score(y_test, predictions))\n", "print(recall_score(y_test, predictions))\n", "print(f1_score(y_test, predictions))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# More detailed evaluation\n", "\n", "The functions you have implemented only look at the overall picture. However, it's often more useful to look at per-class performance.\n", "\n", "We won't implement this here. Instead, we will use `sklearn.metrics`.\n", "\n", "A very useful function for performing lots of evaluation is the classification report."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import classification_report\n", "\n", "print(classification_report(y_test, predictions))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["This generates many of the results you calculated, but with a few differences. For P/R/F1, it reports the macro and weighted averages, not the micro average.\n", "\n", "Macro average is per-label average. Weighted is the macro average weighted by the number of examples of each class in the data. \n", "\n", "If you want control over how averages are calculated, you can do this with the `sklearn.metrics.precision_recall_fscore_support` function.\n", "\n", "This doesn't return such a pretty table, though!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import precision_recall_fscore_support\n", "\n", "print(precision_recall_fscore_support(y_test, predictions, average=None))\n", "\n", "print(precision_recall_fscore_support(y_test, predictions, average='micro'))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Generate per-class statistics\n", "print(\"Per-class stats:\", *(f\"{i}:{x[0], x[1]}\" for i, x in zip(['p', 'r', 'f1'], precision_recall_fscore_support(y_test, predictions, average=None))))\n", "# Generate micro average\n", "print(\"Micro average:\", *(f\"{i}:{x:.2f}\" for i, x in zip(['p', 'r', 'f1'], precision_recall_fscore_support(y_test, predictions, average='micro'))))\n", "# Generate macro average\n", "print(\"Macro average:\", *(f\"{i}:{x:.2f}\" for i, x in zip(['p', 'r', 'f1'], precision_recall_fscore_support(y_test, predictions, average='macro'))))\n", "# Generate weighted average\n", "print(\"Weighted average:\", *(f\"{i}:{x:.2f}\" for i, x in zip(['p', 'r', 'f1'], precision_recall_fscore_support(y_test, predictions, average='weighted'))))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["How would you characterise the model's per-class performance? Where are the strengths and weaknesses?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your thoughts here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["If you were a fish farmer, would you be more concerned with classifying unready fish as ready? Or ready fish as unready?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your thoughts here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's say you are an especially conscientious fish farmer, so you are more concerned with making sure you don't accidentally classify fish as ready.\n", "\n", "Recall that the \"1\" in F1 score is the weight given to recall: F0.5 would favour precision twice as much as recall, F1.5 would favour recall 1.5 times as much as precision.\n", "\n", "Use the `beta` argument of `precision_recall_fscore_support` to compare a range of weights from 0.1 to 2.0, to see how F score changes for your predictions. Use the `macro` average."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Below are the values of P, R and F1 for a range of beta values. As you can see, P and R stay constant while F1 changes, but are bounded by R."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["x_vals = np.linspace(0, 8, 100)\n", "y_p = [precision_recall_fscore_support(y_test, predictions, beta=b, average='macro')[0] for b in x_vals]\n", "y_r = [precision_recall_fscore_support(y_test, predictions, beta=b, average='macro')[1] for b in x_vals]\n", "y_f = [precision_recall_fscore_support(y_test, predictions, beta=b, average='macro')[2] for b in x_vals]\n", "sns.lineplot(x=x_vals, y=y_p, lw=4, label='Precision' );\n", "sns.lineplot(x=x_vals, y=y_r, lw=4, label='Recall' );\n", "sns.lineplot(x=x_vals, y=y_f, lw=4, label='F1');"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Evaluating against baselines\n", "\n", "These figures don't really mean much on their own. We need something against which to compare the model.\n", "\n", "Common simple baselines are predicting the most common class (here, that is `True`) or just randomly guessing.\n", "\n", "Construct two lists for each of these baselines: `random_preds` and `most_common`.\n", "(Make sure each has the same number of elements as your model's predictions!)\n", "\n", "You might find the [`random` module](https://docs.python.org/3/library/random.html) in the Python standard library useful."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import random\n", "\n", "# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's use a heatmap to visualise the predictions of all three.\n", "\n", "`sklearn.metrics.confusion_matrix` will generate the right kind of data for this, so let's look at that first.\n", "\n", "Use `confusion_matrix` to see the results from the model and two baselines."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import confusion_matrix\n", "\n", "#Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Visualising confusion matrices\n", "\n", "`seaborn.heatmap` is a great function for visualising a confusion matrix.\n", "\n", "The code below sets up a 1x3 figure, then plots three heatmaps (one per model/baseline) in them."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,4))\n", "\n", "sns.heatmap(confusion_matrix(y_test, predictions, labels=[1,0]), ax=axes[0], annot=True, cmap='Greens', square=True, cbar=False, \n", "            xticklabels=['Ready', 'Not ready'], yticklabels=['Ready', 'Not ready']);\n", "sns.heatmap(confusion_matrix(y_test, most_common, labels=[1,0]), ax=axes[1], annot=True, cmap='Greens', square=True, cbar=False, \n", "            xticklabels=['Ready', 'Not ready'], yticklabels=['Ready', 'Not ready']);\n", "sns.heatmap(confusion_matrix(y_test, random_preds, labels=[1,0]), ax=axes[2], annot=True, cmap='Greens', square=True, cbar=False, \n", "            xticklabels=['Ready', 'Not ready'], yticklabels=['Ready', 'Not ready']);"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["These look pretty awful. Check [the `seaborn.heatmap` documentation](https://seaborn.pydata.org/generated/seaborn.heatmap.html) for useful configuration options.\n", "\n", "Useful ones:\n", "\n", "* `cmap` : lets you pick the colour scheme. See the [matplotlib colormap reference](https://matplotlib.org/3.1.1/gallery/color/colormap_reference.html) for names of colour schemes.\n", "* `square` : If `True`, it will make all cells nice and square.\n", "* `cbar` : Use `True`/`False` to show/hide the guide bar to the left\n", "* `annot` : If `True`, it will show numbers on the cells. If you want normalised numbers (e.g. percentages), you can do this in `sklearn.metrics.confusion_matrix` first.\n", "\n", "You will probably want to label your cells with the right names. In that case, pass the correct names with `xticklabels` and `yticklabels`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,4))\n", "\n", "# Copy the heatmap code from above and edit it using the arguments described above,\n", "# to make them look nicer and easier to read. \n", "# Use ax=axes[0], ax=axes[1], ax=axes[2] when calling `heatmap`\n", "# to place that heatmap in a specific column of the figure.\n", "\n", "# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Looking at prediction probabilities\n", "\n", "Many models provide information about how they made classification decisions. `sklearn` models generally have a `predict_proba` method. Rather than returning a list of predicted class labels, as with `predict`, it returns a list of lists where each sublist contains the probabilities generated for each class, per prediction.\n", "\n", "From the NB model we stored in `model` earlier, get the probabilities for the `x_test` set and look at the first few."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Calculate the log loss for these predictions against the true labels in `y_test`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import log_loss\n", "\n", "# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's compare this to our \"most common\" baseline. The probabilities for that will always be $[1, 0]$ if it predicts `True`, otherwise $[0, 1]$."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["most_common_probs = [[1,0]  if p == True else [0,1] for p in predictions]\n", "\n", "log_loss(y_test, most_common_probs)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Unsurprisingly, the NB model has much lower loss. Why do you suppose that is the case?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your thoughts here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Going further\n", "\n", "How would you characterise the NB model compared to the baselines? And does using only weight to classify fish as ready/not ready seem reasonable to you? Or do you have to carry on doing it manually?\n", "\n", "If you were to implement this model, how would you evaluate it extrinsically? What additional data would you need?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your thoughts here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Extensions\n", "\n", "* Look at the Species column for the fish data. How would you go about predicting that? What problems do you foresee having?\n", "* Would model performance improve if you used additional features besides weight?\n", "    * If so, how much do you gain?\n", "    * Would it be worth the extra work of measuring fish with a ruler?\n", "* How do different classification algorithms perform?"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Task 2: How much is this fish worth?\n", "\n", "As well as knowing whether a fish is ready for sale, you know exactly how much they are worth. You've recorded this in the `Value` column of your data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["data.head()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Could you predict `Value` from `Weight`? If so, you might never have to look at another fish ever again."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.linear_model import LinearRegression\n", "\n", "x = data[['Weight']]\n", "y = data['Value']\n", "\n", "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n", "\n", "model = LinearRegression()\n", "model.fit(x_train, y_train)\n", "\n", "predictions = model.predict(x_test)\n", "\n", "print(predictions)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["How do these predictions compare to the expected values?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Recall that for $n$ model predictions $\\hat{y} \\in \\hat{Y}$ and ground truth $y \\in Y$, the formula for Mean Squared Error: $$\\frac{1}{n} \\sum_{i=0}^{n - 1} (y_i - \\hat{y}_i)^2$$\n", "\n", "Implement a function to calculate this, given the true values and the predictions."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def mse(ground_truth, predictions):\n", "    #Your code here...\n", "    return mean_square_error"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Calculate MSE for `y_test` and `predictions` using your function. Compare it to the output of `sklearn.metrics.mean_squared_error` to see if they are the same."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["And now implement RMSE."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from math import sqrt\n", "\n", "def rmse(ground_truth, predictions):\n", "    #Your code here...\n", "    return rmse_output\n", "\n", "print(rmse(y_test, predictions))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Interpreting regression evaluation\n", "\n", "The RMSE gives an idea of how close the predictions are using the original units. Is being within roughly \u00a32 of the correct price good or bad? It depends on the task. If you are predicting house prices, being off by \u00a32 is probably good. If you are losing \u00a32 on a lot of fish sales, it could be the end of your fish farm.\n", "\n", "In general, regression evaluations require you to use your domain expertise to interpret them. But since they are error scores, it makes it easy to compare models (trained on the same data) because lower is always better.\n", "\n", "Generate two baselines:\n", "\n", "* The mean of all fish values in the training data\n", "* Some random prices with the same mean and standard deviation of the training data values\n", "\n", "Remember to make sure you have the same number of predictions in your baseline as you do from your model."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Evaluate these two baselines and your model using RMSE. How do they compare?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["model_rmse = rmse(y_test, predictions)\n", "\n", "mean_fish_rmse = rmse(y_test, mean_fish)\n", "\n", "random_fish_rmse = rmse(y_test, random_fish)\n", "\n", "print(f\"Regression: {model_rmse:.3f}\\t Mean: {mean_fish_rmse:.3f}\\t Random: {random_fish_rmse:.3f}\\n\\n\")\n", "\n", "# Your thoughts here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Visualising linear relations for evaluation\n", "\n", "It may be useful to visualise the `Value` data, to see the distribution we are trying to capture, using `seaborn.histplot`. This will show a histogram and a density estimation."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(16,4), sharey=False)\n", "\n", "sns.histplot(y_test, ax=axes[0], kde=True);\n", "sns.histplot(predictions, ax=axes[1], kde=True);\n", "sns.histplot(random_fish, ax=axes[2], kde=True);\n", "sns.histplot(mean_fish, ax=axes[3], kde=False);\n", "\n", "axes[0].set_title('True values')\n", "axes[1].set_title('Linear regression model predictions')\n", "axes[2].set_title('Random model predictions')\n", "axes[3].set_title('Mean model predictions')"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Edit the above code to instead show a `seaborn.regplot` to compare the predictions to the true values. This will fit a regression line with confidence intervals: the narrower the CI at a point on the regression line, the more constrained predictions are for y, given that x.\n", "\n", "(You can't fit a regression to a horizontal line in `seaborn`, so for `mean_fish` set `fit_reg=False`)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,4), sharey=True)\n", "\n", "# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["How would you interpret this information?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your thoughts here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can evaluate how good the relation between prediction and truth is, using the `r2_score` function from `sklearn.metrics`.\n", "\n", "Use this to evaluate the three models and get a better picture of how each model fits the data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import r2_score\n", "\n", "# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["How would you interpret these values?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your thoughts here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Extensions\n", "\n", "* Would using additional features, besides weight, improve the predictive performance of the model?\n", "* And again, what about different regression algorithms?"]}]}