{"cells":[{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["# Probabilistic classification in practice\n","\n","In this practical, we use a dataset of historic trips from the Netherlands to investigate passenger mode choice behaviour. The full data set is from the paper *A comparative study of machine learning classifiers for modeling travel mode choice* (Hagenauer and Helbich, 2017) and is available under CC BY 3.0 [here](https://www.sciencedirect.com/science/article/pii/S0957417417300738).\n","\n","Human choices exhibit significant variability. Even when faced with identical circumstances, two individuals, or even the same person, may make entirely distinct decisions. Hence, human choices serve as a prime illustration for employing probabilistic classification techniques.\n","\n","Now, we can proceed by importing the necessary libraries and loading the data. In this case, we will utilize logistic regression to model the data."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score, confusion_matrix, log_loss"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Importing the data\n","\n","Next we import and inspect the data."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["df = pd.read_csv('data/travel_mode.csv')\n","\n","df.head(10)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Everything looks relatively straightforward with our dataset. Let's investigate the distribution of the target variable, `mode_main`"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Class imbalance\n","\n","Let's check the class imbalance by generating the value counts for the `mode_main` DataFrame.\n","\n","_Hint, you can try normalising the `value_counts` method from Pandas._"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","df.mode_main.value_counts(normalize=True)\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["It is apparent that our dataset consists of four classes, characterized by a significant data imbalance. Evidently, our sample population exhibits a strong aversion towards utilizing public transportation."]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Data preprocessing"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Let's encode our target to an ordered number - 0 for car, 1 for walk, 2 for bike, 3 for public transport. \n","\n","_Hint: you can use `.map` to replace all values in a column at the same time_"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","mode_map = {'car': 0, 'walk': 1, 'bike': 2, 'pt': 3}\n","df.mode_main= df.mode_main.map(mode_map)\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["We can also see there are few more non-numerical categorical columns (all numerical columns are continuous values) - we can investigate these further."]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Print the normalised value counts for the columns `'male', 'ethnicity', 'education', 'license', 'weekend'`"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","for col in ['male', 'ethnicity', 'education', 'license', 'weekend']:\n","    print(f'{col}:\\n{df[col].value_counts(normalize=True)}\\n\\n')\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["After examining the data, it becomes evident that each categorical column consists of three or fewer classes, all of which are adequately represented. Therefore, it is highly likely that all classes will be present in any training fold we choose. Consequently, we can safely encode the categorical data in advance without concerns about data leakage.\n","\n","It is important to remember that there are multiple approaches for encoding categorical data, depending on whether it is nominal or ordinal. For each categorical column, it is necessary to determine the appropriate method to be used and proceed with encoding the respective columns accordingly."]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["\n","Encode male, ethnicity, education, income, license and weekend.\n","\n","Remember to use `.map` if you are doing ordered encoding, and the `OneHotEncoder` from scikit-learn for one-hot encoding.\n","\n","If you run out of time, don't worry - we've provided you with a processed version of the data later in the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","bool_map = {'no': 0, 'yes': 1}\n","education_map = {'lower': 0, 'middle': 1, 'higher': 2}\n","income_map = {'less20': 0, '20to40': 1, 'more40': 2}\n","\n","df.male = df.male.map(bool_map)\n","df.license = df.license.map(bool_map)\n","df.weekend = df.weekend.map(bool_map)\n","df.education = df.education.map(education_map)\n","df.income = df.income.map(income_map)\n","\n","df = pd.get_dummies(df)\n","\n","#drop native column so n-1 catgories\n","df.drop('ethnicity_native', axis=1, inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["df.head(10)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["With the completion of numerical encoding for all the data, we are now prepared to commence model training. However, for the time being, we will exclude the `household_id` column from the dataset. We will why and reintroduce the column later in our process."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["df.drop('household_id', axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Train-test split\n","\n","The following code reloads and encodes the data as required for the modelling exercices, so it is ready to use for model training."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["df = pd.read_csv('data/travel_mode.csv')\n","bool_map = {'no': 0, 'yes': 1}\n","education_map = {'lower': 0, 'middle': 1, 'higher': 2}\n","income_map = {'less20': 0, '20to40': 1, 'more40': 2}\n","mode_map = {'car': 0, 'walk': 1, 'bike': 2, 'pt': 3}\n","\n","df.mode_main = df.mode_main.map(mode_map)\n","df.male = df.male.map(bool_map)\n","df.license = df.license.map(bool_map)\n","df.weekend = df.weekend.map(bool_map)\n","df.education = df.education.map(education_map)\n","df.income = df.income.map(income_map)\n","\n","df = pd.get_dummies(df)\n","\n","#drop native column so n-1 catgories\n","df.drop('ethnicity_native', axis=1, inplace=True)\n","\n","#drop household_id\n","df.drop('household_id', axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["We can first separate the data into our input `X`, and our target `y` - the mode people travelled by."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["y = df.mode_main\n","X = df.drop('mode_main', axis=1)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["And then split it into test and train data."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X, y)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Finally we scale our data. Use the `StandardScaler` to scale the training and testing data, calling it `X_train` and `X_test`. Remember to fit it on the training data and then use the fitted scaler to transform the test data."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","scl = StandardScaler()\n","\n","X_train = scl.fit_transform(X_train_unscaled)\n","X_test = scl.transform(X_test_unscaled)\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["# A multinomial probabilistic classifier\n","\n","We now have all the necessary components to initiate the modeling process. Let's proceed by loading a logistic regression model with default hyperparameters and fitting it to the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# Your code here...\n","clf = LogisticRegression()\n","clf.fit(X_train, y_train)\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Multinomial logistic regression - parameters\n","\n","We can have a look at the model's coefficients and intercepts to see how the multinomial model works:"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["print(f'Model coefficients:\\n{clf.coef_}\\n\\nShape of coefficient matrix:{clf.coef_.shape}\\n\\n')\n","print(f'Model intercepts: {clf.intercept_}')"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["We can see we have a different set of model coefficients and a corresponding intercept for each class. \n","\n","We have four classes, so we have four sets of coefficients/intercepts. \n","\n","The first column is the coefficients for walking, the second for cycling, the third for public transport, and the last for driving. \n","\n","We have 17 input features, so each set of input coefficients contains 17 different values. "]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Assessing our model\n","\n","We have a fitted a classifier, lets evaluate it using accuracy:"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# calculate the accuracy score for the logistic regression model\n","y_pred = clf.predict(X_test)\n","print(f'Accuracy: {accuracy_score(y_test, clf.predict(X_test)):.3f}')"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["The obtained result appears promising, as our model is able to accurately predict over 76% of the trips made by individuals. Now, let's examine the distribution of predicted trips for each mode."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["inv_mode_map =  {v: k for k, v in mode_map.items()}\n","pd.Series(y_pred).map(inv_mode_map).value_counts()"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["We can compare these predictions with the actual outcomes recorded in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["pd.Series(y_test).map(inv_mode_map).value_counts()"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["The analysis of the predicted trip outcomes reveals a significant over-representation of car trips and a noticeable under-representation of other modes of transportation. Particularly, our model predicts less than one-fourth of the actual number of public transport trips. If these flawed predictions were provided to our client, it could have led to potentially detrimental decision-making, such as misguided investments in public transport based on figures inflated by 400%.\n","\n","To gain further insights into the issue, let's examine the confusion matrix and investigate what is happening."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["confusion_matrix(y_test, y_pred)\n","plt.figure(figsize=(12,8))\n","sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Upon closer inspection, it becomes evident that our model exhibits poor recall for the least common modes of transportation. While we achieve a relatively high overall accuracy due to confident predictions of the most common mode, we take significant risks when attempting to predict the least common mode, resulting in inaccurate aggregate predictions."]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Implications\n","\n","The issue highlighted in this problem has far-reaching consequences and can lead to serious practical problems. It has been observed at various levels, including high-profile instances such as the one mentioned [here](https://www.theguardian.com/inequality/2017/aug/08/rise-of-the-racist-robots-how-ai-is-learning-all-our-worst-impulses).\n","\n",">\"In May last year, a stunning report claimed that a computer program used by a US court for risk assessment was biased against black prisoners. The program, Correctional Offender Management Profiling for Alternative Sanctions (Compas), was much more prone to mistakenly label black defendants as likely to reoffend â€“ wrongly flagging them at almost twice the rate as white people (45% to 24%), according to the investigative journalism organisation ProPublica.\"\n","\n","### Proposed Solutions\n","\n","While metrics derived from the confusion matrix, such as precision and recall, provide insights into underpredictions for certain modes, they do not offer a means to rectify the issue in discrete predictions.\n","\n","Two potential solutions can be considered. The first option involves artificially oversampling or undersampling (or a combination of both) the data until all classes are equally represented. For instance, we could randomly sample public transport trips with replacement, repeating them multiple times until their count matches that of car trips. The same process can be applied to cycling and walking trips. However, this approach distorts the data and amplifies any noise present in those particular samples, thereby increasing the Signal to Noise Ratio. Conversely, if we were to undersample the most common class, we would remove valid information (our signal), once again increasing the Signal to Noise Ratio.\n","\n","### Embracing Probabilistic Classification\n","\n","Instead of resorting to data manipulation techniques, let's explore the potential of using our model as a probabilistic classifier. By simply utilizing the `predict_proba` function instead of `predict`, we can leverage the probabilities generated by the model."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["y_probs = clf.predict_proba(X_test)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Now lets have a look at the output:"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["y_probs[:,0]\n","plt.figure(figsize=(15,2))\n","sns.heatmap(np.transpose(y_probs))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["You can see we have a probability distribution for each trip, which gives us the probability of each mode being selected. We can measure the fit using the log-likelihood."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["log_loss(y_test, y_probs)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["The log loss metric penalizes models that exhibit excessive certainty in incorrect predictions. In the case of a deterministic model, which can only provide probabilities of either 1 or 0, and given that its accuracy score is not 1, we can conclude that it would yield an *infinite* log loss score. \n","\n","This highlights why log loss is not typically used to evaluate deterministic models. It appears unfair to compare the two models using this metric since it was specifically designed for probabilistic models. The choice between the two models should ultimately depend on the problem being addressed.\n","\n","### Utilizing the Probabilistic Model for Simulations\n","\n","By employing probability distributions, we can move beyond discrete predictions and simulate our predictions. Instead of relying on a single outcome, the following code allows us to draw values from the provided probability distribution:"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["#create data frame\n","cumprobs = pd.DataFrame()\n","\n","#add cumulative  probabilities\n","cumprobs[0] = y_probs[:,0]\n","cumprobs[1] = y_probs[:,0] + y_probs[:,1]\n","cumprobs[2] = y_probs[:,0] + y_probs[:,1] + y_probs[:,2]\n","\n","#generate random numbers \n","a = np.random.rand(len(y_probs))\n","\n","#predict transport for each individual \n","y_sim = (np.zeros(len(y_probs))\n","         + (a>cumprobs[0])\n","         + (a>cumprobs[1])\n","         + (a>cumprobs[2])).astype(int)\n","\n","y_sim"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Now let's compare the simulated and actual mode shares."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["pd.Series(y_sim).map(inv_mode_map).value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["pd.Series(y_test).map(inv_mode_map).value_counts()"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["As you can see, there is a much closer match between the two. We can actually work out the exact predicted mode shares, simply by summing the probabilities."]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["y_shares = y_probs.sum(axis=0)\n","{k: y_shares[v] for k, v in mode_map.items()}"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["When we do simulations like this we don't generally expect in any one simulation to accurately predict what is going on the underlying data in the same way as we would with a classifier, but it's still nice to visualise the confusion matrix. "]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["plt.figure(figsize=(12,8))\n","sns.heatmap(confusion_matrix(y_test,y_sim), annot=True, fmt=\"d\")\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Whilst this isn't as good as the confusion matrix we generated for the deterministic classifiers (i.e. the values on the diagonal aren't as large), simulated data gives us opportunities to generate lots of realistic \"data\" that we can then use to do more complex calculations with. Because we can repeat these simulations again and again, and then look at the statistics around these simulations we can get really detailed models of how things may behave if the world was different.\n","\n","To take real world examples, example financial derivatives are priced based on simulated stock data, and disease models use simulated people and households."]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Deciding on the Methodology\n","\n","The choice of methodology depends on various factors, including the deployment of the tool and the potential impact of individual classifications. In situations where the tool's classifications carry high-cost implications, such as classifying medical conditions, it is beneficial to have human expertise involved, such as a doctor, who can make the final decision. In such cases, having a probability distribution as an output can complement the decision-making process. This approach may prompt further tests, data collection, or enable the formulation of a diagnosis and treatment plan.\n","\n","On the other hand, for low-cost decisions like determining which advertisement to play next on a video streaming service, a deterministic classifier might be more suitable.\n","\n","However, the probabilistic classifier, being a more sophisticated tool, still offers deployment possibilities. For instance, if the classifier assesses the likelihood of a web user responding to different advertisements, a sequence of ads can be selected based on their descending likelihood. Additionally, tools can be incorporated to update probabilities based on user responses to the ads encountered so far or make use of assumptions about the real world to enhance predictions. For instance, if a user displays an interest in sandwiches, advertising just before lunchtime may be the most opportune moment.\n","\n","Probabilistic models also lend themselves to simulations, which can be highly valuable. Although they generally do not surpass well-designed deterministic classifiers in terms of accuracy or confusion matrix, these simulations can deliver remarkable outcomes. However, exploring the full potential of simulations will be covered later in the course."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7"}},"nbformat":4,"nbformat_minor":2}
