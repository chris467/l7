{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 4, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Discriminative Classifiers: K-Nearest Neighbours\n", "\n", "In this practical, we will first manually implement k-nearest neighbours. We will explore how the algorithm constructs decision boundaries and how it classifies data. We will visualise these boundaries for our own implementation, before using `sklearn` implentations to compare and contrast with our own.\n", "\n", "Then, we will move to using `sklearn` for a more traditional data science task, using a real dataset."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Implementing KNN\n", "\n", "Recall that the KNN algorithm relies on the concept of distance between two items in order to quantify how similar they are. Input data is represented as an array of features, which locates each item within a vector space. We will use data with two dimensions, longitude and latitude, which will make it easy to visualise.\n", "\n", "We will need four functions:\n", "\n", "1. `euclidean_distance(p1, p2)` - given a pair of (x,y) coordinates, calculate how close they are using Euclidean distance: $$ \\sqrt{(x_2-x_1)^2+(y_2-y_1)^2} $$\n", "2. `manhattan_distance(p1, p2)` - calculate $$ | x_1 - x_2 | + | y_1 - y_2 | $$\n", "3. `get_neighbours(new_point, known_points, distance_function)` - given a single coordinate, calculate how close it is to all other points using the provided distance function\n", "4. `classify(neighbours_list, k)` - given a list of points with their distances and their class label, determine which class is most common in the `k` nearest neighbours\n", "\n", "The classification process will work as follows:\n", "\n", "1. For a new point with no class label, get the distance from it to every known point\n", "2. From that list of distances, take the `k` nearest ones\n", "3. From that subset, find out which class is the most common and use it to label the new point\n", "\n", "Below are templates for the functions, which you will complete."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## The distance functions\n", "\n", "Complete the two functions below using the docstring as a guide.\n", "\n", "(Note: to calculate the absolute value for Manhattan distance, use the `abs()` function.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from math import sqrt  # For calculating square root\n", "    \n", "def euclidean_distance(point1, point2):\n", "    '''\n", "    Inputs:\n", "        point1 : a tuple of (lon, lat) coordinates\n", "        point2 : a tuple of (lon, lat) coordinates\n", "    \n", "    Returns:\n", "        distance : a float, the distance between the points\n", "    '''\n", "    \n", "    # Your code here...\n", "    \n", "    distance = sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n", "    \n", "    return distance    \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def manhattan_distance(point1, point2):\n", "    '''\n", "    Inputs:\n", "        point1 : a tuple of (lon, lat) coordinates\n", "        point2 : a tuple of (lon, lat) coordinates\n", "    \n", "    Returns:\n", "        distance : a float, the distance between the points\n", "    '''\n", "    \n", "    # Your code here...\n", "    \n", "    distance = abs(point1[0] - point2[0]) + abs(point1[1] - point2[1])\n", "    \n", "    return distance\n", "    \n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Testing the `distance` functions\n", "\n", "Run the cell below to test your functions are working as expected."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["assert euclidean_distance((0,0), (0,0)) == 0\n", "assert euclidean_distance((0,0), (-1,0)) == 1\n", "assert euclidean_distance((0,0), (2,0)) == 2\n", "assert euclidean_distance((0,0), (1,1)) == 1.4142135623730951\n", "assert euclidean_distance((1233,376.4), (-213.2,0.03)) == 1494.372382272906\n", "\n", "assert manhattan_distance((0,0), (0,0)) == 0\n", "assert manhattan_distance((0,0), (-1,0)) == 1\n", "assert manhattan_distance((0,0), (2,0)) == 2\n", "assert manhattan_distance((0,0), (1,1)) == 2\n", "assert manhattan_distance((1233,376.4), (-213.2,0.03)) == 1822.5700000000002\n", "\n", "print('All good!')"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## The `get_neighbours` function\n", "\n", "This should loop through a list of labeled points and calculate the distance between each one and a new unlabeled point, using your `distance` function.\n", "\n", "(Note: the returned list should be sorted, such that the first item is the nearest and so on. Use `sorted()` on a list. To sort them by the third item in each sub-item, use `sorted(your_list, key=itemgetter(2))` - remember that Python starts counting at 0, not 1.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from operator import itemgetter  # For sorting lists by particular items inside sub-items\n", "\n", "def get_neighbours(new_point, known_points, distance_function):\n", "    '''\n", "    Inputs:\n", "        new_point : a tuple of (lon, lat) coordinates\n", "        known_points : a list of tuples of (lon, lat, label)\n", "        distance_function : a function to calculate distance\n", "    Returns:\n", "        neighbours : a sorted list of tuples with ((lon, lat), label, distance)\n", "    '''\n", "    \n", "    # Your code here...\n", "    \n", "    neighbours = []\n", "    \n", "    for lon, lat, label in known_points:\n", "        d = distance_function(new_point, (lon, lat))\n", "        \n", "        neighbours.append(((lon, lat), label, d))\n", "        \n", "    neighbours = sorted(neighbours, key=itemgetter(2))\n", "    \n", "    return neighbours\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Testing the `distance` function\n", "\n", "Run the cell below to test your function is working as expected."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["known = [(0,0,'A'), (1,1,'A'), (3,4,'B'), (5,4,'B'), (1.5, 1.5,'C')]\n", "new = (2,2)\n", "\n", "assert get_neighbours(new, known, euclidean_distance) == [((1.5, 1.5), 'C', 0.7071067811865476), ((1, 1), 'A', 1.4142135623730951), ((3, 4), 'B', 2.23606797749979), ((0, 0), 'A', 2.8284271247461903), ((5, 4), 'B', 3.605551275463989)]\n", "\n", "assert get_neighbours(new, known, manhattan_distance) == [((1.5, 1.5), 'C', 1.0), ((1, 1), 'A', 2), ((3, 4), 'B', 3), ((0, 0), 'A', 4), ((5, 4), 'B', 5)]\n", "\n", "print('All good!')"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## The `classify` function\n", "\n", "This function should take the output of `get_neighbours` and calculate which class is the most common in the nearest `k` items.\n", "\n", "`collections.Counter` can turn a list of items into a dictionary of item:count.\n", "\n", "`Counter(['A', 'B', 'B'])` gives you `{'A':1, 'B':2}`\n", "\n", "To get the top item, use the `.most_common()` method, which returns a sorted list of item/count pairs. The first item is the most common item."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Demonstration of Counter\n", "from collections import Counter\n", "\n", "some_list = list('AUHSUGQWURBUQWVYFVQFQFQIHIUQBF')\n", "\n", "counts = Counter(some_list)\n", "\n", "print(f\"The counter: {counts}\\n\")\n", "\n", "top_item = counts.most_common()\n", "\n", "print(f'Items sorted by their counts: {top_item}\\n')\n", "print(f'Most common item and its count: {top_item[0]}\\n')\n", "print(f\"Most common item's name: {top_item[0][0]}\\n\")\n", "print(f\"Most common item's count: {top_item[0][1]}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Complete the function below.\n", "# The first part checks to make sure k is a valid value,\n", "# based on the data the function is passed.\n", "\n", "def classify(neighbours_list, k):\n", "    '''\n", "    Inputs:\n", "        neighbours_list : a list of ((lon, lat), label, distance)\n", "        k : the number of neighbours to use for classification\n", "    Returns:\n", "        label : the most commonly observed label of the top k items in neighbours\n", "    '''\n", "    \n", "    if k <= 0:\n", "        raise ValueError('k too low: must be > 0')\n", "    if k > len(neighbours_list):\n", "        raise ValueError('k too high: must be <= the number of neighbours')\n", "    \n", "    # Your code here...\n", "    top_neighbours = neighbours_list[0:k]\n", "    \n", "    top_classes = [neighbour[1] for neighbour in top_neighbours]\n", "    \n", "    counts = Counter(top_classes)\n", "    \n", "    top_item = counts.most_common()[0]\n", "    \n", "    label = top_item[0]\n", "    \n", "    return label\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Testing"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["known = [(0,0,'A'), (1,1,'A'), (3,4,'B'), (5,4,'B'), (1.5, 1.5,'C')]\n", "new = (0.5,0.5)\n", "\n", "neighbours_euc = get_neighbours(new, known, euclidean_distance)\n", "\n", "assert classify(neighbours_euc, 3) == 'A'\n", "\n", "neighbours_man = get_neighbours(new, known, manhattan_distance)\n", "\n", "assert classify(neighbours_man, 3) == 'A'\n", "\n", "print('All good!')"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Putting it all together\n", "\n", "Putting this all together, we can now do KNN classification with the following workflow.\n", "\n", "(It's not as elegant as `sklearn` but it works!)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["train_data = [(1,1,'A'), (1,1.5,'A'), (2,2,'A'), (4,5,'B'), (8,4,'B')]\n", "\n", "new_data = [(0.5, 0.5), (9.3,9.0), (5.1,5.6)]\n", "\n", "for point in new_data:\n", "    neighbours = get_neighbours(point, train_data, euclidean_distance)\n", "    label = classify(neighbours, k=3)\n", "    \n", "    print(f\"Item: {point} Prediction: {label}\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## KNN: assigning towns to regions\n", "\n", "Imagine you are a government official, in charge of assigning new towns to a region.\n", "\n", "This job was easy until someone lost all the boundary maps. Now, nobody knows what the geographical properties of each region actually are.\n", "\n", "Fortunately, you **do** know which region all the existing towns are in, as well as the coordinates of all those towns.\n", "\n", "You decide that the best approach is assign new towns to a region based on which other towns it is nearest to."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import pandas as pd\n", "\n", "data = pd.read_csv('data/uk-towns-sample.csv')\n", "\n", "train_data = data[data['kind'] == 'Train']\n", "test_data = data[data['kind'] == 'Test']\n", "\n", "train_data.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Get the data into the expected format for our functions\n", "\n", "train_data = list(zip(train_data.lon.tolist(), train_data.lat.tolist(), train_data.region.tolist()))\n", "test_data = list(zip(test_data.lon.tolist(), test_data.lat.tolist(), test_data.region.tolist()))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["With your KNN functions, write a loop that goes through the first 3 items of `test_data` and predicts a region for them.\n", "\n", "Get predictions for Euclidean distance, at `k=3` and `k=300`.\n", "\n", "What do you observe? What happens when `k` is large? Why do you think that is?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "\n", "for lon, lat, region in test_data[0:3]:\n", "    \n", "    point = (lon, lat)\n", "    \n", "    neighbours = get_neighbours(point, train_data, euclidean_distance)\n", "    label_3 = classify(neighbours, k=3)\n", "    label_300 = classify(neighbours, k=300)\n", "\n", "    print(f\"Expected region: {region}\")\n", "    print(f\"\\t Prediction \\n\\t k=3: {label_3} \\n\\t k=300 {label_300}\\n\")\n", "\n", "    \n", "print('Accuracy is perfect at k=3 but at k=300 it is zero. This is likely because using too many neighbours simply selects the region containing the most towns.')\n", "    \n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Evaluating the predictions\n", "\n", "The cell below will use the test data to evaluate performance at different `k` values. As you can see, setting `k` too high or too low hurts performance on unseen data.\n", "\n", "Note that our implementation takes around 1 second to run all these predictions."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["%%time\n", "\n", "from sklearn.metrics import f1_score\n", "\n", "for k in [1,3,5,10,20,100,300]:\n", "    predictions = []\n", "    expected = []\n", "\n", "    for lon, lat, region in test_data:\n", "\n", "        point = (lon, lat)\n", "\n", "        neighbours = get_neighbours(point, train_data, euclidean_distance)\n", "        label = classify(neighbours, k=k)\n", "\n", "        predictions.append(label)\n", "        expected.append(region)\n", "\n", "    print(f'F1 score for k={k}\\t: {f1_score(expected, predictions, zero_division=0, average=\"weighted\"):.3f}')\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Recreating the map\n", "\n", "How could you use the data and the model to create a new map, showing the regional boundaries? How would it compare to the \"true\" map?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your thoughts below...\n", "print(\"You could classify every single lon/lat point on a map grid. This should show the structure of the boundaries.\")\n", "print(\"It should look fairly similar to the original map, but the outlines of the regions are not likely to be very high resolution.\")\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Running the cell below will do the following\n", "\n", "1. Determine the min/max lon/lat points\n", "2. Use these to create a grid of (x,y) points\n", "3. Classify all these points using the KNN model\n", "4. Colour each point according to prediction\n", "5. Plot predictions and towns\n", "\n", "(It will take maybe a minute or two, because the KNN prediction process is not optimised! But there is a little progress bar, to show you that things are happening.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import classification_report\n", "import matplotlib.pyplot as plt\n", "from matplotlib.colors import ListedColormap\n", "import numpy as np\n", "from tqdm.notebook import tqdm\n", "\n", "colour_vals = np.linspace(0,1,96)\n", "np.random.seed(5)\n", "np.random.shuffle(colour_vals)\n", "cmap = plt.cm.colors.ListedColormap(plt.cm.hsv(colour_vals))\n", "\n", "\n", "def plot(k):\n", "    # Make coords for predicting\n", "    min_lon, max_lon = min(train_data, key=itemgetter(0))[0], max(train_data, key=itemgetter(0))[0]\n", "    min_lat, max_lat = min(train_data, key=itemgetter(1))[1], max(train_data, key=itemgetter(1))[1]\n", "\n", "    xx, yy = np.meshgrid(np.arange(min_lon, max_lon, 0.1),\n", "                         np.arange(min_lat, max_lat, 0.1))\n", "\n", "    # Do the predictions\n", "    grid_predictions = []\n", "    for point in tqdm(zip(xx.flatten(), yy.flatten()), total=len(xx.flatten())):\n", "        neighbours = get_neighbours(point, train_data, euclidean_distance)\n", "        label = classify(neighbours, k=k)\n", "        grid_predictions.append(label)\n", "    grid_predictions = np.array(grid_predictions).reshape(xx.shape)\n", "\n", "    # Need to be able to convert between text and numerical labels, to plot properly\n", "    p_to_i = {p:i for i, p in enumerate(set([i[2] for i in train_data]))}\n", "    i_to_p = {i:p for i, p in enumerate(set([i[2] for i in train_data]))}\n", "\n", "    \n", "    # Plot each grid prediction\n", "    plt.pcolormesh(xx, yy, np.array([p_to_i[p] for p in grid_predictions.flatten()]).reshape(xx.shape), cmap=cmap, shading='auto')\n", "\n", "    # Plot the towns, coloured by region\n", "    x_vals = [i[0] for i in train_data]\n", "    y_vals = [i[1] for i in train_data]\n", "    colors = [p_to_i[i[2]] for i in train_data]\n", "    plt.scatter(x_vals, y_vals, c=colors, cmap=cmap, edgecolor='k', s=30)\n", "    plt.xlim(xx.min(), xx.max());\n", "    plt.ylim(yy.min(), yy.max());\n", "    plt.title(f\"k={k}\")\n", "    \n", "    # Output a classification report\n", "\n", "f,a = plt.subplots(1,3, figsize=(36,18));\n", "\n", "for i, k in enumerate([5,10,20]):\n", "    plt.sca(a[i])\n", "    plot(k)   "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Interpreting the boundaries\n", "\n", "In the three plots, we used a `k` of 3, 10 and 20. What do you observe in terms of the decision boundaries?\n", "\n", "What problems do you anticipate in terms of using this as a map of the country's regional boundaries?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your thoughts below...\n", "\n", "print('When k is large, a lot of the points are misclassified. You can tell this from the dot colour not matching the area colour.')\n", "print('You can see a lot of misclassified items where boundaries meet, as k gets bigger. Regions in Northern Ireland are especially poorly delineated.')\n", "print('This would make a decent map if it were able to tell the difference between land and sea! You can see the layout of the UK in the dots, but the model has no conception of where it is possible for towns to go.')\n", "print('One way around this would be to only generate predictions for coordinates we know are on land.')\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# KNN: sklearn\n", "\n", "Now let's compare our implementation to that in `sklearn`. Run the cell below. You'll see it only takes 100ms, compared to 1s for ours!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["%%time\n", "\n", "from sklearn.neighbors import KNeighborsClassifier\n", "\n", "data = pd.read_csv('data/uk-towns-sample.csv')\n", "\n", "train_data = data[data.kind == 'Train']\n", "test_data = data[data.kind == 'Test']\n", "\n", "for our_score, k in zip([0.898,0.912,0.891,0.846,0.772,0.382,0.095], [1,3,5,10,20,100,300]):\n", "\n", "    knn = KNeighborsClassifier(n_neighbors=k)\n", "\n", "    knn.fit(train_data[['lon', 'lat']], train_data['region'])\n", "    sklearn_score = knn.score(test_data[['lon', 'lat']], test_data['region'])\n", "    \n", "    print(f\"k={k}\")\n", "    print(f\"sklearn F1 score: {sklearn_score:.3f}\")\n", "    print(f\"Our F1 score: {our_score}\")\n", "    if our_score - sklearn_score > 0:\n", "        print(f'Ours was better by {our_score - sklearn_score:.3f}\\n\\n')\n", "    else:\n", "        print(f'Sklearn was better by {our_score - sklearn_score:.3f}\\n\\n')"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# The Wisconsin Breast Cancer Data Set\n", "\n", "You should now feel very familiar with the KNN model - how it learns from data to construct decision boundaries, the impact of $k$, and how predictions are made.\n", "\n", "In this next part of the practical, we will work with a real dataset of medical data. The features are generated from images of masses taken from breast tissue. The outcome variable is whether the mass is malignant or benign. More information can be found [here](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29).\n", "\n", "We will use a train/test split to explore the impact of the $k$ on performance, looking at the trade-off between bias and variance. We will also look at how the KNN model is sensitive to the values of the features you use.\n", "\n", "First, load the data into a DataFrame and assign the features to `X` and the `diagnosis` variable to `y`.\n", "\n", "Look at the distribution of benign (`y==0`) and malignant (`y==1`). What do you notice?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import pandas as pd\n", "\n", "data = pd.read_csv('data/wisconsin_data.csv')\n", "\n", "\n", "X = data.drop('diagnosis', axis=1)\n", "y = data['diagnosis']\n", "\n", "print(y.value_counts(normalize=True))\n", "\n", "\n", "# Your thoughts here...\n", "\n", "print('The ratio of benign:malginant is skewed heavily towards benign.')\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Creating a test/train split\n", "\n", "In the slides, we briefly mentioned that one way to set the hyper-parameter `k` is to:\n", "   1. Split our data into training and test data\n", "   2. Train multiple models with different values of `k` and evaluate their performance on the test data\n", "   3. Select the model that minimises the train and test error\n", "\n", "This will be covered in more detail later. For now, however, we can just use the [`sklearn.model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to split our data into these training and testing samples for us.\n", "\n", "We should note that the dataset contains roughly twice as many examples of benign compared to malignant outcomes. This means we need to be careful when creating the test/train split: the ratios within each split should be similar to that of the whole dataset. Otherwise, any evaluation we do might not report accurate results.\n", "\n", "You could do this manually but the `sklearn.model_selection.train_test_split` function can handle it all. It takes in data `X` and `y` and splits it into `X_train`, `X_test`, `y_train` and `y_test`.\n", "\n", "Use this function to split up your data. Make the test set contain around 20% of the data using `test_size=0.2`.\n", "\n", "Set `stratify=y` to ensure the ratio of classes in `y_train`/`y_test` is preserved and check this is the case."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5, test_size=0.2, stratify=y)\n", "\n", "print(y_train.value_counts(normalize=True))\n", "\n", "print(y_test.value_counts(normalize=True))\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Training an initial model\n", "\n", "Instantiate a `sklearn.neighbors.KNeighborsClassifier` model with default parameters (`k=5`), named `knn`.\n", "\n", "Use the `.fit()` method to train it on `X_train` and `y_train`.\n", "\n", "Use the `.score()` method of the trained model to find its accuracy using the test set `X_test` and `y_test`"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.neighbors import KNeighborsClassifier\n", "\n", "# Your code here...\n", "\n", "knn = KNeighborsClassifier()\n", "\n", "knn.fit(X_train, y_train)\n", "\n", "print(f\"Accuracy: {knn.score(X_test, y_test):.3f}\")\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Pre-processing data for optimal KNN performance\n", "\n", "Because KNN uses the concept of **distance** between points to determine similarity, if the scales of features differ wildly then it can cause issues.\n", "\n", "For example, if one feature is in the range 1-5, but another in the 400 to 290000, then the Euclidean spaces represented by these features are very far apart. Distances between two points based on these features will be extreme.\n", "\n", "The min/max of `X_train` shows this:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["X_train.describe()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["It is not the raw feature values that matter, but their size relative to each other. Therefore, we can scale all features to be within the same range. This is normally 0 to 1.\n", "\n", "This can be easily done using `sklearn.preprocessing.MinMaxScaler`:\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.preprocessing import MinMaxScaler\n", "\n", "scaler = MinMaxScaler()\n", "\n", "X_train_scaled = scaler.fit_transform(X_train)\n", "X_test_scaled = scaler.transform(X_test)\n", "\n", "print(f\"New min: {X_train_scaled.min():.3f} New max: {X_train_scaled.max():.3f}\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Train and score a new KNN as before, named `knn_scaled`, using the new scaled data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "\n", "knn_scaled = KNeighborsClassifier()\n", "\n", "knn_scaled.fit(X_train_scaled, y_train)\n", "\n", "print(f\"Accuracy: {knn_scaled.score(X_test_scaled, y_test):.3f}\")\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Accuracy has improved quite a bit!"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## The impact of `k` on accuracy\n", "\n", "Recall that the value of $k$ in KNN impacts model **bias** (how well the model captures relevant relations in the features) and model **variance** (how sensitive the model is to noise in the features).\n", "\n", "A KNN model is most prone to overfitting when $k$ is low, and underfitting when $k$ is high.\n", "\n", "For values of $k$ in `range(1, 400)`, create a model using that value of $k$ and `.fit()` it using `X_train_scaled` and `y_train`.\n", "\n", "Use the `.score()` method on the train data (`X_train_scaled` and `y_train`) and store the resulting score in `accs_train`.\n", "\n", "Use the `.score()` method on the test data (`X_test_scaled` and `y_test`) and store the resulting score in `accs_test`.\n", "\n", "(This might take 30 seconds or so to complete!)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["accs_train = []\n", "accs_test = []\n", "\n", "#Your code here...\n", "\n", "for k in range(1,400):\n", "    knn = KNeighborsClassifier(n_neighbors=k)\n", "\n", "    knn.fit(X_train_scaled, y_train)\n", "    \n", "    score_train = knn.score(X_train_scaled, y_train)\n", "    accs_train.append(score_train)\n", "    \n", "    score_test = knn.score(X_test_scaled, y_test)\n", "    accs_test.append(score_test)\n", "    \n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The cell below will plot the results for you, of accuracy at various values of $k$. What do you observe?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import seaborn as sns\n", "\n", "# Make plot a readable size\n", "sns.set(rc={'figure.figsize':(12, 8)})\n", "# Convert data to DataFrame\n", "df = pd.DataFrame(zip(accs_train, accs_test, range(1,400)), columns=['train data (seen)', 'test data (unseen)', 'k'])\n", "# Melt to long format for easy plotting\n", "df = df.melt(var_name='Evaluated against:', id_vars='k', value_name='Accuracy')\n", "# Plot dataframe\n", "g = sns.lineplot(data=df, hue='Evaluated against:', x='k', y='Accuracy')\n", "\n", "\n", "# Your thoughts here...\n", "print('Accuracy is generally quite high until around k=150.')\n", "print('At lower k, train data accuracy is generally higher than test data accuracy - overfitting.')\n", "print('At higher k up to around 250, test data accuracy is higher than train data. Beyond this is underfitting.')\n", "print('Performance plateaus at around 0.63 - this is the percentage of the most-common label in the dataset!')\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Evaluating KNN: true/false positives/negatives\n", "\n", "The `.score()` method used the accuracy metric - the number of correct classifications out of the total classifications made.\n", "\n", "This doesn't really give the best picture of model performance, though. As you saw when $k>350$, accuracy flatlines at 0.63. This is because the model is using almost ALL the other data points for classification and around 63% of them are in the benign class.\n", "\n", "A more useful approach is to see how the model performed for each individual class. Especially for health-related tasks, we are interested in:\n", "\n", "* True Positives (TP): Cases in which the tissue is malignant and it was predicted as such.\n", "* True Negatives (TN): Cases in which the tissue is benign (not malignant) and it was predicted as such.\n", "* False Positives (FP): Cases in which the tissue is benign (not malignant) and it was predicted as malignant. (This is often called Type I error.)\n", "* False Negatives (FN): Cases in which the tissue is malignant and it was predicted as benign. (This is often called Type II error.)\n", "\n", "A confusion matrix can show this and can be computed using `pandas.crosstab` then visualised with `seaborn.heatmap`.\n", "\n", "The cell below will do this for you. What do you observe?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "\n", "# Make readable size\n", "sns.set(rc={'figure.figsize':(18, 6)})\n", "# Get 4 new blank plots in a row \n", "fig, axes = plt.subplots(1,4)\n", "\n", "# Iterate through a few values of k\n", "for e, k in enumerate([50,100,200,350]):\n", "    # Make model\n", "    knn = KNeighborsClassifier(n_neighbors=k)\n", "    # Train on training data\n", "    knn.fit(X_train_scaled, y_train)\n", "    # Get predictions of test data\n", "    y_pred = knn.predict(X_test_scaled)\n", "\n", "    # Make the confusion matrix. Normalise the cells to show percentages overall\n", "    cm = pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], normalize=True)\n", "    \n", "    # Plot confusion matrix, one on each of the blank axes. \n", "    g = sns.heatmap(data=cm, cmap='Blues', square=True, annot=True, ax=axes[e], cbar=False)\n", "    \n", "    # Label them so it's clear which is which\n", "    g.set_title(f\"k = {k}\")\n", "\n", "    \n", "# Your thoughts here...\n", "\n", "print('The top left/bottom right diagonal shows the True Negatives and True Positives.')\n", "print('The top right/bottom left diagonal shows the False Negatives and False Positives.')\n", "print('TN increases with k, until the model is always predicting 0.')\n", "print('TP decreases with k, as the model overfits to the most common class, which is 0.')\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Evaluating KNN: precision, recall, F1 score\n", "\n", "True/false positives/negatives can be combined to make new metrics, to give a more concise understanding of how the model is performing.\n", "\n", "* Precision = TP/TP+FP\n", "    * Ratio of correctly predicted positive observations to the total predicted positive observations\n", "* Recall = TP/TP+FN\n", "    * Ratio of correctly predicted positive observations to all of the observations in that class\n", "* F1 Score = 2*(Recall Precision) / (Recall + Precision)\n", "    * Weighted average of Precision and Recall\n", "    \n", "`sklearn.metrics.classification_report` can provide a nice summary of all of these metrics, per class.\n", "\n", "For values of `k` in `[50,100,200,350]`, train and fit a new model on the scaled training data.\n", "\n", "Use the model's `.predict()` method with the scaled test data. Store as `y_pred`.\n", "\n", "Use `classification_report(y_test, y_pred, zero_division=0)` to calculate metrics for the model and print them out.\n", "\n", "(Note: `zero_division=0` will prevent an error from popping up when precision or recall equal 0.)\n", "\n", "What do you observe?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import classification_report\n", "\n", "\n", "#Your code and thoughts below...\n", "    \n", "for k in [50,100,200,350]:\n", "    knn = KNeighborsClassifier(n_neighbors=k)\n", "    knn.fit(X_train_scaled, y_train)\n", "    y_pred = knn.predict(X_test_scaled)\n", "    \n", "    print(f\"k = {k}\")\n", "    \n", "    print(classification_report(y_test, y_pred, zero_division=0))\n", "    \n", "print('The results follow the trend of lower accuracy as k increases.')\n", "print('They give a more detailed picture per class, however.')\n", "print('You can see how performance for the most common class (benign/0) is always very high.')\n", "print('This highlights the importance of using the right metrics for the task!')\n", "    \n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Conclusion\n", "\n", "In this practical, you implemented and explored supervised algorithms for discriminative classification. You implemented k-nearest neighbours yourself, but also used `sklearn`'s (much faster) version of it.\n", "\n", "If you want to extend the work here, you could explore how the different distance measures impacts KNN performance - can you generate a better boundary map?\n", "\n", "You could also look more into the evaluation metrics for these classifiers. See [the sklearn documentation](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) for a range of classification metrics."]}]}