{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 4, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Discriminative Classifiers: K-Nearest Neighbours\n", "\n", "### The Wisconsin Breast Cancer Data Set\n", "\n", "In this practical, we work with a real dataset of medical data. The features are generated from images of masses taken from breast tissue. The outcome variable is whether the mass is malignant or benign. More information can be found [here](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29).\n", "\n", "We use a train/test split to explore the impact of the $k$ on performance, looking at the trade-off between bias and variance. We also look at how the KNN model is sensitive to the values of the features you use."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["First, load the data into a DataFrame and "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import pandas as pd\n", "\n", "# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Assign the features to `X` and the `diagnosis` variable to `y`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Look at the distribution of benign (`y==0`) and malignant (`y==1`). What do you notice?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code and thoughts here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Creating a test/train split\n", "\n", "In the slides, we briefly mentioned that one way to set the hyper-parameter `k` is to:\n", "   1. Split our data into training and test data\n", "   2. Train multiple models with different values of `k` and evaluate their performance on the test data\n", "   3. Select the model that minimises the train and test error\n", "\n", "To perform this split, we can use the `train_test_split` function from the `sklearn.model_selection` module: [`sklearn.model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n", "\n", "It's important to note that the dataset contains roughly twice as many examples of benign compared to malignant outcomes. Therefore, we need to ensure that the ratios of benign and malignant outcomes in the training and test sets are similar to those in the original dataset. This can be done by setting the `stratify` parameter to `y`, which ensures that the classes are distributed equally among the training and test sets.\n", "\n", "To create a test set containing around 20% of the data, we can set `test_size=0.2` in the `train_test_split` function. Once we have split the data, we should check that the ratio of classes in `y_train` and `y_test` is preserved.\n", "\n", "Set `stratify=y` to ensure the ratio of classes in `y_train`/`y_test` is preserved and check this is the case."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "# X_train, X_test, y_train, y_test = ...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Training an initial model\n", "\n", "Instantiate a `sklearn.neighbors.KNeighborsClassifier` model with default parameters (`k=5`), named `knn`.\n", "\n", "Use the `.fit()` method to train it on `X_train` and `y_train`.\n", "\n", "Use the `.score()` method of the trained model to find its accuracy using the test set `X_test` and `y_test`"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.neighbors import KNeighborsClassifier\n", "\n", "# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Pre-processing data for optimal KNN performance\n", "\n", "Because KNN uses the concept of **distance** between points to determine similarity, if the scales of features differ wildly then it can cause issues.\n", "\n", "For example, if one feature is in the range 1-5, but another in the 400 to 290000, then the Euclidean spaces represented by these features are very far apart. Distances between two points based on these features will be extreme.\n", "\n", "The min/max of `X_train` shows this:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["X_train.describe()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Rather than focusing on the absolute values of the features, we want to consider their size relative to each other. Scaling all the features to the same range, such as between 0 and 1, is a common way to achieve this.\n", "\n", "This can be easily done using `sklearn.preprocessing.MinMaxScaler`:\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.preprocessing import MinMaxScaler\n", "\n", "# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Train and score a new KNN as before, named `knn_scaled`, using the new scaled data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Accuracy has improved quite a bit!"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## The impact of `k` on accuracy\n", "\n", "Recall that the value of $k$ in KNN impacts model **bias** (how well the model captures relevant relations in the features) and model **variance** (how sensitive the model is to noise in the features).\n", "\n", "A KNN model is most prone to overfitting when $k$ is low, and underfitting when $k$ is high.\n", "\n", "For values of $k$ in `range(1, 400)`, create a model using that value of $k$ and `.fit()` it using `X_train_scaled` and `y_train`.\n", "\n", "Use the `.score()` method on the train data (`X_train_scaled` and `y_train`) and store the resulting score in `accs_train`.\n", "\n", "Use the `.score()` method on the test data (`X_test_scaled` and `y_test`) and store the resulting score in `accs_test`.\n", "\n", "(This might take 30 seconds or so to complete!)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["accs_train = []\n", "accs_test = []\n", "\n", "#Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The cell below plots the accuracy at various values of $k$. What do you observe?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import seaborn as sns\n", "\n", "# Make plot a readable size\n", "sns.set(rc={'figure.figsize':(12, 8)})\n", "# Convert data to DataFrame\n", "df = pd.DataFrame(zip(accs_train, accs_test, range(1,400)), columns=['train data (seen)', 'test data (unseen)', 'k'])\n", "# Melt to long format for easy plotting\n", "df = df.melt(var_name='Evaluated against:', id_vars='k', value_name='Accuracy')\n", "# Plot dataframe\n", "g = sns.lineplot(data=df, hue='Evaluated against:', x='k', y='Accuracy')\n", "\n", "\n", "# Your thoughts here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Evaluating KNN: true/false positives/negatives\n", "\n", "The `.score()` method used the accuracy metric - the number of correct classifications out of the total classifications made.\n", "\n", "This doesn't really give the best picture of model performance, though. As you saw when $k>350$, accuracy flatlines at 0.63. This is because the model is using almost ALL the other data points for classification and around 63% of them are in the benign class.\n", "\n", "A more useful approach is to see how the model performed for each individual class. Especially for health-related tasks, we are interested in:\n", "\n", "* True Positives (TP): Cases in which the tissue is malignant and it was predicted as such.\n", "* True Negatives (TN): Cases in which the tissue is benign (not malignant) and it was predicted as such.\n", "* False Positives (FP): Cases in which the tissue is benign (not malignant) and it was predicted as malignant. (This is often called Type I error.)\n", "* False Negatives (FN): Cases in which the tissue is malignant and it was predicted as benign. (This is often called Type II error.)\n", "\n", "A confusion matrix can show this and can be computed using `pandas.crosstab` then visualised with `seaborn.heatmap`.\n", "\n", "The cell below will do this for you. What do you observe?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "\n", "# Make readable size\n", "sns.set(rc={'figure.figsize':(18, 6)})\n", "# Get 4 new blank plots in a row \n", "fig, axes = plt.subplots(1,4)\n", "\n", "# Iterate through a few values of k\n", "for e, k in enumerate([50,100,200,350]):\n", "    # Make model\n", "    knn = KNeighborsClassifier(n_neighbors=k)\n", "    # Train on training data\n", "    knn.fit(X_train_scaled, y_train)\n", "    # Get predictions of test data\n", "    y_pred = knn.predict(X_test_scaled)\n", "\n", "    # Make the confusion matrix. Normalise the cells to show percentages overall\n", "    cm = pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], normalize=True)\n", "    \n", "    # Plot confusion matrix, one on each of the blank axes. \n", "    g = sns.heatmap(data=cm, cmap='Blues', square=True, annot=True, ax=axes[e], cbar=False)\n", "    \n", "    # Label them so it's clear which is which\n", "    g.set_title(f\"k = {k}\")\n", "\n", "    \n", "# Your thoughts here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Evaluating KNN: precision, recall, F1 score\n", "\n", "True/false positives/negatives can be combined to make new metrics, to give a more concise understanding of how the model is performing.\n", "\n", "* Precision = TP/TP+FP\n", "    * Ratio of correctly predicted positive observations to the total predicted positive observations\n", "* Recall = TP/TP+FN\n", "    * Ratio of correctly predicted positive observations to all of the observations in that class\n", "* F1 Score = 2*(Recall Precision) / (Recall + Precision)\n", "    * Weighted average of Precision and Recall\n", "    \n", "`sklearn.metrics.classification_report` can provide a nice summary of all of these metrics, per class.\n", "\n", "For values of `k` in `[50,100,200,350]`, train and fit a new model on the scaled training data.\n", "\n", "Use the model's `.predict()` method with the scaled test data. Store as `y_pred`.\n", "\n", "Use `classification_report(y_test, y_pred, zero_division=0)` to calculate metrics for the model and print them out.\n", "\n", "(Note: `zero_division=0` will prevent an error from popping up when precision or recall equal 0.)\n", "\n", "What do you observe?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import classification_report\n", "\n", "\n", "#Your code and thoughts below...\n"]}]}