{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 4, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Logistic Regression - Advanced\n", "\n", "In the lesson slides for this learning unit, we looked at:\n", "\n", "* The form and function of the logistic regression model\n", "* Use of binary logistic regression to classify a single variable\n", "* Evaluating a trained model with the accuracy score\n", "* Interpreting model predictions via probability scores\n", "\n", "In this practical we will implement a simple multivariate logistic regression, before moving on to letting `sklearn` do all the work."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Part 1: implementing logistic regression\n", "\n", "The matrix notation form of the logistic model is $$ y = \\sigma(Xw)$$\n", "\n", "Where $X$ is a matrix of observations x features and $w$ are the model parameters.\n", "\n", "Key components are:\n", "\n", "1. The sigmoid function: $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n", "\n", "2. A model that takes features and parameters, and combines them to make predictions.\n", "\n", "3. The cross entropy function: $$L(w) = -\\frac{1}{N}\\sum_{i=1}^{N}log\\left ( p_i^{y_i}(1-p_i)^{(1-y_i)} \\right )$$\n", "\n", "4. A loss function to combine the above.\n", "\n", "5. An optimiser that can solve $y = \\sigma(Xw)$ for $w$"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## The sigmoid function\n", "\n", "First, implement the `sigmoid` function using numpy. It should take in an array and apply the sigmoid function to each item in it. It should return an array of the transformed values."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import numpy as np\n", "\n", "# Your code here\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## The logistic model\n", "\n", "The `logistic_model` function should implement $y = \\sigma(Xw)$.\n", "\n", "Inputs:\n", "\n", "1. `X` - a matrix of observations x features\n", "2. `w` - a vector of model parameters (a bias, plus one component per feature in X)\n", "\n", "Should return `p`, the model predictions\n", "\n", "Note: you can multiply one `numpy` array by another using the first array's `.dot()` method."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## The cross-entropy function\n", "\n", "Now, implement the cross-entropy function.\n", "\n", "Inputs:\n", "\n", "1. `truth` - an vector of true class labels ($0$ or $1$)\n", "2. `preds` - a vector of predicted probabilities (between $0$ and $1$), from `logistic_model`\n", "\n", "Should return `loss`, a single float representing the cross-entropy loss."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Loss function\n", "\n", "Using the functions you have built so far, we can create a loss function to be optimised.\n", "\n", "This takes in:\n", "\n", "1. `X` - a matrix of observations x features\n", "2. `w` - a vector of model parameters (a bias, plus one component per feature in X)\n", "3. `y` - a vector of the true class labels"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def loss(w, X, y):\n", "    p = logistic_model(X, w)\n", "    loss = cross_entropy(y, p)\n", "    \n", "    return loss"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Optimiser\n", "\n", "Rather than implement gradient descent or something similar, we'll let `scipy.optimize.minimize` sort this out for us!\n", "\n", "The `minimize` function takes in your loss function, your initial guess at the parameters and the X and y data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from scipy.optimize import minimize"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Testing it all so far\n", "\n", "Below is some fake data: one features (plus our bias term) predicting one of two classes.\n", "\n", "The `minimize` function finds the best values for the bias and weights, which you can see labeled `x` at the end of the output."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["X = np.array([[1, 0.1],\n", "              [1, 0.8],\n", "              [1, 0.3],\n", "              [1, 0.2],\n", "              [1, 0.9]\n", "             ])\n", "\n", "y = np.array([0, 1, 0, 0, 1])\n", "\n", "w = np.array([0, 0])\n", "\n", "results = minimize(loss, w, args=(X, y))\n", "\n", "results"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Checking the curve\n", "\n", "Since we have a simple 2D example, we can use the parameters from `results.x` and plot the learned curve."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import seaborn as sns\n", "\n", "# Get a whole range of x values\n", "x = np.linspace(0, 1, 100).reshape(-1,1)\n", "# Add the weight term\n", "x = np.insert(x, 0, values=1, axis=1)\n", "\n", "# Use the model to get predictions for all of them, using the learned parameters\n", "y_pred = logistic_model(x, results.x)\n", "\n", "# Plot the model\n", "sns.lineplot(x=x[:,1], y=y_pred, lw=4);"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Part 2 : logistic regression in sklearn\n", "\n", "For the rest of this practical, we will use sklearn to create and evaluate the model.\n", "\n", "First, let's look at binary classification - predicting breast cancer from a 30 different measurements."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn import datasets\n", "import pandas as pd\n", "\n", "data = datasets.load_breast_cancer()\n", "\n", "X = pd.DataFrame(data['data'], columns=data['feature_names'])\n", "y = data.target\n", "\n", "X.head()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Instantiate a LogisticRegression model using `sklearn` and name it `model`.\n", "\n", "(You should set `max_iter` to something larger, like 10000, for the model to converge during training.)\n", "\n", "Use its `.fit()` method to learn from the data in `X` and `y`.\n", "\n", "How accurate is the model? You can quickly compute this using the model's `.score()` method and passing it the data in `X` and `y`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\n", "\n", "# Your code here\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Look at the distribution of class labels in the first 500 data points. What do you notice?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Use `model` to get predictions for all the data points in `X`, via its `.predict()` method.\n", "\n", "Use these, and the true classes, to calculate overall precision, recall and F1 score for the model using the imported functions from `sklearn.metrics`. These functions take two arguments: the original `y` values and those predicted using the `.predict()` method."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import precision_score, recall_score, f1_score\n", "\n", "# Your code here\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["These figures are averaged over all the classes. It could be useful to see how the model does on each class. Print a classification report. What do you observe?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import classification_report\n", "\n", "# Your code here\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# sklearn vs DIY\n", "\n", "Let's finish by comparing sklearn to your implementation, in terms of accuracy. What do you make of the results?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import accuracy_score\n", "\n", "X = np.array([[1, 0.1],\n", "              [1, 0.8],\n", "              [1, 0.3],\n", "              [1, 0.2],\n", "              [1, 0.9]\n", "             ])\n", "\n", "y = np.array([0, 1, 0, 0, 1])\n", "\n", "model = LogisticRegression()\n", "model.fit(X, y)\n", "print(f\"sklearn Logistic Regression accuracy: {model.score(X, y)}\")\n", "\n", "y_pred = logistic_model(X, results.x)\n", "print(f\"DIY Logistic Regression accuracy: {accuracy_score(y, y_pred.round())}\")\n", "\n", "# Your thoughts here\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Summary\n", "\n", "In this practical, you implemented the core components of the logistic regression model, before going on to use `sklearn` to do all the work - training the model and also evaluating it using suitable metrics.\n", "\n", "In practice, you wouldn't train a model on the entire dataset. So you could explore the models here a bit more and see how they perform when asked to classify new, totally unseen, data points.\n", "\n", "To go even further, you could implement regularisation (using an L1 or L2 norm) for your DIY model and see if you can match the output of `sklearn`."]}]}