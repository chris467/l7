{"metadata": {"kernelspec": {"display_name": "Python3 (base)", "language": "python", "name": "base"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 4, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Logistic Regression Introduction\n", "\n", "In the lesson slides for this learning unit, we looked at:\n", "\n", "* The form and function of the logistic regression model\n", "* Use of binary logistic regression to classify a single variable\n", "* Evaluating a trained model with the accuracy score\n", "* Interpreting model predictions via probability scores\n", "\n", "In this practical we will focus on using `sklearn` to demonstrate the above points. The focus will be on the simple univariate case, using a single input feature to predict a binary output class."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# The data\n", "\n", "`data.csv` contains 100 observations of some mystery process, with two possible classes for each - 0 or 1.\n", "\n", "There are six features recorded for each observation. These have been normalised so that each column ranges between 0 and 1."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import pandas as pd\n", "\n", "data = pd.read_csv('data/data.csv')\n", "\n", "data.head()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Visualising the data\n", "\n", "Below shows a `seaborn.displot`. Each subplot shows the distribution of a feature's values, across the two classes."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import seaborn as sns\n", "\n", "data_long = data.melt(value_vars=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'], id_vars='class', var_name='feature', value_name='value')\n", "\n", "sns.displot(data=data_long,  hue='class', x='value', kind='kde', col='feature', col_wrap=3, fill=True);"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Logistic regression: model assumptions\n", "\n", "Compared to linear regression, the logistic model has few restrictions/assumptions. However, for the univariate classification task it is useful for the predictive feature to be **linearly seperable** - the distribution of that feature should make it easy to distinguish between the classes.\n", "\n", "Looking at the six features above, which would you choose to predict the `class` variable? What do you think are the strengths/weaknesses of each feature?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Training a logistic regression model: toy example\n", "\n", "This is easy to do with `sklearn`. The steps are:\n", "\n", "1. Instantiate a model from `sklearn.linear_model.LogisticRegression`\n", "2. Use the `.fit()` method to train the model\n", "3. Access model accuracy score through `.score()`\n", "4. Use the `.predict()` method of the trained model to get new predictions\n", "5. Use the `.predict_proba()` method of the trained model to get predictions probabilities\n", "\n", "An example is given below."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import numpy as np\n", "from sklearn.linear_model import LogisticRegression\n", "\n", "x = np.random.random(50)\n", "x = x.reshape(-1, 1)\n", "# When using a single feature in sklearn, it must be reshaped\n", "# from the form [1,2,3,4] to [[1],[2],[3],[4]]\n", "\n", "y = [0] * 25 + [1] * 25\n", "\n", "model = LogisticRegression()\n", "\n", "model.fit(x, y)\n", "\n", "print(f\"Weight for single feature: {model.coef_}\")\n", "print(f\"Accuracy score for model: {model.score(x, y):.5f}\")\n", "\n", "unseen_x = [[0.2], [0.11], [0.8], [0.45]]\n", "\n", "print(\"Predictions:\", [i for i in model.predict(unseen_x)])\n", "print(\"Prob of class 1:\", [i[1] for i in model.predict_proba(unseen_x)])"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The model was trained on random data and unsurprisingly does not perform very well!\n", "\n", "Note how all the probabilities are around 0.5, no matter what the input.\n", "\n", "Also the accuracy for the model is exactly 50% - it is basically at chance because the input data is not at all informative about the two classes we want to predict."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Training a logistic regression model: more realistic\n", "\n", "Now, train six invidual logistic models using each feature `['f1', 'f2', 'f3', 'f4', 'f5', 'f6']` from `data`. Remember you will need to convert the dataframe column (which is a `pandas.Series` object) to a numpy array and reshape it, because it only contains a single feature."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Examining logistic regression models\n", "\n", "For each model, get the probability of class 1 for a range of values of `x` provided in `new_x`.\n", "\n", "Append each of these results to the list `probs`.\n", "\n", "(Note: `.predict_proba()` returns a list of probabilities for both classes, for each input in `new_x`. You want the one at position `[1]` in each item in that list!) "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["new_x = [[i] for i in np.linspace(0, 1, 100)]\n", "\n", "probs = []\n", "\n", "# Your code here\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The code below will plot all the values of `x` between 0 and 1, and each model's predicted `y`. The line represents the learned model.\n", "\n", "Also shown are the distribution of the classes for that feature.\n", "\n", "What do you observe?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "sns.set(rc={'figure.figsize':(12, 8)})\n", "\n", "df = pd.DataFrame(probs).T\n", "df.columns = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6']\n", "\n", "fig, axes = plt.subplots(2,3, sharex=False)\n", "\n", "for a, f in enumerate(['f1', 'f2', 'f3', 'f4', 'f5', 'f6']):\n", "    \n", "    x = data[f] * 100\n", "    y = data['class']\n", "    \n", "    sns.lineplot(data=df[f], lw=4, ax=axes.flatten()[a], color='g')\n", "    sns.scatterplot(x=x, y=y, ax=axes.flatten()[a], hue=y, palette=['b', 'r'] )\n", "\n", "# Your code here\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Summary\n", "\n", "In this practical, we explored logistic regression and its implementation in `sklearn` - how to fit a model from data, get predictions, access prediction probabilities. We also examined how the relationship between the input features and the classes we want to predict impacts the model and its performance."]}]}