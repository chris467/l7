{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Model Validation\n", "\n", "Main goals:\n", "\n", "* Use train/test/validation sets\n", "* Use cross-validation\n", "* Tune hyperparameters\n", "* Visualise results\n", "\n", "We will work with `sklearn` and the usual data science libraries that we import now:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import seaborn as sns\n", "import pandas as pd\n", "from matplotlib import pyplot as plt\n", "import sklearn"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Spam or not spam?\n", "\n", "The `spam.csv` file is described [here](https://www.kaggle.com/somesh24/spambase).\n", "\n", "It contains emails classified as spam (class==1) and not spam (class==0). Each email also has some features associated with it, measuring the frequency of certain words and how many capital letters were used in the email.\n", "\n", "Load the file into `pandas` and have a look at it. How many data points are there? Are the spam/not spam classes balanced?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["data = pd.read_csv('data/spam.csv')\n", "\n", "print(f\"There are {data.shape[0]} data points with {data.shape[1]} variables each\")\n", "\n", "print(data['class'].value_counts(normalize=True))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Data partitioning\n", "\n", "You will train a classifier for labeling email as spam or not. The k-nearest neighbours (KNN) classifier will need fine-tuning and you also want to be confident in the model's performance: the available data is slightly imbalanced and also potentially noisy.\n", "\n", "Use `sklearn.model_selection.train_test_split` to first create a train/test split with appropriate proportions.\n", "\n", "Then, split the test partition in half again to create test and validation sets.\n", "\n", "Save the results in the variables `x_train`, `x_test`, `x_val`, `y_train`, `y_test`, `y_val`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "# x should be every column except the one we are trying to predict!\n", "x = data.drop(['class'], axis=1)\n", "# y is *only* the column we want to predict\n", "y = data['class']\n", "\n", "# Your code below...\n", "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8)\n", "\n", "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Check the number of items in each partition: train/test/validation.\n", "\n", "Make sure the x and y parts of each contain the same number of items."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code below...\n", "print(len(x_train), len(y_train))\n", "print(len(x_test), len(y_test))\n", "print(len(x_val), len(y_val))\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["`train_test_split` works on all sorts of data types: lists, `numpy` arrays, `pandas` Series and DataFrames. It returns the same type of data it was given.\n", "\n", "Here, the `x` features are in a DataFrame, the `y` labels are in a Series.\n", "\n", "How many examples of spam/not spam are in each partition?\n", "\n", "(Because the `y` values are 0 and 1, you can use the `.sum()` method on a Series and divide by the length of it to get the proportion of positive classes. Or you can use the `.value_counts(normalize=True)` method - this is better when the values are not 0/1 but something like A/B.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "\n", "# Using .sum()\n", "print(f\"% spam in all data: {data['class'].sum()/len(data):.3f}\")\n", "print(f\"% spam in training data: {y_train.sum()/len(y_train):.3f}\")\n", "print(f\"% spam in test data: {y_test.sum()/len(y_test):.3f}\")\n", "print(f\"% spam in validation data: {y_val.sum()/len(y_val):.3f}\")\n", "\n", "# One example of .value_counts()\n", "\n", "print(\"Training data class proportions:\")\n", "\n", "print(y_train.value_counts(normalize=True))\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Train a model\n", "\n", "Over the following steps, you will set up and train a model, then evaluate it.\n", "\n", "First, create a `KNN` model (`model`) with the default settings."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.neighbors import KNeighborsClassifier as KNN\n", " \n", "# Your code here...\n", "model = KNN()\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Now, use the model's `.fit()` method to learn the training data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "model.fit(x_train, y_train)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["With the trained model, use its `.predict()` method to classify the test set. Save the result in the variable `predictions`.\n", "\n", "Compare the first ten predictions to the expected values.\n", "\n", "(Note: The output of the models is a `numpy` array, but the train/test data from `train_test_split` is in a `pandas.Series` object. You can get the raw values from a Series via the `.values` attribute.)  "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "predictions = model.predict(x_test)\n", "\n", "print(predictions[0:10])\n", "print(y_test[0:10].values)\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Calculate the accuracy and F1 score for the model's predictions."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import accuracy_score, f1_score\n", "\n", "# Your code here...\n", "acc = accuracy_score(y_test, predictions)\n", "f1 = f1_score(y_test, predictions)\n", "              \n", "print(f'Accuracy: {acc}')\n", "print(f'F1: {f1}')\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Use `sklearn.metrics.classification_report` to generate a report on the model's performance.\n", "\n", "How does the model perform for each class? Would you rather a model which is better at classifying emails as spam, or as not spam?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import classification_report\n", "\n", "# Your code here...\n", "print(classification_report(y_test, predictions))\n", "\n", "print('The model performs fairly well in general, but is better at identifying not-spam.')\n", "print('So it will more often let spam through, and will also falsely flag real email as spam.')\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Cross-validation: confidence in model performance\n", "\n", "How confident are you that the observed performance represents the \"true\" performance of the model? Could it just be a fluke of the way the training data was randomly split up? Perhaps the test examples were oddly similar to the training ones, or maybe too different?\n", "\n", "Cross-validation will help you investigate this. We will use a non-exhaustive approach where we train on a few subsets of the data rather than every possible combination. Because cross-validation in `sklearn` automatically partitions data for us, we no longer need to worry about `train_test_split`.\n", "\n", "We are going to use `sklearn.model_selection.cross_validate` to do all the work for us, but it needs a variety of parameters: a model, the data, a cross-validator, and some metrics for scoring.\n", "\n", "First, set up a default `KNN` model as before (named `model`) and also a suitable cross-validator (named `my_cv`) with 5 folds. Given the data you are working with, should you use a stratified approach?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "from sklearn.model_selection import StratifiedKFold\n", "\n", "model = KNN()\n", "my_cv = StratifiedKFold(n_splits=5)\n", "\n", "print(\"Stratified is probably best here, given the class imbalance.\")\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Cross-validation: choosing metrics\n", "\n", "Next, we set up a dictionary of the metrics we want to use. The keys are our names for them, the values are strings which `sklearn` recognises and maps to metrics. Below, we've chosen useful ones for classification."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["my_metrics = {'accuracy': 'accuracy',\n", "              'precision': 'precision_macro',\n", "              'recall': 'recall_macro',\n", "              'f1': 'f1_macro'}"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Cross-validation: putting it all together\n", "\n", "Now, call the `cross_validate` function and pass it your model, the data (the `x` and `y` from the start), the cross-validator, and the metrics dictionary. Save the result in the output `scores`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.model_selection import cross_validate\n", "\n", "# Your code here...\n", "scores = cross_validate(model, x, y, cv=my_cv, scoring=my_metrics)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Cross-validation: the results\n", "\n", "Print the mean and standard deviation of all the results across all the folds. (Tip: the data returned by `cross_validate` is all in `numpy` arrays, which have `.mean()` and `.std()` methods!)\n", "\n", "How does this compare to the single model you trained before, using just one set of data?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "for k, v in scores.items():\n", "    print(f\"Mean {k}: {v.mean():.4f}\")\n", "    print(f\" (stdev = {v.std():.4f})\")\n", "    \n", "print(\"The accuracy here is pretty similar to before, but p/r/f1 are all a little lower.\")\n", "print(\"This suggests we got a bit lucky with the first model.\")\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Reporting performance\n", "\n", "Now that we have some results in the `scores` variable, let's make them a bit more presentable. This makes it easier to share results with others.\n", "\n", "We make a `pandas` DataFrame (`scores_df`) of the results and take a look."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["scores_df = pd.DataFrame(scores)\n", "scores_df"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Each row represents the results of one fold. There are six columns: the first two relate to how long the model took to train and to make predictions, while the others are the evaluation scores.\n", "\n", "These scores are probably of more interest.\n", "\n", "Let's drop the columns we don't need: this can be done by passing a list of column names to the `.drop()` method of a DataFrame and specifying `axis=1`, which is the column axis. `axis=0` is the rows."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["scores_df = scores_df.drop(['fit_time', 'score_time'], axis=1)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Finally, `seaborn` has a `barplot` function which can take this DataFrame and plot it. It will treat each row as an observation of a repeated experiment and automatically calculate and display the standard deviation for each column as error bars.\n", "\n", "We call the `seaborn.barplot`  function and pass `scores_df` to it via the `data=` argument. Also include `ci='sd'` - otherwise `seaborn` will use a 95% confidence intervals instead of using the standard deviation of the data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["sns.barplot(data=scores_df, ci='sd');"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Plots in `seaborn` generally have sensible default values, but they are also highly configurable. This is usually done through a combination of initial argument settings, calling methods after the plot is created, or using `pyplot` to change specific things."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Changing the size\n", "plt.figure(figsize=(10,4))\n", "\n", "# Changing colours, order of the bars, etc\n", "g = sns.barplot(data=scores_df, ci='sd', palette='pastel', capsize=0.1, order=['test_f1', 'test_precision', 'test_recall', 'test_accuracy']);\n", "\n", "# Changing font sizes and adding labels\n", "g.set_title('Model performance', fontsize=22)\n", "g.set_xlabel('Metric')\n", "g.set_ylabel('Score')\n", "g.set_ylim(0.65, 0.9)\n", "\n", "# Rotating labels - useful if they are too long and overlap.\n", "plt.xticks(rotation=45);"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# GridSearch: model tuning and cross-validation\n", "\n", "Now, let's use grid search and cross-validation for fine-tuning the model. The `KNN` model has one hyperparameter we can adjust.\n", "\n", "The GridSearchCV class needs a model, a dictionary of `{hyperparameters:values}` to try, a cross-validation object, and some metrics to use for evaluation.\n", "\n", "First, create a default `KNN` model named `model`, as before. Then, create a `StratifiedKFold` object, with 5 folds, named `my_cv`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.neighbors import KNeighborsClassifier as KNN\n", "from sklearn.model_selection import StratifiedKFold\n", "\n", "# Your code here...\n", "model = KNN()\n", "my_cv = StratifiedKFold(5)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Grid search: hyperparameters\n", "\n", "The `KNN` hyperparameter we want to investigate is called `n_neighbors` - this determines how many of the most similar data points are used to classify new data points.\n", "\n", "Create a dictionary named `hyperparams` with one key, named after the hyperparameter we want to tune. The value for it should be a list with all the settings you want to try for that hyperparameter. Try the values 1 - 10 for this."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "hyperparams = {\"n_neighbors\":[1,2,3,4,5,6,7,8,9,10]}\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Grid search: scoring\n", "\n", "Knowing when the best hyperparameters have been found requires all models to be scored using some metric. `sklearn` will generally set some sensible default for this, but you can also choose your own. The ones available are below."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["print(sorted(sklearn.metrics.SCORERS.keys()))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Now create a dictionary named `my_metrics` of the metrics you want to use to evaluate the models with.\n", "\n", "The keys are the names you use to identify the metrics. The values should be strings which `sklearn` recognises: use at least `accuracy` and `f1_macro`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "my_metrics = {'accuracy': 'accuracy',\n", "              'precision': 'precision_macro',\n", "              'recall': 'recall_macro',\n", "              'f1': 'f1_macro'}\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Grid search: compile and fit\n", "\n", "The final step is to create the `GridSearchCV` object and pass it all the arguments from above.\n", "\n", "(Tip: Since we are using multiple metrics with `GridSearchCV`, we need to set `refit=False` because by default it tries to fit a best model overall once finished, but can only select the best model using a single metric, not multiple.)\n", "\n", "Then, as with most things in `sklearn`, call the `.fit()` method.\n", "\n", "It will chug along for a few seconds."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.model_selection import GridSearchCV\n", "\n", "gridsearch = GridSearchCV(model, hyperparams, cv=my_cv, scoring=my_metrics, refit=False)\n", "\n", "gridsearch.fit(x, y)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Grid search: getting the results\n", "\n", "The various statistics generated are stored in the `cv_results_` attribute of the `GridSearchCV` object you created. It's a big dictionary of data with scores per fold, per hyperparameter.  It also includes information about how long each model took to train and to evaluate.\n", "\n", "Keys are descriptive strings of the data, while the values are arrays containing per-parameter scores and statistics. \n", "\n", "We used 10 settings for 1 hyperparameter, with 5 folds and 2+ metrics. Therefore, for each metric we have 5 items, each containing 10 items. There is also an additional key telling you the rank of each combination, so you can easily find the best one.\n", "\n", "The cell below shows all the contents of this dictionary. Note that the metric names used in `my_metrics` are used as part of the names here - this could be useful for finding what you want."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["gridsearch.cv_results_"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Reporting fine-tuning results\n", "\n", "That's a lot of data! But the most useful ones are those which give the mean scores per metric, per hyperparameter setting. For example `mean_test_f1` and `mean_test_accuracy` if you specified those metrics when setting up `GridSearchCV`.\n", "\n", "Visualising this is a bit more fiddly, because the data is not in the ideal format for passing directly to `seaborn` if we want error bars shown.\n", "\n", "Below, you can see how to create a DataFrame from the results and visualise the key components."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Turn GridSearch results into a dataframe\n", "grid_results = pd.DataFrame(gridsearch.cv_results_)\n", "\n", "# Drop all the junk\n", "grid_results = grid_results.drop(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n", "       'params', 'mean_test_accuracy', 'std_test_accuracy',\n", "       'rank_test_accuracy', 'mean_test_precision', 'std_test_precision',\n", "       'rank_test_precision', 'mean_test_recall', 'std_test_recall', 'rank_test_recall',\n", "       'mean_test_f1', 'std_test_f1', 'rank_test_f1'], axis=1)\n", "\n", "# Melt the wide dataframe into a long one, using the hyperparameter value as the ID.\n", "grid_results = grid_results.melt(id_vars='param_n_neighbors', var_name='metric')\n", "\n", "# Remove digits in the metric names so that seaborn will group them for us\n", "grid_results['metric'] = grid_results['metric'].str.replace(r'\\d_', '_')\n", "# Remove extra junk from metric names \n", "grid_results['metric'] = grid_results['metric'].str.replace(r'split_test_', '')\n", "\n", "# Plot it\n", "sns.catplot(data=grid_results, x='metric', y='value', hue='param_n_neighbors',kind='bar', ci='sd', height=5, aspect=2)\n", "\n", "# Zoom in on the y-axis limit to show detail a bit better \n", "sns.mpl.pyplot.ylim(0.65, 0.85);"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": [" # Getting the best model\n", " \n", "The `GridSearchCV` above doesn't store the best model, because it used multiple scoring metrics. Now, repeat the process but only use `f1_macro` score."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "\n", "model = KNN()\n", "my_cv = StratifiedKFold(5)\n", "hyperparams = {\"n_neighbors\":[1,2,3,4,5,6,7,8,9,10]}\n", "\n", "gridsearch = GridSearchCV(model, hyperparams, cv=my_cv, scoring='f1_macro')\n", "gridsearch.fit(x, y);\n", "\n", "print(gridsearch.best_estimator_)\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Summary\n", "\n", "* Data can be split into train/test/validation sets easily with `train_test_split`\n", "* Cross-validation tools in `sklearn.model_selection` mean you can avoid doing that directly yourself\n", "* Same with doing grid search: `GridSearchCV` handles everything\n", "* Visualise results by reporting mean and std of scores across folds, per hyperparameter combination\n", "* Extract the best model through the `best_estimator_` attribute of your grid search"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Going further\n", "\n", "* With the information gained from the grid search, train a new model with the optimal settings\n", "* Compare the results here to some kind of baseline model, to get a better idea of how good performance is\n", "    * What data should you use to make your baselines?\n", "* Try other models you are familiar with, which have additional hyperparameters you can tune\n", "* Look into `sklearn.pipeline.Pipeline` for streamlining the process of testing multiple models and hyperparameters"]}]}