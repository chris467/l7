{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Classification and Model Selection\n","\n","## Classifying Kickstarter Campaigns\n","\n","Kickstarter is a crowdfunding platform with a community of more than 10 million people comprising of creative, tech enthusiasts who help in bringing new projects to life.\n","\n","Until now, more than $3 billion dollars have been contributed by the members in fueling creative projects.\n","The projects can be literally anything â€“ a device, a game, an app, a film, etc.\n","\n","Kickstarter works on all or nothing basis: a campaign is launched with a certain amount they want to raise, if it doesn't meet its goal, the project owner gets nothing. For example: if a projects's goal is $\\$5000$ and it receives $\\$4999$ in funding, the project won't be a success.\n","\n","If you have a project that you would like to post on Kickstarter now, can you predict whether it will be successfully funded or not? Looking into the dataset, what useful information can you extract from it, which variables are informative for your prediction and can you interpret the model?\n","\n","The goal of this project is to build a classifier to predict whether a project will be successfully funded or not. \n","\n","**ðŸ’¡ You can use any algorithm of your choice.**\n","\n","We will use `sklearn` and the usual data science libraries such as `pandas` and `numpy`."]},{"cell_type":"markdown","metadata":{},"source":["KATE expects your code to define variables with specific names that correspond to certain things we are interested in.\n","\n","KATE will run your notebook from top to bottom and check the latest value of those variables, so make sure you don't overwrite them.\n","\n","* Remember to uncomment the line assigning the variable to your answer and don't change the variable or function names.\n","* Use copies of the original or previous DataFrames to make sure you do not overwrite them by mistake.\n","\n","You will find instructions below about how to define each variable.\n","\n","Once you're happy with your code, upload your notebook to KATE to check your feedback."]},{"cell_type":"markdown","metadata":{},"source":["### Baseline Model\n","\n","In this exercise, we are looking to outperform the performance of a simple `baseline` model. This `baseline` is a simple logistic regression with only two features: `goal_usd` (adjusted goal) and `usa` (whether the campaign happened in the US)\n","\n","The code to build this `baseline` is shown below:\n","\n","```Python\n","from sklearn.linear_model import LogisticRegression\n","\n","# Conduct some custom processing on your training data\n","df[\"usa\"] = df[\"country\"] == \"US\"\n","df[\"goal_usd\"] = df[\"goal\"] * df[\"static_usd_rate\"]\n","\n","df = df[[\"goal_usd\", \"usa\", \"state\"]]\n","\n","# Conduct the same processing on your testing data\n","df_eval[\"usa\"] = df_eval[\"country\"] == \"US\"\n","df_eval[\"goal_usd\"] = df_eval[\"goal\"] * df_eval[\"static_usd_rate\"]\n","\n","df_eval = df_eval[[\"goal_usd\", \"usa\", \"state\"]]\n","\n","X = df.drop([\"state\"], axis=1)\n","y = df[\"state\"]\n","\n","X_eval = df_eval.drop([\"state\"], axis=1)\n","\n","model = LogisticRegression()\n","model.fit(X, y)\n","\n","y_pred = model.predict(X_eval)\n","```"]},{"cell_type":"markdown","metadata":{},"source":["### Our Model\n","\n","To kick things off, let's import and use our favourite data processing library, `pandas`, to retrieve the data that we will use to build a machine learning model.\n","\n","In this assignment, we are going to load in two datasets. The first, `df`, is going to contain all the data we will need to train and test a model. This will include the labels indicating whether or not the project was successfully funded. The second dataset, `df_eval`, is going to contain all the data that our model will be evaluated on by **KATE**. It does not include the labels indicating project success, so can be viewed as held-out test data. \n","\n","We will need to process `df_eval` in exactly the same way as `df`, then use our model trained on `df` to make predictions about `df_eval`. On submission, **KATE** will evaluate these predictions against their labels (which **KATE** has access to).\n","\n","\n","Run the cell below to load the raw data. Note that `pandas` is pretty smart and can read these ZipFiles into regular `DataFrames`:"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(50000, 26)\n","(10000, 26)\n"]}],"source":["import pandas as pd\n","\n","df = pd.read_csv(\"data/kickstarter.gz\")\n","df_eval = pd.read_csv(\"data/kickstarter_eval.gz\")\n","\n","print(df.shape)\n","print(df_eval.shape)"]},{"cell_type":"markdown","metadata":{},"source":["We have also displayed the dimensions of our `df` and `df_eval`. Notice that the `df_eval` is only $10,000$ rows. As mentioned earlier, this will be our test set for submissions to **KATE**.\n","\n","**This means that we cannot train on `df_eval`**\n","\n","\n","The aim of this practical is to:\n","  * Process `df` into an input dataframe `X` and a label dataframe `y`\n","  * Process `df_eval` into an input dataframe `X_eval` (in the same way as we processed `df` into `X`)\n","  * Train a classification model of our choice on `X` and `y`\n","  * Submit our code to KATE, where our model will be evaluated on `X_eval` and `y_eval`\n","\n","Let's kick things off by checking out our `df`. Note that the `state` column contains our success labels:"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>photo</th>\n","      <th>name</th>\n","      <th>blurb</th>\n","      <th>goal</th>\n","      <th>slug</th>\n","      <th>disable_communication</th>\n","      <th>country</th>\n","      <th>currency</th>\n","      <th>currency_symbol</th>\n","      <th>...</th>\n","      <th>location</th>\n","      <th>category</th>\n","      <th>profile</th>\n","      <th>urls</th>\n","      <th>source_url</th>\n","      <th>friends</th>\n","      <th>is_starred</th>\n","      <th>is_backing</th>\n","      <th>permissions</th>\n","      <th>state</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>805910621</td>\n","      <td>{\"small\":\"https://ksr-ugc.imgix.net/assets/012...</td>\n","      <td>DOCUMENTARY FILM titled FROM RAGS TO SPIRITUAL...</td>\n","      <td>A MOVIE ABOUT THE WILLINGNESS TO BREAK FREE FR...</td>\n","      <td>125000.0</td>\n","      <td>movie-made-from-book-titled-from-rags-to-spiri...</td>\n","      <td>False</td>\n","      <td>US</td>\n","      <td>USD</td>\n","      <td>$</td>\n","      <td>...</td>\n","      <td>{\"country\":\"US\",\"urls\":{\"web\":{\"discover\":\"htt...</td>\n","      <td>{\"urls\":{\"web\":{\"discover\":\"http://www.kicksta...</td>\n","      <td>{\"background_image_opacity\":0.8,\"should_show_f...</td>\n","      <td>{\"web\":{\"project\":\"https://www.kickstarter.com...</td>\n","      <td>https://www.kickstarter.com/discover/categorie...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1279627995</td>\n","      <td>{\"small\":\"https://ksr-ugc.imgix.net/assets/011...</td>\n","      <td>American Politics, Policy, Power and Profit</td>\n","      <td>Everything you should know about really big go...</td>\n","      <td>9800.0</td>\n","      <td>american-politics-policy-power-and-profit</td>\n","      <td>False</td>\n","      <td>US</td>\n","      <td>USD</td>\n","      <td>$</td>\n","      <td>...</td>\n","      <td>{\"country\":\"US\",\"urls\":{\"web\":{\"discover\":\"htt...</td>\n","      <td>{\"urls\":{\"web\":{\"discover\":\"http://www.kicksta...</td>\n","      <td>{\"background_image_opacity\":0.8,\"should_show_f...</td>\n","      <td>{\"web\":{\"project\":\"https://www.kickstarter.com...</td>\n","      <td>https://www.kickstarter.com/discover/categorie...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1306016155</td>\n","      <td>{\"small\":\"https://ksr-ugc.imgix.net/assets/013...</td>\n","      <td>Drew Jacobs Official \"Kiss Me\" Music Video</td>\n","      <td>Be a part of the new \"Kiss Me\" Official Music ...</td>\n","      <td>2500.0</td>\n","      <td>drew-jacobs-official-kiss-me-music-video</td>\n","      <td>False</td>\n","      <td>US</td>\n","      <td>USD</td>\n","      <td>$</td>\n","      <td>...</td>\n","      <td>{\"country\":\"US\",\"urls\":{\"web\":{\"discover\":\"htt...</td>\n","      <td>{\"urls\":{\"web\":{\"discover\":\"http://www.kicksta...</td>\n","      <td>{\"background_image_opacity\":0.8,\"should_show_f...</td>\n","      <td>{\"web\":{\"project\":\"https://www.kickstarter.com...</td>\n","      <td>https://www.kickstarter.com/discover/categorie...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>658851276</td>\n","      <td>{\"small\":\"https://ksr-ugc.imgix.net/assets/011...</td>\n","      <td>Still Loved</td>\n","      <td>When their dreams are shattered by the loss of...</td>\n","      <td>10000.0</td>\n","      <td>still-loved</td>\n","      <td>False</td>\n","      <td>GB</td>\n","      <td>GBP</td>\n","      <td>Ã‚Â£</td>\n","      <td>...</td>\n","      <td>{\"country\":\"GB\",\"urls\":{\"web\":{\"discover\":\"htt...</td>\n","      <td>{\"urls\":{\"web\":{\"discover\":\"http://www.kicksta...</td>\n","      <td>{\"background_image_opacity\":0.8,\"should_show_f...</td>\n","      <td>{\"web\":{\"project\":\"https://www.kickstarter.com...</td>\n","      <td>https://www.kickstarter.com/discover/categorie...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1971770539</td>\n","      <td>{\"small\":\"https://ksr-ugc.imgix.net/assets/012...</td>\n","      <td>Nine Blackmon's HATER Film Project</td>\n","      <td>HATER is a mock rock doc about why the Rucker ...</td>\n","      <td>5500.0</td>\n","      <td>nine-blackmons-hater-film-project</td>\n","      <td>False</td>\n","      <td>US</td>\n","      <td>USD</td>\n","      <td>$</td>\n","      <td>...</td>\n","      <td>{\"country\":\"US\",\"urls\":{\"web\":{\"discover\":\"htt...</td>\n","      <td>{\"urls\":{\"web\":{\"discover\":\"http://www.kicksta...</td>\n","      <td>{\"background_image_opacity\":0.8,\"should_show_f...</td>\n","      <td>{\"web\":{\"project\":\"https://www.kickstarter.com...</td>\n","      <td>https://www.kickstarter.com/discover/categorie...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 26 columns</p>\n","</div>"],"text/plain":["           id                                              photo  \\\n","0   805910621  {\"small\":\"https://ksr-ugc.imgix.net/assets/012...   \n","1  1279627995  {\"small\":\"https://ksr-ugc.imgix.net/assets/011...   \n","2  1306016155  {\"small\":\"https://ksr-ugc.imgix.net/assets/013...   \n","3   658851276  {\"small\":\"https://ksr-ugc.imgix.net/assets/011...   \n","4  1971770539  {\"small\":\"https://ksr-ugc.imgix.net/assets/012...   \n","\n","                                                name  \\\n","0  DOCUMENTARY FILM titled FROM RAGS TO SPIRITUAL...   \n","1        American Politics, Policy, Power and Profit   \n","2         Drew Jacobs Official \"Kiss Me\" Music Video   \n","3                                        Still Loved   \n","4                 Nine Blackmon's HATER Film Project   \n","\n","                                               blurb      goal  \\\n","0  A MOVIE ABOUT THE WILLINGNESS TO BREAK FREE FR...  125000.0   \n","1  Everything you should know about really big go...    9800.0   \n","2  Be a part of the new \"Kiss Me\" Official Music ...    2500.0   \n","3  When their dreams are shattered by the loss of...   10000.0   \n","4  HATER is a mock rock doc about why the Rucker ...    5500.0   \n","\n","                                                slug  disable_communication  \\\n","0  movie-made-from-book-titled-from-rags-to-spiri...                  False   \n","1          american-politics-policy-power-and-profit                  False   \n","2           drew-jacobs-official-kiss-me-music-video                  False   \n","3                                        still-loved                  False   \n","4                  nine-blackmons-hater-film-project                  False   \n","\n","  country currency currency_symbol  ...  \\\n","0      US      USD               $  ...   \n","1      US      USD               $  ...   \n","2      US      USD               $  ...   \n","3      GB      GBP              Ã‚Â£  ...   \n","4      US      USD               $  ...   \n","\n","                                            location  \\\n","0  {\"country\":\"US\",\"urls\":{\"web\":{\"discover\":\"htt...   \n","1  {\"country\":\"US\",\"urls\":{\"web\":{\"discover\":\"htt...   \n","2  {\"country\":\"US\",\"urls\":{\"web\":{\"discover\":\"htt...   \n","3  {\"country\":\"GB\",\"urls\":{\"web\":{\"discover\":\"htt...   \n","4  {\"country\":\"US\",\"urls\":{\"web\":{\"discover\":\"htt...   \n","\n","                                            category  \\\n","0  {\"urls\":{\"web\":{\"discover\":\"http://www.kicksta...   \n","1  {\"urls\":{\"web\":{\"discover\":\"http://www.kicksta...   \n","2  {\"urls\":{\"web\":{\"discover\":\"http://www.kicksta...   \n","3  {\"urls\":{\"web\":{\"discover\":\"http://www.kicksta...   \n","4  {\"urls\":{\"web\":{\"discover\":\"http://www.kicksta...   \n","\n","                                             profile  \\\n","0  {\"background_image_opacity\":0.8,\"should_show_f...   \n","1  {\"background_image_opacity\":0.8,\"should_show_f...   \n","2  {\"background_image_opacity\":0.8,\"should_show_f...   \n","3  {\"background_image_opacity\":0.8,\"should_show_f...   \n","4  {\"background_image_opacity\":0.8,\"should_show_f...   \n","\n","                                                urls  \\\n","0  {\"web\":{\"project\":\"https://www.kickstarter.com...   \n","1  {\"web\":{\"project\":\"https://www.kickstarter.com...   \n","2  {\"web\":{\"project\":\"https://www.kickstarter.com...   \n","3  {\"web\":{\"project\":\"https://www.kickstarter.com...   \n","4  {\"web\":{\"project\":\"https://www.kickstarter.com...   \n","\n","                                          source_url friends is_starred  \\\n","0  https://www.kickstarter.com/discover/categorie...     NaN        NaN   \n","1  https://www.kickstarter.com/discover/categorie...     NaN        NaN   \n","2  https://www.kickstarter.com/discover/categorie...     NaN        NaN   \n","3  https://www.kickstarter.com/discover/categorie...     NaN        NaN   \n","4  https://www.kickstarter.com/discover/categorie...     NaN        NaN   \n","\n","  is_backing permissions state  \n","0        NaN         NaN   0.0  \n","1        NaN         NaN   0.0  \n","2        NaN         NaN   1.0  \n","3        NaN         NaN   1.0  \n","4        NaN         NaN   0.0  \n","\n","[5 rows x 26 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"markdown","metadata":{},"source":["In addition to using the `.head()` function, let's also retrieve some more information about our data:"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 50000 entries, 0 to 49999\n","Data columns (total 26 columns):\n"," #   Column                  Non-Null Count  Dtype  \n","---  ------                  --------------  -----  \n"," 0   id                      50000 non-null  int64  \n"," 1   photo                   50000 non-null  object \n"," 2   name                    49998 non-null  object \n"," 3   blurb                   49998 non-null  object \n"," 4   goal                    50000 non-null  float64\n"," 5   slug                    50000 non-null  object \n"," 6   disable_communication   50000 non-null  bool   \n"," 7   country                 50000 non-null  object \n"," 8   currency                50000 non-null  object \n"," 9   currency_symbol         50000 non-null  object \n"," 10  currency_trailing_code  50000 non-null  bool   \n"," 11  deadline                50000 non-null  int64  \n"," 12  created_at              50000 non-null  int64  \n"," 13  launched_at             50000 non-null  int64  \n"," 14  static_usd_rate         50000 non-null  float64\n"," 15  creator                 50000 non-null  object \n"," 16  location                49770 non-null  object \n"," 17  category                50000 non-null  object \n"," 18  profile                 50000 non-null  object \n"," 19  urls                    50000 non-null  object \n"," 20  source_url              50000 non-null  object \n"," 21  friends                 3 non-null      object \n"," 22  is_starred              3 non-null      object \n"," 23  is_backing              3 non-null      object \n"," 24  permissions             3 non-null      object \n"," 25  state                   50000 non-null  float64\n","dtypes: bool(2), float64(3), int64(4), object(17)\n","memory usage: 9.3+ MB\n"]}],"source":["df.info()"]},{"cell_type":"markdown","metadata":{},"source":["From the above, we can see that there are $50,000$ projects in `df` and, apart from a handful of columns, most of our data is not null - what a relief!"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10000 entries, 0 to 9999\n","Data columns (total 26 columns):\n"," #   Column                  Non-Null Count  Dtype  \n","---  ------                  --------------  -----  \n"," 0   id                      10000 non-null  int64  \n"," 1   photo                   10000 non-null  object \n"," 2   name                    10000 non-null  object \n"," 3   blurb                   10000 non-null  object \n"," 4   goal                    10000 non-null  float64\n"," 5   slug                    10000 non-null  object \n"," 6   disable_communication   10000 non-null  bool   \n"," 7   country                 10000 non-null  object \n"," 8   currency                10000 non-null  object \n"," 9   currency_symbol         10000 non-null  object \n"," 10  currency_trailing_code  10000 non-null  bool   \n"," 11  deadline                10000 non-null  int64  \n"," 12  created_at              10000 non-null  int64  \n"," 13  launched_at             10000 non-null  int64  \n"," 14  static_usd_rate         10000 non-null  float64\n"," 15  creator                 10000 non-null  object \n"," 16  location                9959 non-null   object \n"," 17  category                10000 non-null  object \n"," 18  profile                 10000 non-null  object \n"," 19  urls                    10000 non-null  object \n"," 20  source_url              10000 non-null  object \n"," 21  friends                 9 non-null      object \n"," 22  is_starred              9 non-null      object \n"," 23  is_backing              9 non-null      object \n"," 24  permissions             9 non-null      object \n"," 25  state                   0 non-null      float64\n","dtypes: bool(2), float64(3), int64(4), object(17)\n","memory usage: 1.9+ MB\n"]}],"source":["df_eval.info()"]},{"cell_type":"markdown","metadata":{},"source":["Unlike `df`, `df_eval` contains only $10,000$ projects. Also note that the target variable, `state` is null for all entries - as mentioned earlier, this is stored on **KATE** for evaluating our model when we submit our code."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["df labels:       [0. 1.]\n","df_eval labels:  [nan]\n"]}],"source":["print(\"df labels:      \", df.state.unique())\n","print(\"df_eval labels: \", df_eval.state.unique())"]},{"cell_type":"markdown","metadata":{},"source":["**Notes on the dataset**:\n","* The target `state` corresponds to a binary outcome: `0` for failed, `1` for successful. \n","* The variables `'deadline'`, `'created_at'`, `'launched_at'` are stored in Unix time format."]},{"cell_type":"markdown","metadata":{},"source":["## Part 1: Preprocessing the data\n","\n","Although our data is relatively clean, it is not yet in a state where we can train a model. For instance, both `df` and `df_eval` contains columns used for training (features) as well as the target column although this is, of course, null for `df_eval`.\n","\n","What we have to do now is preprocess our data. Specifically, we need to:\n"," - Build a training set: `X` and `y`\n"," - Build an evaluation set: `X_eval`\n"," \n"," <br>\n"," \n","Let's start by extracting the `state` column from `df` into a variable called `y`:\n"," - Create a new variable `y` from `df[\"state\"]`\n"," - Drop the `state` column from `df` and `df_eval`"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["y = df[\"state\"]\n","\n","df.drop(\"state\", axis=1, inplace=True)\n","df_eval.drop(\"state\", axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1 Remove redundant columns\n","\n","After removing the target variable `y` from our input data, we can start processing.\n","\n","Remove all the columns that **you** think are not salient for this classification task. For instance, `id`, `photo`, `slug`, and `disable_communication` are some features which are not likely to be relevant. The choice of which features to retain, however, is yours to make. Remember to remove the same columns from `df` as `df_eval`.\n","\n","You can use the `.drop()` function from `pandas` to remove columns (remember to specify `axis=1`)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Your code here:\n","columns_to_drop = ['id','photo','slug','disable_communication', 'friends','is_starred','is_backing','permissions', 'location','profile','urls','source_url','creator','currency_symbol','name']\n","\n","df = df.drop(columns=columns_to_drop, axis = 1)\n","df_eval = df_eval.drop(columns=columns_to_drop, axis = 1)"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2 Fill null values\n","\n","Looking at the output of `df.info()` above, we can see that some of the columns which we might be interested in as features contain some null values. Null values are, in general, a problem for machine learning models and can cause your code to break. How you choose to deal with them, however, will depend in large part on how you intend to process your data. For instance, if your input data consists of strings that you wish to generate a word count feature from, you can just fill in the null values with empty strings (`\"\"`).\n","\n","Thankfully, `pandas` has a helpful function for dealing with null values: the `.fillna()` function. Remember to do the same to `df` as `df_eval`:"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Your code here:\n","df['blurb'] = df['blurb'].fillna('')\n","df_eval['blurb'] = df_eval['blurb'].fillna('')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.3 Additional Processing\n","\n","In the previous two exercises we have covered the most basic steps in preprocessing: dropping redundant columns and working with null values. However, there is *so* much more that we can do to extract useful information from our data. \n","\n","For instance, the `blurb` column contains unique strings and so, in its current form, isn't a particularly useful feature. Instead, we could create a new feature representing the length of the `blurb`, or the number of words in the `blurb`. \n","\n","Other string-type columns, such as `country`, contain categorical data. As there are a lot of countries represented, we might want to aggregate these into regions (e.g. `Europe`, `Asia`, ...). We can then convert this categorical data into a one-hot encoding using `sklearn`.\n","\n","What we are describing here is what's known as feature engineering and is an art and a science in its own right. \n","\n","Let's start this processing by importing some libraries and functions that can help us create features. Notice that we import the `StandardScaler` from `sklearn`. We can use this function on our numerical data to normalise it, which is an important step in training a machine learning model.\n","\n","**ðŸ’¡ In the following cell, you can use feature engineering to create features that you think might be useful.**\n","\n","<br>\n","\n","You may want to put all your processing within a function (such as `processing()`) or may want to do it just as plain Python code. It's entirely up to you!\n","\n","However, once you have processed `df` and `df_eval`, you must assign them to input variables `X` and `X_eval`."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["country_map = {'AO': 'Africa', 'BF': 'Africa', 'BI': 'Africa', 'BJ': 'Africa', 'BW': 'Africa', 'CF': 'Africa', 'CG': 'Africa', 'CI': 'Africa', 'CM': 'Africa', 'CV': 'Africa', 'DJ': 'Africa', 'DZ': 'Africa',\n","               'EG': 'Africa', 'EH': 'Africa', 'ER': 'Africa', 'ET': 'Africa', 'GA': 'Africa', 'GH': 'Africa', 'GM': 'Africa', 'GN': 'Africa', 'GQ': 'Africa', 'GW': 'Africa', 'KE': 'Africa', 'KM': 'Africa',\n","               'LR': 'Africa', 'LS': 'Africa', 'LY': 'Africa', 'MA': 'Africa', 'MG': 'Africa', 'ML': 'Africa', 'MR': 'Africa', 'MU': 'Africa', 'MW': 'Africa', 'MZ': 'Africa', 'NA': 'Africa', 'NE': 'Africa',\n","               'NG': 'Africa', 'RE': 'Africa', 'RW': 'Africa', 'SC': 'Africa', 'SD': 'Africa', 'SL': 'Africa', 'SN': 'Africa', 'SO': 'Africa', 'ST': 'Africa', 'SZ': 'Africa', 'TD': 'Africa', 'TG': 'Africa',\n","               'TN': 'Africa', 'TZ': 'Africa', 'UG': 'Africa', 'YT': 'Africa', 'ZA': 'Africa', 'ZM': 'Africa', 'ZR': 'Africa', 'ZW': 'Africa', 'AG': 'Americas', 'AI': 'Americas', 'AN': 'Americas', 'AR': 'Americas',\n","                'AW': 'Americas', 'BB': 'Americas', 'BM': 'Americas', 'BO': 'Americas', 'BR': 'Americas', 'BS': 'Americas', 'BZ': 'Americas', 'CA': 'America', 'CL': 'Americas', 'CO': 'Americas', 'CR': 'Americas', \n","                'CU': 'Americas', 'DM': 'Americas', 'DO': 'Americas', 'EC': 'Americas', 'FK': 'Americas', 'GD': 'Americas', 'GF': 'Americas', 'GL': 'America', 'GP': 'Americas', 'GT': 'Americas', 'GY': 'Americas', \n","                'HN': 'Americas', 'HT': 'Americas', 'JM': 'Americas', 'KN': 'Americas', 'KY': 'Americas', 'LC': 'Americas', 'MQ': 'Americas', 'MS': 'Americas', 'MX': 'Americas', 'NI': 'Americas', 'PA': 'Americas', \n","                'PE': 'Americas', 'PM': 'America', 'PR': 'Americas', 'PY': 'Americas', 'SR': 'Americas', 'SV': 'Americas', 'TC': 'Americas', 'TT': 'Americas', 'UE': 'Americas', 'US': 'America', 'UY': 'Americas', \n","                'VC': 'Americas', 'VG': 'Americas', 'VI': 'Americas', 'AQ': 'Antarctica', 'AE': 'Asia', 'AF': 'Asia', 'AM': 'Asia', 'AZ': 'Asia', 'BD': 'Asia', 'BH': 'Asia', 'BN': 'Asia', 'BT': 'Asia', 'CC': 'Asia', \n","                'CN': 'Asia', 'CX': 'Asia', 'CY': 'Asia', 'GE': 'Asia', 'HK': 'Asia', 'ID': 'Asia', 'IL': 'Asia', 'IN': 'Asia', 'IO': 'Asia', 'IQ': 'Asia', 'IR': 'Asia', 'JO': 'Asia', 'JP': 'Asia', 'KG': 'Asia', \n","                'KH': 'Asia', 'KP': 'Asia', 'KR': 'Asia', 'KW': 'Asia', 'KZ': 'Asia', 'LA': 'Asia', 'LB': 'Asia', 'LK': 'Asia', 'MM': 'Asia', 'MN': 'Asia', 'MO': 'Asia', 'MV': 'Asia', 'MY': 'Asia', 'NP': 'Asia', \n","                'OM': 'Asia', 'PH': 'Asia', 'PK': 'Asia', 'QA': 'Asia', 'RU': 'Asia', 'SA': 'Asia', 'SG': 'Asia', 'SY': 'Asia', 'TH': 'Asia', 'TJ': 'Asia', 'TM': 'Asia', 'TP': 'Asia', 'TR': 'Asia', 'TW': 'Asia', \n","                'UZ': 'Asia', 'VN': 'Asia', 'YE': 'Asia', 'BV': 'Atlantic Ocean', 'GS': 'Atlantic Ocean', 'SH': 'Atlantic Ocean', 'AD': 'Europe', 'AL': 'Europe', 'AT': 'Europe', 'BA': 'Europe', 'BE': 'Europe', \n","                'BG': 'Europe', 'BY': 'Europe', 'CH': 'Europe', 'CZ': 'Europe', 'DE': 'Europe', 'DK': 'Europe', 'EE': 'Europe', 'ES': 'Europe', 'FI': 'Europe', 'FO': 'Europe', 'FR': 'Europe', 'FX': 'Europe', \n","                'GI': 'Europe', 'GR': 'Europe', 'HR': 'Europe', 'HU': 'Europe', 'IE': 'Europe', 'IS': 'Europe', 'IT': 'Europe', 'LI': 'Europe', 'LT': 'Europe', 'LU': 'Europe', 'LV': 'Europe', 'MC': 'Europe', \n","                'MD': 'Europe', 'MK': 'Europe', 'MT': 'Europe', 'NL': 'Europe', 'NO': 'Europe', 'PL': 'Europe', 'PT': 'Europe', 'RO': 'Europe', 'SE': 'Europe', 'SI': 'Europe', 'SJ': 'Europe', 'SK': 'Europe', \n","                'SM': 'Europe', 'UA': 'Europe', 'GB': 'Europe', 'VA': 'Europe', 'YU': 'Europe', 'HM': 'Indian Ocean', 'AS': 'Oceania', 'AU': 'Oceania', 'CK': 'Oceania', 'FJ': 'Oceania', 'FM': 'Oceania', \n","                'GU': 'Oceania', 'KI': 'Oceania', 'MH': 'Oceania', 'MP': 'Oceania', 'NC': 'Oceania', 'NF': 'Oceania', 'NR': 'Oceania', 'NU': 'Oceania', 'NZ': 'Oceania', 'PF': 'Oceania', 'PG': 'Oceania', \n","                'PN': 'Oceania', 'PW': 'Oceania', 'SB': 'Oceania', 'TK': 'Oceania', 'TO': 'Oceania', 'TV': 'Oceania', 'UM': 'Oceania', 'VU': 'Oceania', 'WF': 'Oceania', 'WS': 'Oceania'}"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["import json\n","import numpy as np\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","\n","# Your code here:\n","#\n","def processing(df):\n","    # change dates in to a window open date thing\n","    df['time_open1'] = df['deadline'] - df['launched_at']\n","    # df['deadline'] = pd.to_datetime(df['deadline'], unit='s')\n","    # df['created_at'] = pd.to_datetime(df['created_at'], unit='s')\n","    # df['launched_at'] = pd.to_datetime(df['launched_at'], unit='s')\n","    # df['time_open'] = df['deadline'] - df['launched_at']\n","\n","    # edit goal feature to be a consistent currency across\n","    df['goal_usd'] = df['goal'] * df['static_usd_rate']\n","\n","    # after reviewing the json files, category seems decent -- just keep the slug_1\n","    df_cat = df.copy()\n","    df_cat = df_cat[['category']]\n","    df_cat['parsed_data'] = df_cat['category'].apply(json.loads)\n","    df_cat2 = pd.json_normalize(df_cat['parsed_data'])\n","\n","    slug_split = df_cat2['slug'].str.split(pat = '/', expand = True)\n","\n","    df_cat2[['slug_1', 'slug_2']] = slug_split\n","    df_cat2 = df_cat2[['slug_1']]\n","    df = pd.concat([df, df_cat2],axis=1)\n","    df = df.drop(columns=['category'], axis = 1)\n","\n","    # map country into area (America on it's own)\n","    df['area'] = df['country'].map(country_map)\n","\n","    #length of string variables\n","    df['blurb_len'] = df['blurb'].str.len()\n","\n","    # scaler = StandardScaler()\n","\n","    df_scaled = df.copy()\n","    df_scaled = df_scaled[['goal_usd','blurb_len']]\n","    # ,'time_open1'\n","    # df_scaled = pd.DataFrame(scaler.fit_transform(df_scaled),columns = df_scaled.columns)\n","\n","    # df = df.drop(columns=['goal_usd', 'time_open1','blurb_len'], axis=1)\n","    # df = pd.concat([df, df_scaled], axis = 1)\n","\n","\n","    df_cat = df[['area','slug_1']]\n","    # ,'currency'\n","    df2 = pd.get_dummies(df_cat)\n","\n","    # onehotencoder\n","    # ohe = OneHotEncoder(handle_unknown='ignore', sparse_output = False).set_output(transform='pandas')\n","\n","    # ohe_slug_1 = pd.get_dummies(.fit_transform(df[['slug_1']])\n","\n","    # df = pd.concat([df, ohe_slug_1], axis = 1).drop(columns = ['slug_1'], axis = 1)\n","\n","    # ohe_currency = ohe.fit_transform(df[['currency']])\n","    # df = pd.concat([df, ohe_currency], axis = 1).drop(columns = ['currency'], axis = 1)\n","\n","    # ohe_area = ohe.fit_transform(df[['area']])\n","    # df = pd.concat([df, ohe_area], axis = 1).drop(columns = ['area'], axis = 1)\n","\n","    df2 = pd.concat([df2, df_scaled], axis =1)\n","    # drop columns\n","    #df = df.drop(columns = ['blurb','goal','country','deadline','created_at','launched_at','static_usd_rate'], axis = 1)\n","    return df2\n","#\n","X = processing(df)\n","X_eval = processing(df_eval)\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>area_America</th>\n","      <th>area_Americas</th>\n","      <th>area_Asia</th>\n","      <th>area_Europe</th>\n","      <th>area_Oceania</th>\n","      <th>slug_1_art</th>\n","      <th>slug_1_comics</th>\n","      <th>slug_1_crafts</th>\n","      <th>slug_1_dance</th>\n","      <th>slug_1_design</th>\n","      <th>...</th>\n","      <th>slug_1_film &amp; video</th>\n","      <th>slug_1_food</th>\n","      <th>slug_1_games</th>\n","      <th>slug_1_journalism</th>\n","      <th>slug_1_music</th>\n","      <th>slug_1_photography</th>\n","      <th>slug_1_publishing</th>\n","      <th>slug_1_technology</th>\n","      <th>goal_usd</th>\n","      <th>blurb_len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>125000.000</td>\n","      <td>134</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>9800.000</td>\n","      <td>131</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>2500.000</td>\n","      <td>52</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>16800.793</td>\n","      <td>120</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>5500.000</td>\n","      <td>125</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 21 columns</p>\n","</div>"],"text/plain":["   area_America  area_Americas  area_Asia  area_Europe  area_Oceania  \\\n","0          True          False      False        False         False   \n","1          True          False      False        False         False   \n","2          True          False      False        False         False   \n","3         False          False      False         True         False   \n","4          True          False      False        False         False   \n","\n","   slug_1_art  slug_1_comics  slug_1_crafts  slug_1_dance  slug_1_design  ...  \\\n","0       False          False          False         False          False  ...   \n","1       False          False          False         False          False  ...   \n","2       False          False          False         False          False  ...   \n","3       False          False          False         False          False  ...   \n","4       False          False          False         False          False  ...   \n","\n","   slug_1_film & video  slug_1_food  slug_1_games  slug_1_journalism  \\\n","0                 True        False         False              False   \n","1                False        False         False              False   \n","2                False        False         False              False   \n","3                 True        False         False              False   \n","4                 True        False         False              False   \n","\n","   slug_1_music  slug_1_photography  slug_1_publishing  slug_1_technology  \\\n","0         False               False              False              False   \n","1         False               False               True              False   \n","2          True               False              False              False   \n","3         False               False              False              False   \n","4         False               False              False              False   \n","\n","     goal_usd  blurb_len  \n","0  125000.000        134  \n","1    9800.000        131  \n","2    2500.000         52  \n","3   16800.793        120  \n","4    5500.000        125  \n","\n","[5 rows x 21 columns]"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["X.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Part 2: Training the model\n","\n","Now that we have separated our data into train and evaluation data, we can start training models and evaluating their performance. At this point, you are welcome to explore any model architecture, so long as it is a **classification** model.\n","\n","Check out the `sklearn` [documentation](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) for a selection of possible models and implementation examples.\n","\n","Note that most `sklearn` models have the same interface. Once imported you can create an instantiation of your model (specifying custom settings as you see fit), and assign it to a variable. For instance:\n","\n","``` Python\n","from sklearn.linear_model import LogisticRegression\n","\n","model = LogisticRegression()\n","```\n","\n","Once you have created your `model` variable, you can call `.fit()` and pass `X` and `y` as arguments.\n","\n","**ðŸ’¡ For KATE to work, your model must be assigned to a variable called `model`**\n","\n","**NOTE**: Since with this project your model will be trained directly on KATE, it is limited to models that can be trained under 1min. You will receive a `TimeoutError` if your model takes too long.\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy score for Logistic Regression model: 0.5694\n","Accuracy score for GaussianNB  model: 0.51534\n","Accuracy score for DecisionTreeClassifier  model: 0.69864\n"]}],"source":["# Your code here:\n","\n","from sklearn.linear_model import LogisticRegression\n","\n","lr = LogisticRegression(solver='lbfgs', max_iter = 1000)\n","lr.fit(X, y)\n","\n","print(f\"Accuracy score for Logistic Regression model: {lr.score(X, y)}\")\n","\n","\n","from sklearn.naive_bayes import GaussianNB\n","\n","gnb = GaussianNB()\n","gnb.fit(X, y)\n","print(f\"Accuracy score for GaussianNB  model: {gnb.score(X, y)}\")\n","\n","from sklearn.tree import DecisionTreeClassifier\n","\n","model = DecisionTreeClassifier(max_depth=40, min_samples_split=80)\n","model.fit(X, y)\n","print(f\"Accuracy score for DecisionTreeClassifier  model: {model.score(X, y)}\")\n","\n","\n","# from sklearn.ensemble import RandomForestClassifier \n","# rfc = RandomForestClassifier()\n","# rfc.fit(X, y)\n","# print(f\"Accuracy score for DecisionTreeClassifier  model: {rfc.score(X, y)}\")\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Once trained, we can use the `.score()` function to evaluate our model's performance on the train set. Remember to pass `X` and `y` as arguments."]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy score for model: 0.69864\n"]}],"source":["# Your code here:\n","print(f\"Accuracy score for model: {model.score(X, y)}\")"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.69864\n","F1: 0.7010554717879534\n"]}],"source":["from sklearn.metrics import accuracy_score, f1_score\n","\n","predictions = model.predict(X)\n","# Your code here...\n","acc = accuracy_score(y, predictions)\n","f1 = f1_score(y, predictions)\n","              \n","print(f'Accuracy: {acc}')\n","print(f'F1: {f1}')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Part 3: Making predictions\n","\n","Now that our model is trained, we can use the `.predict()` function to make predictions for the rows in our data where `y` is not known.\n"," - Call `.predict()` on the `model` variable, and pass `X_eval`\n"," - Assign the output of `.predict()` to a variable called `y_pred`"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["# Your code here:\n","y_pred = model.predict(X_eval)"]},{"cell_type":"markdown","metadata":{},"source":["Note that in the previous exercise, we used the `.score()` function to evaluate our model on the training data. However, we do not have the ground truth for our `X_eval` data points - to see how well the model performs on the test set, you will have to submit it to **KATE**!"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
