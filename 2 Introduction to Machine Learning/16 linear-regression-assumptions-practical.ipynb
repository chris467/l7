{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Assumptions of linear models\n", "\n", "In the material for this learning unit, we covered the following:\n", "\n", "* Recap of linear regression\n", "* Error and the true model\n", "* Assumptions of linear models\n", "* Addressing linear model limitations\n", "\n", "In this practical, we'll explore the linear regression assumptions in more detail. The aim is to demonstrate points covered in the learning unit on real-world data, whilst focusing on the simple univariate case where we use a single input feature to predict an output variable."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# The dataset - linearity\n", "\n", "We'll use a dataset of diabetes patients, which has ten features based on health measurements and one outcome variable of disease progression after one year.\n", "\n", "The features are as follows:\n", "\n", "| Feature \t| Description                           \t|\n", "|-----------|-------------------------------------------|\n", "| `age`    \t| Age in years                          \t|\n", "| `sex`    \t| Sex                                   \t|\n", "| `bmi`    \t| Body Mass Index                       \t|\n", "| `bp`     \t| Average blood pressure                \t|\n", "| `tc`     \t| T-Cells (a type of white blood cells) \t|\n", "| `ldl`    \t| low-density lipoproteins              \t|\n", "| `hdl`    \t| high-density lipoproteins             \t|\n", "| `tch`    \t| thyroid stimulating hormone           \t|\n", "| `ltg`    \t| lamotrigine                           \t|\n", "| `glu`    \t| blood sugar level                     \t|\n", "\n", "The data has already been pre-processed to standardise and normalise it all, to make it suitable for use in machine learning models.\n", "\n", "But first, plot each of the ten values `['age', 'sex', 'bmi', 'bp', 'tc', 'ldl', 'hdl', 'tch', 'ltg', 'glu']` against `progression` to see if it they meet the linearity requirement of a linear model.\n", "\n", "Use seaborn's `pairplot` method to plot the ten `x_vars` against a single `y_vars` in one quick line. What do you see?\n", "\n", "(If the plot seems small in your notebook, set the \"height\" argument to something around 4 or 5, then doubleclick on it to enlarge when it renders.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import pandas as pd\n", "import seaborn as sns\n", "\n", "data = pd.read_csv('data/diabetes.csv')\n", "\n", "# Your code here\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# The dataset - lack of multicollinearity\n", "\n", "The assumption of a lack of multicollinearity requires there to be no strong correlation between any features (assuming you have more than one feature to predict $y$!)\n", "\n", "We will only be modelling `progression` using a single feature at a time, so independence of the features isn't really a factor here. But let's look at it anyway.\n", "\n", "Compute the correlation matrix of the dataset using the `.corr()` method. Remember, you only want to check the features against each other, not the target variable!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Using seaborn's `heatmap` function, visualise the matrix. Which features stand out?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Assumptions\n", "\n", "Apart from linearity and a lack of multicollinearity, we looked at three other assumptions of the linear regression model:\n", "\n", "* Homoscedasticity (Constant variance)\n", "* Normality\n", "* Independence\n", "\n", "Let's look at the homoscedasticity and normality assumptions in more detail - what do they look like in practice and how can we detect them? Later on, when you are doing multivariate linear regression, you will use these techniques yourself. But first let's investigate them in our simple univariate case/"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## The dataset - homoscedasticity and normality\n", "\n", "To test these assumptions, we need a trained model. So first we will look at how to do this and test as we go."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Training a linear regression model\n", "\n", "This is easy to do with `sklearn`. The steps are:\n", "\n", "1. Instantiate a model from `sklearn.linear_model.LinearRegression`\n", "2. Use the `.fit()` method to train the model, by minimising the error against the true y values.\n", "3. Access model parameters through `.coef_` and R2 score through `.score()`\n", "4. Use the `.predict()` method on the trained model to get new predictions\n", "\n", "An example is given below."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.linear_model import LinearRegression\n", "\n", "model = LinearRegression()\n", "\n", "x = data['bmi']\n", "x = x.values.reshape(-1, 1)\n", "# When using a single feature in sklearn, it must be reshaped\n", "# from the form [1,2,3,4] to [[1],[2],[3],[4]]\n", "\n", "y = data['progression']\n", "\n", "model.fit(x, y)\n", "\n", "print(f\"Weight for single feature (bmi): {model.coef_}\")\n", "print(f\"R2 score for model: {model.score(x, y):.5f}\")\n", "\n", "unseen_x = [[0.2], [0.11], [0.8], [0.45]]\n", "\n", "print(\"Predictions:\", [i for i in model.predict(unseen_x)])"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Now, using the diabetes dataset, do the following for each of the ten features:\n", "\n", "1. Instantiate and `.fit()` a LinearRegression model.\n", "2. Print the R2 score.\n", "3. Print a scatterplot of the residuals (predictions-truth) against the predictions to test for constant variance.\n", "4. Perform a Shapiro-Wilk's test on the residuals (predictions-truth) to test for normality.\n", "\n", "To access a single feature at a time, you can iterate through a list of the features and use it to index the dataframe by column.\n", "\n", "(And because sklearn expects at least two features normally, don't forget to reshape the x values!)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from scipy.stats import shapiro\n", "import matplotlib.pyplot as plt\n", "# Use plt.show() after you make a graph to show it during a for loop\n", "# Otherwise they all end up at the end of the output cell!\n", "\n", "# Your code here\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["How would you interpret these R2 scores and p-values for the S-W test?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Visualising simple linear regression\n", "\n", "In general, visualising multivariate linear models with more than 3 features isn't possible. But in the univariate case, it's as simple as plotting a straight line, using the coefficient and bias term.\n", "\n", "Alternatively, just use `lmplot()` from seaborn - it will plot the data, the line and show confidence intervals."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["for feature in ['age', 'sex', 'bmi', 'bp', 'tc', 'ldl', 'hdl', 'tch', 'ltg', 'glu']:\n", "    x = data[feature]\n", "    x = x.values.reshape(-1, 1)\n", "    y = data['progression']\n", "    \n", "    model = LinearRegression()\n", "    model.fit(x, y)\n", "    \n", "    g = sns.lmplot(data=data, x=feature, y='progression');\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Summary\n", "\n", "Looking at the univariate case of linear regression, we focused on training a linear model to predict an output variable from a single input feature.\n", "\n", "This isn't the most realistic (or exciting!) situation, but the goal was to introduce the core aspects and assumptions of the Linear Regression model. These become more important when working with linear models which use more features."]}]}