{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 5, "cells": [{"cell_type": "markdown", "id": "81557a4f", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Intro to Linear Regression Using Gradient Descent - Practical\n", "\n", "* Linear regression is one of the most widely used techniques in statistical modeling. \n", "\n", "* It is a method used to model the relationship between a dependent variable and one or more explanatory variables. \n", "\n", "* The goal of linear regression is to find the line or hyperplane of best fit that accurately represents the relationship between the variables. \n", "\n", "In the lecture notes, we computed the weights for the best fit for the univariate case, using gradient descent. We also introduced the slight variations in the algebra for computing the weights for best fit for the multivariate case. \n", "\n", "In this practical, we will implement linear regression for the multivariate case, using synthetic data."]}, {"cell_type": "markdown", "id": "ca17bb8e", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Generating our data"]}, {"cell_type": "markdown", "id": "f9f27fbd", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Here we provide the code to generate data with 2 independently normally explanatory variables, with additive random noise. "]}, {"cell_type": "code", "execution_count": null, "id": "06f064cf", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import numpy as np\n", "\n", "# Generate data with 2 features and a target variable\n", "def generate_data_3d(n):\n", "    np.random.seed(n)\n", "    X = np.stack([np.random.normal(scale=2, size=n), np.random.normal(scale=5, size=n)]).T\n", "    beta = np.absolute(np.random.randn(3))\n", "    # add slight some standard normal noise\n", "    y = beta[0] + beta[1]*X[:,0] + beta[2]*X[:,1] + np.random.normal(scale=1, size=n)\n", "    return X, y, beta\n", "\n", "n = 100\n", "X, y, beta_true = generate_data_3d(n)\n", "\n", "X.shape"]}, {"cell_type": "markdown", "id": "95946dd5", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["As we control the data generation process, we know the true parameters for the linear model:"]}, {"cell_type": "code", "execution_count": null, "id": "113823db", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["print(f'True parameter values: b: {beta_true[0]:.2f}, weight 1: {beta_true[1]:.2f}, weight 2: {beta_true[2]:.2f}')"]}, {"cell_type": "markdown", "id": "34715915", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Visualisation of 3D data"]}, {"cell_type": "markdown", "id": "685994e5", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can use the [*plotly*](https://plotly.com/python/) library to plot an interactive 3D scatter of our data. Plotly works really well at generating interactive graphs and visualisations. \n", "\n", "The [*express*](https://plotly.com/python/plotly-express/) module is designed to be very high-level and easy to use."]}, {"cell_type": "code", "execution_count": null, "id": "44b279ab", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import plotly.express as px\n", "fig = px.scatter_3d(x=X[:, 0], y=X[:, 1], z=y, labels = { 'x':'x1', 'y':'x2', 'z':'y'})\n", "fig.show()"]}, {"cell_type": "markdown", "id": "02f2391c", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Multivariate linear regression - gradient descent\n", "\n", "Now that we have more than one feature, our linear regression model will have more than one weight. \n", "\n", "We can therefore generate our predictions as:\n", "$$\n", "\\hat{y_i} = b + w_1 x_{1,i} + w_2 x_{2,i}\n", "$$\n", "\n", "We can first try to find the parameters separately by thinking about multivariate linear regressions as a series of independent 1D linear regressions, and using gradient descent to find the parameters. Try adapting the univariate gradient descent code from the lecture notes to work for two $x$ features!\n", "\n", "*Hint: As we assume the impacts of the features in $x$ on the target $y$ are independent, we can calculate the gradients at each iteration ignoring all other parameters*"]}, {"cell_type": "code", "execution_count": null, "id": "20510cce", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Extract each feature as separate vectors \n", "x1 = X[:,0]\n", "x2 = X[:,1]\n", "\n", "# Initial values for the weight and bias\n", "b = 0\n", "w1 = 0\n", "w2 = 0\n", "\n", "eta = 0.001  # The learning rate\n", "iterations = 1000  # The number of iterations to perform gradient descent\n", "\n", "n = float(len(x1)) # Number of observations in X\n", "\n", "# Performing Gradient Descent \n", "for i in range(iterations): \n", "    # write your code here\n", "    y_pred = b + w1*x1 + w2*x2 # The current predicted value of Y\n", "    D_w1 = (-2/n) * sum(x1 * (y - y_pred))  # Derivative wrt w_1\n", "    D_w2 = (-2/n) * sum(x2 * (y - y_pred))  # Derivative wrt w_2\n", "    D_b = (-2/n) * sum(y - y_pred)  # Derivative wrt b\n", "    w1 = w1 - eta * D_w1  # Update w_1\n", "    w2 = w2 - eta * D_w2  # Update w_2\n", "    b = b - eta * D_b  # Update b\n", "    \n", "print(f'Estimated parameters: bias: {b:.2f}, weight 1: {w1:.2f}, weight 2: {w2:.2f}')"]}, {"cell_type": "markdown", "id": "c6e2d9d8", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["These are not too far away from our true values - not bad! \n", "\n", "But how can we know when we have fully converged?\n", "\n", "Try playing around with values for the learning rate (eta) and number of iterations. \n", "\n", "* What happens if you increase the learning rate? (e.g. eta = 0.1)\n", "\n", "    * Why?\n", "\n", "* What happens if you decrease the number of iterations? (e.g. iterations = 100)\n", "\n", "    * Why?"]}, {"cell_type": "markdown", "id": "e8e2fef9", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Linear algebra to the (partial) rescue!\n", "\n", "Whilst our code above was able to estimate usable parameters, it is quite clunky. \n", "\n", "We can make the code much simpler by treating $X$ as a *matrix* and using *linear algebra!*"]}, {"cell_type": "markdown", "id": "390d6b93", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Accounting for the bias term\n"]}, {"cell_type": "markdown", "id": "8f72b40b", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["In order to perform the matrix multiplication and compute the gradient effectively in the multivariate case, we augment $X$ with a column of ones to account for the bias term. \n", "\n", "Call the new matrix `Xmat`\n", "\n", "*Hint: You can use `np.hstack` to stack a vector of ones on an existing array.*"]}, {"cell_type": "code", "execution_count": null, "id": "f190637b", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Add a dim to x with ones to account for the bias term\n", "Xmat = np.hstack((np.ones((100,1)), X))\n"]}, {"cell_type": "markdown", "id": "f24ec3fc", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Now when computing `y_pred`, the updates for `beta`, and the error term, we do not need to account for the bias separately."]}, {"cell_type": "markdown", "id": "c202e693", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Parameters as a vector\n", "\n", "Instead of calculating each parameter separately, we can instead now calculate our *vector* of parameters $\\beta$ at the same time, including the bias term. As such, we need to initialise our parameters to a vector of zeros."]}, {"cell_type": "code", "execution_count": null, "id": "47fc5add", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Initial values for the weights (which include the bias term or beta_0)\n", "# beta = ...\n", "beta = np.zeros(Xmat.shape[1])\n"]}, {"cell_type": "markdown", "id": "edc47bc0", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Gradient descent with linear algebra \n", "\n", "Now try rewriting the gradient descent code to work with linear algebra. \n", "\n", "Recall that we can rewrite our prediction formula as: \n", "\n", "$$\n", "\\hat{y}=\\beta^T X\n", "$$\n", "\n", "This can be achieved with the notation `y_pred = X@w`.\n", "\n", "*Hint: Make sure you are using* `Xmat` *and not* `X`*!*\n", "\n", "*Additional hint: to multiply our residuals by* `X` *in linear algebra, you may first need to take the transpose of* `X`"]}, {"cell_type": "code", "execution_count": null, "id": "cbe8a242", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Linear algebra gradient descent\n", "beta = np.zeros(Xmat.shape[1])\n", "eta = 0.001  # The learning rate\n", "iterations = 1000  # The number of iterations to perform gradient descent\n", "\n", "n = float(Xmat.shape[0]) # Number of observations in X\n", "\n", "# Performing Gradient Descent \n", "for i in range(iterations): \n", "    # write your code here\n", "    y_pred = Xmat@beta  # The current predicted value of Y\n", "    D_beta = -(2/n) * Xmat.T @ (y - y_pred)  # Derivative wrt beta\n", "    beta = beta - eta * D_beta  # Update beta\n", "print(f'beta: {beta}')"]}, {"cell_type": "markdown", "id": "55057a97", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Note these are the exact same parameters we had before, but the code to get there is much neater!"]}, {"cell_type": "code", "execution_count": null, "id": "05a44849", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["np.isclose(beta, [b, w1, w2])"]}, {"cell_type": "markdown", "id": "cf735124", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Visualising model hyperplane\n", "\n", "How does our model actually look?\n", "\n", "Given we have two gradients ($\\beta_1$ and $\\beta_2$) and an intercept ($\\beta_0$), we can see the model defines a 2D plane in 3D. "]}, {"cell_type": "code", "execution_count": null, "id": "08ddfa0c", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import plotly.graph_objects as go\n", "\n", "fig1 = px.scatter_3d(x=Xmat[:, 1], y=Xmat[:, 2], z=y, labels = { 'x':'x1', 'y':'x2', 'z':'y'})\n", "\n", "# Create a mesh grid to get predictions for and plot a hyperplane\n", "xy = np.array([[i,j] for i in np.linspace(-4,4,20) for j in np.linspace(-15,10,60)])\n", "y_plane = np.hstack((np.ones((1200,1)), xy))@beta\n", "\n", "fig2 = go.Scatter3d(x=xy[:,0], y=xy[:,1], z=y_plane, opacity=0.7,  mode='markers',\n", "    marker=dict(\n", "        size=4,\n", "        color=np.ones_like(y_plane),\n", "        colorscale='Viridis',\n", "        opacity=0.8\n", "    )\n", ")\n", "fig3 = go.Figure(data=(fig2,)+ fig1.data)\n", "\n", "fig3.update_layout(\n", "    scene=dict(\n", "        xaxis_title='x1',\n", "        yaxis_title='x2',\n", "        zaxis_title='y',\n", "    )\n", ")\n", "\n", "fig3.show()"]}, {"attachments": {}, "cell_type": "markdown", "id": "fc24dbc8", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["More generally, for $J$ input features (i.e. a $J$-dimensional feature space), the linear regression model represents a $J$-dimensional hyperplane in $J+1$-dimensional space."]}, {"cell_type": "markdown", "id": "8959617f", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Summary\n", "\n", "In this practical you have implemented linear regression using gradient descent, and then improved the code using linear algebra. \n", "\n", "However, we can actually use linear algebra to find our linear regression parameters directly (i.e. without needing to use gradient descent), using a technique known as *Ordinary Least Squares (OLS)*."]}]}