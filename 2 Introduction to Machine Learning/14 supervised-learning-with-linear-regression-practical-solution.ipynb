{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}, "vscode": {"interpreter": {"hash": "00b0853e5e621ed0cb8e773b2dc990d66ff9b3e0945a1e344ef97e954a3f98d7"}}}, "nbformat": 4, "nbformat_minor": 5, "cells": [{"cell_type": "markdown", "id": "8633a363", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Supervised Learning with Linear Regression\n", "##\u00a0Practical\n", "\n", "\n", "Aims of this practical:\n", "\n", "- Supervised learning approach\n", "- Linear regression as an ML regressor\n", "- Out-of- sample validation\n", "- L1 and L2 regularisation\n", " "]}, {"cell_type": "code", "execution_count": null, "id": "89f23dfe", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import mean_squared_error, r2_score"]}, {"cell_type": "markdown", "id": "6987b2de", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We know that Lasso regression is more robust to outliers in the data than linear regression:\n", "- Generate a simple data set with one explanatory varaible with outliers and test if this is true"]}, {"cell_type": "code", "execution_count": null, "id": "afc48aac", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["np.random.seed(46)\n", "n_samples = 100\n", "X = np.linspace(0, 1, n_samples)\n", "y = 2*X + 0.7*np.random.randn(n_samples)\n", "\n", "# add some outliers to the dataset\n", "outliers_indices = [60, 65, 70, 75, 80, 45, 50, 61]\n", "y[outliers_indices] = y[outliers_indices] + 16"]}, {"cell_type": "markdown", "id": "05f2901d", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Create a scatterplot with `plt.scatter` to visualise the data and all the outliers"]}, {"cell_type": "code", "execution_count": null, "id": "9ed51ef7", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n", "plt.figure(figsize=(12, 12))\n", "plt.scatter(X, y)\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "4f70e6bd", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Use `LinearRegression` and run `.fit()` to find the optimal parameters for the standard linear regression. \n", "\n", "- Remember to use `X.reshape(-1,1)` because we only have 1 explanatory varaible."]}, {"cell_type": "code", "execution_count": null, "id": "14586b60", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n", "lr = LinearRegression()\n", "lr.fit(X.reshape(-1, 1), y)\n"]}, {"cell_type": "markdown", "id": "cc7fff44", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Create a scatter plot of the observation in the data and plot the predictions. \n", "- Use `.predict()` to obtain the predictions for each observation in the data set.\n", "- Remember to use `X.reshape(-1,1)` because we only have 1 explanatory varaible."]}, {"cell_type": "code", "execution_count": null, "id": "ec6f6574", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n", "plt.figure(figsize=(12, 12))\n", "plt.scatter(X, y)\n", "plt.plot(X, lr.predict(X.reshape(-1, 1)), color='red')\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "c6db0522", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can see that the standard linear regression model is pulled towards the outliers. \n", "\n", "We can apply Lasso regression with an additional penalty term to improve the fit of this model.\n", "- Use `Lasso` from `sklearn` and run `.fit()` to obtain the oprimal parameters.\n", "- Set the regularisation term $\\lambda$ as parameter `aplha=0.1`\n", "- Remember to use `X.reshape(-1,1)` because we only have 1 explanatory varaible.\n"]}, {"cell_type": "code", "execution_count": null, "id": "b85a8386", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n", "lasso = Lasso(alpha=0.1)\n", "lasso.fit(X.reshape(-1, 1), y)\n"]}, {"cell_type": "markdown", "id": "589cfde2", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's visualise the results of both the linear and the lasso regression model predictions on the entire data set."]}, {"cell_type": "code", "execution_count": null, "id": "9dbd867c", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["plt.figure(figsize=(12, 12))\n", "plt.scatter(X, y)\n", "plt.plot(X, lr.predict(X.reshape(-1, 1)), color='red', label='Linear Regression')\n", "plt.plot(X, lasso.predict(X.reshape(-1, 1)), color='green', label='Lasso Regression')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "24782d90", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can see that Lasso regression predictions are more robust against outliers.\n", "\n", "Change the strength of the regularisation term $\\lambda$ to `alpha=2`.\n", "- What do you observe when you create the same scatter plot with the new Lasso regression model?"]}, {"cell_type": "code", "execution_count": null, "id": "89df00bc", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n", "lasso = Lasso(alpha=2)\n", "lasso.fit(X.reshape(-1, 1), y)\n"]}, {"cell_type": "code", "execution_count": null, "id": "a84044c3", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["plt.figure(figsize=(12, 12))\n", "plt.scatter(X, y)\n", "plt.plot(X, lr.predict(X.reshape(-1, 1)), color='red', label='Linear Regression')\n", "plt.plot(X, lasso.predict(X.reshape(-1, 1)), color='green', label='Lasso Regression')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "3fc80c1e", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The regularisation parameter $\\lambda$ controls the strength of the regularisation applied. "]}, {"cell_type": "markdown", "id": "cddc43f8", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["For the next example we'll use data from `sklearn`: California Housing dataset and apply linear , lasso and ridge regression. We will also evaluate these models based on the MSE of the test set.\n", "\n", "#### Data information\n", "\n", "- Number of observations: 20640\n", "- Target variable is the median house value for California districts,\n", "expressed in hundreds of thousands of dollars ($100,000).\n", "- Number of explanatory variables: 8 numeric, predictive attributes and the target\n", "\n", "- Attribute Information:\n", "            - MedInc        median income in block group\n", "            - HouseAge      median house age in block group\n", "            - AveRooms      average number of rooms per household\n", "            - AveBedrms     average number of bedrooms per household\n", "            - Population    block group population\n", "            - AveOccup      average number of household members\n", "            - Latitude      block group latitude\n", "            - Longitude     block group longitude\n", "\n", "This dataset was obtained from the StatLib repository.\n", "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html"]}, {"cell_type": "markdown", "id": "1fc3d25a", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We've provided a copy of the dataset in the `data/` folder, let's load it with pandas:"]}, {"cell_type": "code", "execution_count": null, "id": "c641f3aa", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["df = pd.read_csv(\"data/housing.csv\")"]}, {"cell_type": "markdown", "id": "3e6242ad", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["You can run `.head()`, `.describe()` and `.info()` to get an overview of the data"]}, {"cell_type": "code", "execution_count": null, "id": "c9af254b", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "markdown", "id": "8b662821", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Create a test(20%) and train(80%) set split\n", "- We drop the target variable `PRICE` from the explanatory varaibles"]}, {"cell_type": "code", "execution_count": null, "id": "abd9a6de", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(df.drop('Price', axis=1), \n", "                                                    df['Price'], test_size=0.2, random_state=42)"]}, {"cell_type": "markdown", "id": "6592b430", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can view the number of observations per split with `.shape`"]}, {"cell_type": "code", "execution_count": null, "id": "8ec881d8", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["print('Training input data Shape:', X_train.shape)\n", "print('Training output data Shape:', y_train.shape)\n", "print('Testing input data Shape:', X_test.shape)\n", "print('Testing output data Shape:', y_test.shape)"]}, {"cell_type": "markdown", "id": "053dc47a", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Call `LinearRegression`, `Ridge` and `Lasso` and run `fit()` for each model on the train set.\n", "- Use the regularisation term $\\lambda$ as parameter `alpha=1.0` for both Ridge and Lasso regression."]}, {"cell_type": "code", "execution_count": null, "id": "df4713f8", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n", "lr = LinearRegression()\n", "lr.fit(X_train, y_train)\n", "\n", "ridge = Ridge(alpha=1.0)\n", "ridge.fit(X_train, y_train)\n", "\n", "lasso = Lasso(alpha=1.0)\n", "lasso.fit(X_train, y_train)\n"]}, {"cell_type": "markdown", "id": "88dac8b4", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can view the coefficients for the linear, ridge and lasso regression to compare\n", "- We add the column names for the explanatory variables with `.columns`\n", "- We add the model name as the index"]}, {"cell_type": "code", "execution_count": null, "id": "701cf349", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["pd.DataFrame([lr.coef_, ridge.coef_, \n", "              lasso.coef_], index=['linear',\n", "                                   'ridge', 'lasso'], columns=df.drop('Price', axis=1).columns)"]}, {"cell_type": "markdown", "id": "a1330b32", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can see that lasso regression shrunk `AveRooms`, `AveBedrms`, `AveOccup`, `Latitude` and `Longitude` all the way to 0."]}, {"cell_type": "markdown", "id": "0c873cac", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's create predictions for the test set for each model and calculate the $R^2$ score and the MSE\n", "- We will use `mean_squared_error` and `r2_score` from `sklearn.metrics`\n", "- Linear and ridge regression has been set up and all that's left is to add lasso regression"]}, {"cell_type": "code", "execution_count": null, "id": "dd822f21", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["y_pred_lr = lr.predict(X_test)\n", "mse_lr = mean_squared_error(y_test, y_pred_lr)\n", "r2_lr = r2_score(y_test, y_pred_lr)\n", "\n", "y_pred_ridge = ridge.predict(X_test)\n", "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n", "r2_ridge = r2_score(y_test, y_pred_ridge)\n", "\n", "# Your code here \n", "y_pred_lasso = lasso.predict(X_test)\n", "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n", "r2_lasso = r2_score(y_test, y_pred_lasso)\n"]}, {"cell_type": "markdown", "id": "d39c6d57", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's print out the results for each model"]}, {"cell_type": "code", "execution_count": null, "id": "a9d95b0a", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["print(\"Linear Regression MSE: {:.3f}, R-squared: {:.3f}\".format(mse_lr, r2_lr))\n", "print(\"Ridge Regression MSE: {:.3f}, R-squared: {:.3f}\".format(mse_ridge, r2_ridge))\n", "print(\"Lasso Regression MSE: {:.3f}, R-squared: {:.3f}\".format(mse_lasso, r2_lasso))"]}, {"cell_type": "markdown", "id": "6cee69ab", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The Lasso regression with $\\lambda=0.1$ did not perform as well on this data set."]}, {"cell_type": "markdown", "id": "2d2c4ef0", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["There is another regression technique that we can implement in `sklearn`:\n", "- A random forest regression \n", "- This technique is based on lots of decision trees working together to get a prediction for the test set"]}, {"cell_type": "code", "execution_count": null, "id": "c597014c", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestRegressor"]}, {"cell_type": "markdown", "id": "7bf60d5c", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can run the model with the exact same functions as linear regression\n", "- we still use `.fit()` to get the optimal parameters"]}, {"cell_type": "code", "execution_count": null, "id": "8e536c42", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Instantiate model\n", "rf = RandomForestRegressor()\n", "\n", "# Train the model on training data\n", "rf.fit(X_train, y_train)"]}, {"cell_type": "markdown", "id": "488374ef", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can run the model with the exact same functions as linear regression\n", "- we still use `.predict()` to get the predictions for the test set"]}, {"cell_type": "code", "execution_count": null, "id": "f054ef00", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Use the forest's predict method on the test data\n", "y_pred_randomf = rf.predict(X_test)"]}, {"cell_type": "markdown", "id": "d5b17169", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can run the model with the exact same functions as linear regression\n", "- we still use `.mean_squared_error()` to obtain the MSE for the test set"]}, {"cell_type": "code", "execution_count": null, "id": "b5f64ca0", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["mse_randomf = mean_squared_error(y_test, y_pred_randomf)\n", "print(mse_randomf)"]}, {"cell_type": "markdown", "id": "93561236", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The MSE is much lower for the random forest regression model than it is for the linear, ridge and lasso regression models."]}]}