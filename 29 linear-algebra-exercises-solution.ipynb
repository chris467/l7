{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 4, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Linear algebra exercises"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import sys\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "# Import custom functions for this notebook\n", "sys.path.append('./data')\n", "from linalg import plot_vector, load_data"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Constructing and plotting 2D vectors\n", "\n", "Generate two 2D vectors `v` and `w` with random elements.\n", "\n", ">Hint: use the function `np.random.randn()` to generate an array of random elements (documentation [here](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Sample random vectors\n", "v = np.random.randn(2)\n", "w = np.random.randn(2)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Visualize the two vectors using the `plot_vector()` function. Code for this is provided."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Plot them\n", "plot_vector(v, label='v', color='r')\n", "plot_vector(w, label='w', color='b')"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Vector norms\n", "\n", "Complete the below function `norm` for calculating the $L^1$, $L^2$, and $L^\\infty$ norms of a vector."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def norm(v, norm_type):\n", "    \"\"\"\n", "    Returns the norm of a vector v\n", "\n", "    Arguments:\n", "        v: the vector whose norm to return\n", "        norm_type: 1, 2, or np.inf, indicating whether to return\n", "            L1 norm, L2 norm, or L-infinity norm\n", "    \n", "    Returns:\n", "        scalar, the norm of v\n", "    \n", "    \"\"\"\n", "    if norm_type == 1:\n", "        return np.abs(v).sum()\n", "    if norm_type == 2:\n", "        return np.sqrt((v ** 2).sum())\n", "    if norm_type == np.inf:\n", "        return np.abs(v).max()\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Next, use the `np.linalg.norm()` function to calculate their norms. Set the `ord` argument of this function to switch between the $L^1$, $L^2$, or $L^\\infty$ norms (see [documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)). \n", "\n", "Hint: use a loop to iterate over the list `[1, 2, np.inf]` and, in each iteration, calculate the norm using `np.linalg.norm()` and `norm()` and compare them.\n", "\n", "Does your function agree with the outputs of this function?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["for norm_type in [1, 2, np.inf]:\n", "    print(f'L{norm_type} norm of v from my function norm(): {norm(v, norm_type)}')\n", "    print(f'L{norm_type} norm of v from np.linalg.norm(): {np.linalg.norm(v, ord=norm_type)}')\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Cosine similarity\n", "\n", "Complete the below function `cossim` for calculating the cosine similarity between two vectors. Use the following relationship between the cosine similarity and the dot product:\n", "$$ \\cos\\theta = \\frac{\\sum_{i = 1}^M u_i v_i}{\\sqrt{ \\sum_{i = 1}^M u_i^2 \\sum_{i = 1}^M v_i^2 }} = \\frac{\\overbrace{\\mathbf{u} \\cdot \\mathbf{v}}^{\\text{dot product}}}{||\\mathbf{u}||_2||\\mathbf{v}||_2} $$\n", "\n", "Use the function `np.dot()` to compute the dot product between two vectors (documentation [here](https://numpy.org/doc/stable/reference/generated/numpy.dot.html))."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def cossim(v, w):\n", "    \"\"\"\n", "    Returns the cosine similarity between two vectors\n", "\n", "    Arguments:\n", "        v, w: the two vectors whose cosine similarity to return\n", "    \n", "    Returns:\n", "        scalar, the cosine similarity between v and w\n", "    \n", "    \"\"\"\n", "    return np.dot(v, w) / (np.linalg.norm(v, ord=2) * np.linalg.norm(w, ord=2))    \n", "\n", "print(f'cosine similarity between v and w: {cossim(v, w)}')"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Try playing around with different settings of `v` and `w` and see how the cosine similarity changes. Does this notion of similarity agree with your intuition of how similar the vectors are when you look at them plotted in two dimensions?"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Linear combinations\n", "\n", "Complete the below function `combine` for calculating a linear combination of two vectors."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def combine(v, w, a, b):\n", "    \"\"\"\n", "    Returns a linear combination of two vectors\n", "\n", "    Arguments:\n", "        v, w: the two vectors to combine\n", "        a, b: the weighting coefficients for each vector\n", "    \n", "    Returns:\n", "        a vector\n", "    \n", "    \"\"\"\n", "    return a * v + b * w\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Use this function to create a new vector `u` that is a linear combination of `v` and `w`, and plot `u`, `v`, and `w` together."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["u = combine(v, w, 0.5, -1.2)\n", "\n", "plot_vector(v, label='v', color='r')\n", "plot_vector(w, label='w', color='b')\n", "plot_vector(u, label='u', color='g')\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Did you use matrix-vector multiplication to perform the linear combination? If not, write a new function `combine_using_matrix` that performs the same calculation but using matrix-vector multiplication. Matrix-vector multiplication can be performed using the `np.dot()` function from above, but now inserting a matrix in its first argument and vector in its second (rather than two vectors, as we did above for the dot product).\n", "\n", ">Hint: you can stack together a list of vectors into a matrix using the `np.stack()` function (documentation [here](https://numpy.org/doc/stable/reference/generated/numpy.stack.html))\n", "\n", "Check that the two functions return the same results."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def combine_using_matrix(v, w, a, b):\n", "    \"\"\"\n", "    Returns a linear combination of two vectors\n", "\n", "    Arguments:\n", "        v, w: the two vectors to combine\n", "        a, b: the weighting coefficients for each vector\n", "    \n", "    Returns:\n", "        a vector\n", "    \n", "    \"\"\"\n", "    matrix = np.stack([v, w], axis=1)\n", "    vector = np.array([a, b])\n", "    return np.dot(matrix, vector)\n", "\n", "# Check that combine() and combine_using_matrix() return the same results\n", "a = -0.2\n", "b = 1.3\n", "print(f'output from combine(): {combine(v, w, a, b)}')\n", "print(f'outputer from combine_using_matrix(): {combine_using_matrix(v, w, a, b)}')\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Manipulating data: the design matrix\n", "\n", "In the next cell, a `pandas` `DataFrame` is loaded with data containing the weights, heights, and speeds of various athletes. These are all measured relative to the average, which is why they can be positive and negative."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["data = load_data()\n", "data.head()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Extract the design matrix from this `DataFrame`. This can be easily done by simply appending `.to_numpy()` to the end of the data frame.\n", "\n", "How many athletes' data are in this data set?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["design_matrix = data.to_numpy()\n", "print(f'data set contains data from {design_matrix.shape[0]} athletes')\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Predict each athlete's speed by subtracting their weight from their height:\n", "$$ \\text{predicted speed} = \\text{height} - \\text{weight} $$\n", "Do this by multiplying the design matrix with a weight vector\n", "$$ \\mathbf{w} = \\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix} $$\n", "This should result in a vector with predicted speeds for each athlete."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["weights = np.array([-1, 1, 0])\n", "speed = design_matrix.dot(weights)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Plot each athlete's predicted speed against her true speed. How close are the predictions?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["plt.plot(speed, design_matrix[:, -1], '.');\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Could we do better with a better weight vector? In other words, is there a linear combination of the design matrix weight and height columns that could yield better predictions? How much better could this prediction get? Think about the linear independence of the columns of the design matrix, and whether they form a complete basis."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Advanced Linear Algebra"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Rotation matrices\n", "\n", "A rotation matrix is a special square matrix that, when multiplied with a vector, produces a new vector of the same length that is simply rotated from the original one by an angle.\n", "\n", "$2 \\times 2$ rotation matrices can be easily constructed via the following formula:\n", "$$ \\mathbf{R} = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix} $$\n", "Multiplying a vector with $\\mathbf{R}$ will result in a new vector rotated by $\\theta^o$.\n", "\n", "Complete the below function `rotation_matrix` for constructing a rotation matrix."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def rotation_matrix(theta):\n", "    \"\"\"\n", "    Returns a rotation matrix that rotates vectors by an angle theta\n", "\n", "    Arguments:\n", "        theta: the rotation angle\n", "    \n", "    Returns:\n", "        the corresponding 2 x 2 rotation matrix\n", "    \n", "    \"\"\"\n", "    return np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Use this function to create a vector `u` that is simply `v` rotated by 120$^o$. Then plot `u` and `v` together.\n", "\n", "**Note:** you'll have to convert angles to radians to pass them into `np.cos` and `np.sin`. You can do this easily using the `np.deg2rad()` function."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["R = rotation_matrix(np.deg2rad(120))\n", "u = R.dot(v)\n", "\n", "plot_vector(v, label='v', color='r')\n", "plot_vector(u, label='u', color='g')\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Compute the inverse of the rotation matrix, and check that multiplying it with `u` gives you back `v`. You can compute the inverse of a matrix using the function `np.linalg.inv()`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["Rinv = np.linalg.inv(R)\n", "\n", "plot_vector(u, label='u', color='g')\n", "plot_vector(Rinv.dot(u), label='z', color='m')\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["A special property of rotation matrices is that its inverse is equal to its transpose. Verify this. The transpose of a matrix can be calculated in `numpy` by simply appending `.T` to the end of a matrix."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["print(f'inverse:\\n{Rinv}')\n", "print(f'transpose:\\n{R.T}')\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Eigenvectors and eigenvalues\n", "\n", "A matrix `A` and two vectors `a1` and `a2` are provided below. In two separate cells, plot each vector together with its matrix-vector product with `A`.\n", "\n", "Which one of these vectors is an eigenvector?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["A = np.array([[-0.14,  0.64],\n", "              [ 0.16,  0.34]])\n", "a1 = np.array([1, 1])\n", "a2 = np.array([-.25, .75])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["Aa1 = A.dot(a1)\n", "plot_vector(a1, label='a1', color='k')\n", "plot_vector(Aa1, label='Aa1', color='r')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["Aa2 = A.dot(a2)\n", "plot_vector(a2, label='a2', color='k')\n", "plot_vector(Aa2, label='Aa2', color='r')\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Calculate the eigenvalue associated with that eigenvector. Recall that, for an eigenvector $\\mathbf{v}$ of a matrix $\\mathbf{A}$,\n", "$$ \\mathbf{Av} = \\lambda \\mathbf{v} $$\n", "so that \n", "$$ \\|\\mathbf{Av}\\|_2 = \\|\\lambda \\mathbf{v}\\|_2 = \\lambda \\|\\mathbf{v}\\|_2 $$\n", "Its eigenvalue $\\lambda$ thus satisfies the following equation:\n", "$$ \\lambda = \\frac{\\|\\mathbf{Av}\\|_2}{\\|\\mathbf{v}\\|_2} $$"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["eigvector = a1\n", "eigvalue = np.linalg.norm(Aa1) / np.linalg.norm(a1)\n", "print(f'eigenvalue = {eigvalue}')\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["How many more linearly independent eignvectors does `A` have? We can calculate them using the `np.linalg.eig()` function (documentation [here](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html)), which returns all the eigenvectors (re-scaled to have $L^2$ norm of 1) along with their associated eigenvalues:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["eigvals, eigvecs = np.linalg.eig(A)\n", "print(f'eigenvalues:\\n{eigvals}')\n", "print(f'eigenvectors:\\n{eigvecs}')"]}]}