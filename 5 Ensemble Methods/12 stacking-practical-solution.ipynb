{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 4, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Part 1: Stacking with sklearn"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "from scipy.stats import sem\n", "from numpy.random import permutation\n", "\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.model_selection import cross_val_score, KFold\n", "from sklearn.ensemble import StackingRegressor\n", "from sklearn.tree import DecisionTreeRegressor\n", "from sklearn.svm import SVR\n", "from sklearn.metrics import mean_squared_error"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Wine quality prediction"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We will use a dataset of wines, with their attributes and quality. The goal is to predict the quality from the given attributes for unseen data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["data = pd.read_csv(\"data/wine_quality.csv\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Inspect the dataset with `.head()`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "data.head()\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Create an input data array `X` (with all columns except quality), and the output array `y`. Set the output array to `float64` type."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "X, y = data.iloc[:,:-1].values, data.iloc[:,-1].values\n", "y = y.astype('float64')\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Print how many rows and columns your input array contains."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "X.shape\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Divide your input data into 5-fold cross-validation sets, using the `KFold` method. Assign it to the variable `cv`.\n", "\n", "Use randomized splits by setting the `shuffle` attribute to `True`. This is required since our data has not been randomized and is in fact ordered by wine type (reds and whites). Not randomizing the splits would make different sample distributions for the training and test sets."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "cv = KFold(n_splits=5, shuffle=True)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Training multiple classifiers"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's start by training different kinds of classifiers, which we will bundle together later.\n", "\n", "Train a Linear Regression model by iterating over the 5-fold data split you created, fitting a model to the training data, and calculating the prediction error for the test data. Use the `mean_squared_error()` metric. \n", "\n", "Save the final scores to `scores_linreg` and print it."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "scores_linreg = []\n", "for train_index, test_index in cv.split(X):\n", "    X_train, X_test = X[train_index], X[test_index]\n", "    y_train, y_test = y[train_index], y[test_index]\n", "    \n", "    model_linreg = LinearRegression().fit(X_train, y_train)\n", "    \n", "    error = mean_squared_error(model_linreg.predict(X_test), y_test)\n", "    scores_linreg += [error]\n", "scores_linreg\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Now let us train a Support Vector Regression (`SVR`) model. This time use the convenience method `cross_val_score()` that implements the cross-validation and scoring procedure. Set the `scoring` attribute to `\"neg_mean_squared_error\"`.\n", "\n", "Save the final scores to `scores_svr` and print it."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "model_svr = SVR()\n", "scores_svr = cross_val_score(\n", "    model_svr,\n", "    X,\n", "    y,\n", "    scoring='neg_mean_squared_error',\n", "    cv=cv\n", ")\n", "scores_svr\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["As a third model, train a `DecisionTreeRegressor`, again using the `cross_val_score()`. \n", "\n", "Save the final scores to `scores_dtr` and print it."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "model_dtr = DecisionTreeRegressor()\n", "scores_dtr = cross_val_score(\n", "    model_dtr,\n", "    X,\n", "    y,\n", "    scoring='neg_mean_squared_error',\n", "    cv=cv\n", ")\n", "scores_dtr\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Creating a Stacking ensemble"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The Stacking method takes individual prediction models and combines their predictions through an additional model.\n", "\n", "Create a Stacking model with `StackingRegressor()` that combines the three types of models we have used (Linear Regression, SVR and Decision Tree Regressor) and uses a Linear Regression model as the final layer. \n", "\n", "Set it up with a 6-fold cross-validation split. Note that this is the level-2 cross-validation split, which is different to the cross-validation of the dataset done at the start.\n", "\n", "Fit and score the model with `cross_val_score()`. Save the scores to `score_stack` and print it."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "stack_linreg = LinearRegression()\n", "model_stack = StackingRegressor(\n", "    [\n", "        (\"linreg\", LinearRegression()),\n", "        (\"svc\", SVR()),\n", "        (\"dtr\", DecisionTreeRegressor())\n", "    ],\n", "    final_estimator=stack_linreg,\n", "    cv=6\n", ")\n", "scores_stack = cross_val_score(\n", "    model_stack,\n", "    X,\n", "    y,\n", "    scoring='neg_mean_squared_error',\n", "    cv=cv\n", ")\n", "scores_stack\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Print the average score of each for the four approaches. Note that the sign of the score may be inverted depending on the scoring method used.\n", "\n", "Which model has the smallest test error?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "np.mean(scores_linreg), np.mean(scores_svr), np.mean(scores_dtr), np.mean(scores_stack)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Create a bar plot to visualize the resulting scores, with `plt.bar()`.\n", "\n", "Calculate the standard error of the mean with `sem()` and use it for the `yerr` parameter.\n", "\n", "Name each bar with the model label.\n", "\n", "Set the vertical axis lower limit with `ylim(0.4)`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "scores = np.array([scores_linreg, -scores_svr, -scores_dtr, -scores_stack])\n", "labels = [\"linreg\", \"svc\", \"dtr\", \"stacking\"]\n", "\n", "plt.bar(range(4), scores.mean(axis=1), yerr=sem(scores.T))\n", "plt.xticks(range(4), labels)\n", "plt.ylim(0.4)\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The Stacking model combined the predictions of the individual models. As some models give better predictions than others, this should be reflected in how the stacking model weights each of them.\n", "\n", "To inspect this, create a Stacking model as above (or reuse it) and now fit it to the whole dataset `X`. \n", "\n", "Print the final coefficients of the meta-learner layer, stored under `.final_estimator_.coef_`. Which model is given more weight and why?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "stack = model_stack.fit(X, y)\n", "stack.final_estimator_.coef_\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Part 2: Blending implementation"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Blending is a simplified version of Stacking. Instead of using K-fold cross-validation, it uses one single split of the training data: one for the individual models, one for the meta-learner.\n", "\n", "Here we will implement this ensemble method for the same data and prediction models as in Part 1. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Level-1 Blending model"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let us start by implementing only level-1 (without level-2 cross-validation)."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Divide the dataset `X` and `y` into training and validation sets (`X_train`, `y_train`, `X_val`, `y_val`), with 500 samples.\n", "\n", "You may use `permutation()` to shuffle your data samples."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "inds = permutation(len(X))\n", "X_train, y_train = X[inds[:-500]], y[inds[:-500]]\n", "X_val, y_val = X[inds[-500:]], y[inds[-500:]]\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Create the three individual prediction models, as in Part 1, and fit them to the training set."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "model_linreg = LinearRegression().fit(X_train, y_train)\n", "model_svr = SVR().fit(X_train, y_train)\n", "model_dtr = DecisionTreeRegressor().fit(X_train, y_train)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Stack their predictions for the validation set into a into a single array."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "stacked_output = np.array(\n", "    [\n", "        model_linreg.predict(X_val),\n", "        model_svr.predict(X_val), \n", "        model_dtr.predict(X_val)\n", "    ]\n", ").T\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Create a Linear Regression meta-learner and fit it to the validation set."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "meta_learner = LinearRegression().fit(stacked_output, y_val)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Create a function that implements the Blending prediction, by combining the prediction of the individual models and the meta-learner."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "blending = lambda x: meta_learner.predict(np.array(\n", "    [\n", "        model_linreg.predict(x), \n", "        model_svr.predict(x),\n", "        model_dtr.predict(x)\n", "    ]\n", ").T)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Check the error of the Blending model to the data `X`. \n", "\n", "Note that this is the training error, not the test error, of our Blending model, as we did not do level-2 cross-validation yet, and it will be lower than the test error."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "mean_squared_error(y, blending(X))\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Combine the previous steps into a function named `train_blending(X_train, y_train, X_val, y_val)`, that takes a split dataset and returns the Blending model. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "def train_blending(X_train, y_train, X_val, y_val):\n", "    model_linreg = LinearRegression().fit(X_train, y_train)\n", "    model_svr = SVR().fit(X_train, y_train)\n", "    model_dtr = DecisionTreeRegressor().fit(X_train, y_train)\n", "    \n", "    blending = lambda x: meta_learner.predict(np.array(\n", "        [\n", "            model_linreg.predict(x), \n", "            model_svr.predict(x),\n", "            model_dtr.predict(x)\n", "        ]\n", "    ).T)\n", "    \n", "    return blending\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Level-2 cross-validation"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Now we will do things properly and first divide the dataset in a level-2 cross-validation split."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Create a 5-Fold randomized split of the dataset with `KFold()`. Iterate over the splits, each time creating the Blending model with `train_blending()` and the training set, and saving the error for the test set. Print the average error and compare with the models in Part 1."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "cv1 = 5\n", "val_size = 500\n", "\n", "splits = KFold(n_splits=cv1, shuffle=True).split(X)\n", "\n", "scores = []\n", "for train_index, test_index in splits:\n", "    X_train, X_test = X[train_index], X[test_index]\n", "    y_train, y_test = y[train_index], y[test_index]\n", "    \n", "    model = train_blending(\n", "        X_train[:-val_size],\n", "        y_train[:-val_size],\n", "        X_train[-val_size:],\n", "        y_train[-val_size:]\n", "    )\n", "    scores += [mean_squared_error(y_test, model(X_test))]\n", "    \n", "score_blending = np.mean(scores)\n", "print(score_blending)\n"]}]}