{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 4, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Random Forests\n", "\n", "In this practical we will look at using random forests for a classification task: predicting the outcome of crowdfunded projects on Kickstarter."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Classification - Predicting Kickstarter Success\n", "\n", "`kickstarter_sample.csv` contains a random sample of 5000 Kickstart projects. \n", "\n", "The features are as follows:\n", "\n", "| Feature       \t| Description                                             \t|\n", "|---------------\t|---------------------------------------------------------\t|\n", "| main_category \t| Major category the project belongs to                   \t|\n", "|      category \t| More detailed category label                            \t|\n", "|          goal \t| How much money the project wants to raise               \t|\n", "|       backers \t| How many people had donated, when time was up           \t|\n", "|       country \t| Country the project came from                           \t|\n", "|       outcome \t| Whether the project was successful, failed or cancelled \t|\n", "\n", "The code below loads in the data then splits it into features (`X`) and labels (`y`)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import pandas as pd\n", "\n", "data = pd.read_csv('data/kickstarter_sample.csv')\n", "\n", "X = data.drop(['outcome'], axis=1)\n", "y = data['outcome']"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Exploring the data\n", "\n", "Have a quick look at `data`.\n", "\n", "How balanced are the three classes in `state`? (Tip: use `.value_counts()` and specify `normalize=True`)\n", "\n", "For each outcome, what was the mean goal? (Tip: use `.groupby()` and then `.mean()` on the relevant column.)\n", "\n", "What do the numbers suggest?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Getting the data ready\n", "\n", "One issue with the data is that it contains categorical variables - `country`, `category`, and `main_category`. We need to encode these so that they can be used in a machine learning model.\n", "\n", "We will use `pandas`'s `get_dummies()` function to one-hot encode the data. Assign the output of `get_dummies()` to `X`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We will also need to split the data up into training/testing sets - evaluating on the same data we learned from is generally a bad idea.\n", "\n", "`sklearn.model_selection` has a function `train_test_split()` which will split up data for us.\n", "\n", "Set `random_state=5`, and call the new variables `X_train`, `X_test`, `y_train`, and `y_test`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Setting a baseline\n", "\n", "Before we move on to using a random forest, it will be useful to know how well a single model performs. This will set a baseline for us to improve upon.\n", "\n", "The code below creates a single Naive Bayes and a single Decision Tree model.\n", "\n", "Use the `.fit()` and `.score()` methods of each model to train them using the training data and evaluate them on the test set."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.naive_bayes import GaussianNB\n", "from sklearn.tree import DecisionTreeClassifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["nb_model = GaussianNB()\n", "\n", "dtc_model = DecisionTreeClassifier(random_state=5)\n", "\n", "# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Accuracy only gives one impression. We have three classes here, so print a classification report for each of the baseline models.\n", "\n", "`sklearn.metrics.classification_report` takes two arguments: the true y labels and a model's predictions.\n", "\n", "You can get predictions for `X_test` by using the `.predict()` method of a trained model.\n", "\n", "Do you observe any differnces in the models besides their accuracy scores?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import classification_report\n", "\n", "# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Training a random forest\n", "\n", "Training a random forest uses the same steps as above.\n", "\n", "Train the model below and get a classification report, as you did for the baseline models.\n", "\n", "How does it compare to a single Decision Tree?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier\n", "\n", "rf = RandomForestClassifier(random_state=5)\n", "\n", "# Your code below\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Optimising a random forest\n", "\n", "So far we have just used the default hyperparameters of the random forest class in `sklearn`: 100 trees, and trees can go as deep as they like.\n", "\n", "One convenient way to find the best hyperparameters in `sklearn` is through grid search. This will try all the combinations you request and return the results.\n", "\n", "The `GridSearchCV` class takes a model, and a dictionary of hyperparameter and values. Then you just fit/train it as usual, using the training data from before.\n", "\n", "We'll try the following:\n", "\n", "1. Different depths of the trees - deeper trees can completely overfit to the training data, which can impact generalisability\n", "2. `min_samples_split` and `min_samples_leaf` - hyperparameters for deciding how the decision trees should be built\n", "3. `class_weight` - we can try assigning different weights to the classes, to help with class imbalance\n", "4. `max_samples` - what proportion of the available data each tree should train on\n", "5. `n_estimators` - the number of trees to train\n", "\n", "This is 3645 combinations! To speed up the process, we will restrict the hyperparameter search for max_depth, min_samples_split and max_samples, but feel free to run the full search on your local machine.\n", "\n", "Below, create a `GridSearchCV` in the same way you would a model: assign it to a variable named `gcv`, pass it the `rf` as your basic model and include the parameters we wish to investigate with `param_grid=params`. Fit it to the training data.\n", "\n", "To speed things up, set `n_jobs=-1` to use all available CPU cores. Set `verbose=2` so you get updates as it proceeds - useful for making sure it is actually working!\n", "\n", "This may take some time..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.model_selection import GridSearchCV\n", "\n", "params = dict(\n", "    max_depth=[None], #, 1, 2],\n", "    min_samples_split=[2], #, 3, 4],\n", "    min_samples_leaf=[1, 2, 3],\n", "    class_weight=[{'canceled': 2, 'failed': 1, 'successful':1},\n", "                  {'canceled': 1, 'failed': 2, 'successful':1},\n", "                  {'canceled': 1, 'failed': 1, 'successful':2}],\n", "    max_samples=[0.75, 0.25], #, 0.9],\n", "    n_estimators=[100, 200, 300]\n", ")\n", "\n", "rf = RandomForestClassifier(random_state=5, n_jobs=-1)\n", "\n", "# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# What was the best model?\n", "\n", "Note that the `GridSearchCV` evaluated each possible model using the accuracy metric.\n", "\n", "The best model is stored inside `gcv` as `best_estimator_`. Its score is in `gcv.best_score_` and the actual hyperparameters used are in `gcv.best_params_`.\n", "\n", "Take a look at these and for the best model create a `classification_report` as you did before, using the test set. How does it compare to the non-optimised model?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Most useful features\n", "\n", "Now, using the best model, let's look at the `feature_importances_` attribute. This is an array of importance scores for each feature.\n", "\n", "In this case, it will align with the columns of our training data, `X_train`.\n", "\n", "Create a DataFrame from a dictionary that has two keys `feature` (with values from `X_train.columns`) and `score` (with values from `feature_importances_`).\n", "\n", "Sort the DataFrame by the `score` column.\n", "\n", "which are the most useful features for the model?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Conclusion\n", "\n", "Ensemble methods generally perform better than single models. Although they are not as fully interpretable as some individual models (e.g. decision trees), it is still possible to gain some insight into what features are most useful for your task.\n", "\n", "The improvements over the baselines were quite good. Fine-tuning the hyperparameters gave only very modest improvements, but this is more likely due to the features used. Think about what kind of features you would ideally have for modeling project funding success/failure. Overall, this illustrates how powerful random forests can be right out of the box."]}]}