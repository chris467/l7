{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Prudential risk prediction"]},{"cell_type":"markdown","metadata":{},"source":["KATE expects your code to define variables with specific names that correspond to certain things we are interested in.\n","\n","KATE will run your notebook from top to bottom and check the latest value of those variables, so make sure you don't overwrite them.\n","\n","* Remember to uncomment the line assigning the variable to your answer and don't change the variable or function names.\n","* Use copies of the original or previous DataFrames to make sure you do not overwrite them by mistake.\n","\n","You will find instructions below about how to define each variable.\n","\n","Once you're happy with your code, upload your notebook to KATE to check your feedback."]},{"cell_type":"markdown","metadata":{},"source":["We will work with a dataset published by an insurance company which contains anonymised information about their clients.\n","\n","The aim is to predict people's risk profile based on their properties.\n","\n","You will be given a description of the data set and the goal is to develop a prediction model."]},{"cell_type":"markdown","metadata":{},"source":["##  Dataset"]},{"cell_type":"markdown","metadata":{},"source":["The data provided consists of three csv files in the `data/` folder:\n","* `X_train.csv`: the training set\n","* `y_train.csv`: the target for the training set, valued from 1 to 8\n","* `X_test.csv`: the test set that will be evaluated\n","\n","Below we give the description of the data features, some categorical, others numerical. The dataset has been thoroughly anonymized, which makes it extra challenging. \n","\n","Although the risk profile is ordered, we will consider this problem as being a classification problem and the exact category accuracy will be used for evaluating your model. It has low signal, and a 8-classes classification problem, hence accuracy can be quite low."]},{"cell_type":"markdown","metadata":{},"source":["## Get Started\n","\n","Your task is to train a model to predict the target variable. You should save the predictions for the test set in the variable called `y_pred`, which will be evaluated against the ground truth. Below we give you a sample baseline implementation.\n","\n","You are free to use all your modelling skills to get the best possible performance.\n","\n","Good luck!"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset info\n","\n","**Variable descriptions:**\n","- Id - A unique identifier associated with an application.\n","- Product_Info_1-7 - A set of normalized variables relating to the product applied for\n","- Ins_Age - Normalized age of applicant\n","- Ht - Normalized height of applicant\n","- Wt - Normalized weight of applicant\n","- BMI - Normalized BMI of applicant\n","- Employment_Info_1-6 - A set of normalized variables relating to the employment history of the applicant.\n","- InsuredInfo_1-6 - A set of normalized variables providing information about the applicant.\n","- Insurance_History_1-9 - A set of normalized variables relating to the insurance history of the applicant.\n","- Family_Hist_1-5 - A set of normalized variables relating to the family history of the applicant.\n","- Medical_History_1-41 - A set of normalized variables relating to the medical history of the applicant.\n","- Medical_Keyword_1-48 - A set of dummy variables relating to the presence of/absence of a medical keyword being associated with the application.\n","- Response - This is the target variable, an ordinal variable relating to the final decision associated with an application\n","\n","**Categorical (nominal) features:**\n","```\n","Product_Info_1, Product_Info_2, Product_Info_3, Product_Info_5, Product_Info_6, Product_Info_7, Employment_Info_2, Employment_Info_3, Employment_Info_5, InsuredInfo_1, InsuredInfo_2, InsuredInfo_3, InsuredInfo_4, InsuredInfo_5, InsuredInfo_6, InsuredInfo_7, Insurance_History_1, Insurance_History_2, Insurance_History_3, Insurance_History_4, Insurance_History_7, Insurance_History_8, Insurance_History_9, Family_Hist_1, Medical_History_2, Medical_History_3, Medical_History_4, Medical_History_5, Medical_History_6, Medical_History_7, Medical_History_8, Medical_History_9, Medical_History_11, Medical_History_12, Medical_History_13, Medical_History_14, Medical_History_16, Medical_History_17, Medical_History_18, Medical_History_19, Medical_History_20, Medical_History_21, Medical_History_22, Medical_History_23, Medical_History_25, Medical_History_26, Medical_History_27, Medical_History_28, Medical_History_29, Medical_History_30, Medical_History_31, Medical_History_33, Medical_History_34, Medical_History_35, Medical_History_36, Medical_History_37, Medical_History_38, Medical_History_39, Medical_History_40, Medical_History_41\n","```\n","\n","**Continuous features:**\n","```\n","Product_Info_4, Ins_Age, Ht, Wt, BMI, Employment_Info_1, Employment_Info_4, Employment_Info_6, Insurance_History_5, Family_Hist_2, Family_Hist_3, Family_Hist_4, Family_Hist_5\n","```\n","\n","**Discrete features:**\n","```\n","Medical_History_1, Medical_History_10, Medical_History_15, Medical_History_24, Medical_History_32\n","Medical_Keyword_1-48 are dummy variables.\n","```"]},{"cell_type":"markdown","metadata":{},"source":["### Baseline model"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false},"outputs":[],"source":["import pandas as pd\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import  make_column_transformer\n","\n","X_train = pd.read_csv(\"data/X_train.csv\")\n","y_train = pd.read_csv(\"data/y_train.csv\")\n","X_test = pd.read_csv(\"data/X_test.csv\")\n","\n","categories = [\"Product_Info_1\", \"Product_Info_2\", \"Product_Info_3\",\n","              \"Product_Info_5\", \"Product_Info_6\", \"Product_Info_7\"]\n","\n","preprocessor = make_column_transformer((OneHotEncoder(handle_unknown=\"ignore\"), categories))\n","    \n","model = make_pipeline(preprocessor, DecisionTreeClassifier())\n","\n","model.fit(X_train, y_train)\n","\n","y_pred = model.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["Creating a model for a multi-class classification problem with a mixture of categorical, numerical, and ordinal data involves several steps. Here’s a comprehensive guide on how to handle this:\n","\n","1. **Data Preparation:**\n","   - Handle missing values.\n","   - Encode categorical variables.\n","   - Scale numerical variables.\n","   - Encode ordinal variables appropriately.\n","\n","2. **Model Selection:**\n","   - Choose a suitable classifier. For this example, we’ll use `GradientBoostingClassifier`.\n","\n","3. **Train-Test Split:**\n","   - Split the data into training and testing sets.\n","\n","4. **Hyperparameter Tuning:**\n","   - Optionally, use techniques like GridSearchCV for hyperparameter optimization.\n","\n","5. **Model Training and Evaluation:**\n","   - Train the model and evaluate its performance.\n","\n","### Example Code\n","\n","Here’s a detailed example using the `GradientBoostingClassifier` from scikit-learn:\n","\n","```python\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Sample data creation\n","# Assume data has a mix of numerical, categorical, and ordinal features\n","data = {\n","    'numerical_feature1': np.random.randn(1000),\n","    'numerical_feature2': np.random.randn(1000),\n","    'categorical_feature': np.random.choice(['cat1', 'cat2', 'cat3'], size=1000),\n","    'ordinal_feature': np.random.choice(['low', 'medium', 'high'], size=1000),\n","    'target': np.random.choice(range(8), size=1000)  # 8 different classes\n","}\n","df = pd.DataFrame(data)\n","\n","# Define features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Define the preprocessing for numerical, categorical, and ordinal features\n","numerical_features = ['numerical_feature1', 'numerical_feature2']\n","categorical_features = ['categorical_feature']\n","ordinal_features = ['ordinal_feature']\n","\n","# Define ordinal categories order\n","ordinal_categories = [['low', 'medium', 'high']]\n","\n","# Preprocessing pipelines\n","numerical_transformer = Pipeline(steps=[\n","    ('scaler', StandardScaler())\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","])\n","\n","ordinal_transformer = Pipeline(steps=[\n","    ('ordinal', OrdinalEncoder(categories=ordinal_categories))\n","])\n","\n","# Combine preprocessing steps\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numerical_transformer, numerical_features),\n","        ('cat', categorical_transformer, categorical_features),\n","        ('ord', ordinal_transformer, ordinal_features)\n","    ])\n","\n","# Define the model\n","model = GradientBoostingClassifier(random_state=42)\n","\n","# Create a pipeline that includes preprocessing and the model\n","clf = Pipeline(steps=[\n","    ('preprocessor', preprocessor),\n","    ('classifier', model)\n","])\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Optionally, perform hyperparameter tuning with GridSearchCV\n","param_grid = {\n","    'classifier__n_estimators': [50, 100, 200],\n","    'classifier__learning_rate': [0.01, 0.1, 0.5],\n","    'classifier__max_depth': [3, 5, 7]\n","}\n","\n","grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n","grid_search.fit(X_train, y_train)\n","\n","# Print the best parameters and best score\n","print(f\"Best Parameters: {grid_search.best_params_}\")\n","print(f\"Best Cross-Validation Score: {grid_search.best_score_}\")\n","\n","# Train the best model on the full training set\n","best_model = grid_search.best_estimator_\n","best_model.fit(X_train, y_train)\n","\n","# Predict on the test data\n","y_pred = best_model.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","print(f\"Test Accuracy: {accuracy}\")\n","print(\"Classification Report:\")\n","print(report)\n","```\n","\n","### Explanation\n","\n","1. **Sample Data Creation:**\n","   - A synthetic dataset with numerical, categorical, and ordinal features is created.\n","\n","2. **Preprocessing:**\n","   - Numerical features are scaled using `StandardScaler`.\n","   - Categorical features are one-hot encoded using `OneHotEncoder`.\n","   - Ordinal features are encoded using `OrdinalEncoder` with a specified order.\n","\n","3. **Pipeline:**\n","   - A pipeline is created to combine preprocessing steps and the classifier.\n","\n","4. **Train-Test Split:**\n","   - The data is split into training and testing sets.\n","\n","5. **Hyperparameter Tuning:**\n","   - `GridSearchCV` is used to find the best hyperparameters for the `GradientBoostingClassifier`.\n","\n","6. **Training and Evaluation:**\n","   - The model is trained using the best parameters found, and its performance is evaluated on the test set.\n","\n","By following these steps, you can effectively build and optimize a gradient boosting model for a multi-class classification problem with mixed data types."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}
