{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Ensembles for Customer Satisfaction Prediction"]},{"cell_type":"markdown","metadata":{},"source":["KATE expects your code to define variables with specific names that correspond to certain things we are interested in.\n","\n","KATE will run your notebook from top to bottom and check the latest value of those variables, so make sure you don't overwrite them.\n","\n","* Remember to uncomment the line assigning the variable to your answer and don't change the variable or function names.\n","* Use copies of the original or previous DataFrames to make sure you do not overwrite them by mistake.\n","\n","You will find instructions below about how to define each variable.\n","\n","Once you're happy with your code, upload your notebook to KATE to check your feedback."]},{"cell_type":"markdown","metadata":{},"source":["Businesses can improve their services by tailoring them to individual customers. One important factor is knowing when customers are dissatisfied. Based on their records, one can use machine learning tools to make predictions about which customers are more at risk of being dissatisfied than others. Such predictions allow for individualized actions that may help retain customers and will improve quality."]},{"cell_type":"markdown","metadata":{},"source":["In this assignment, we will build a prediction model for bank account owners' satisfaction. The record includes more than 300 features for each client, including variable related to their balance and which banking operations they have performed. Many of these variables are sparse; some numerical, some categorical. \n","\n","Ensemble methods based on decision trees, such as random forests and boosting algorithms, have been very successful in modeling such heterogeneous tabular data. To learn how these models work, you will implement them step-by-step, and see how the performance of your predictions improve."]},{"cell_type":"markdown","metadata":{},"source":["### Load the data"]},{"cell_type":"markdown","metadata":{},"source":["Load the data in `data/train_data.csv` with `pandas`. Inspect its content with `.head()`, `.shape` and other methods of your choice."]},{"cell_type":"code","execution_count":1,"metadata":{"scrolled":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>var3</th>\n","      <th>var15</th>\n","      <th>imp_ent_var16_ult1</th>\n","      <th>imp_op_var39_comer_ult1</th>\n","      <th>imp_op_var39_comer_ult3</th>\n","      <th>imp_op_var40_comer_ult1</th>\n","      <th>imp_op_var40_comer_ult3</th>\n","      <th>imp_op_var40_efect_ult1</th>\n","      <th>imp_op_var40_efect_ult3</th>\n","      <th>...</th>\n","      <th>saldo_medio_var33_hace2</th>\n","      <th>saldo_medio_var33_hace3</th>\n","      <th>saldo_medio_var33_ult1</th>\n","      <th>saldo_medio_var33_ult3</th>\n","      <th>saldo_medio_var44_hace2</th>\n","      <th>saldo_medio_var44_hace3</th>\n","      <th>saldo_medio_var44_ult1</th>\n","      <th>saldo_medio_var44_ult3</th>\n","      <th>var38</th>\n","      <th>TARGET</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>47739</td>\n","      <td>2</td>\n","      <td>29</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>46565.040000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4212</td>\n","      <td>2</td>\n","      <td>38</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>90736.770000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>48967</td>\n","      <td>2</td>\n","      <td>23</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>117310.979016</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11077</td>\n","      <td>2</td>\n","      <td>23</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>172107.720000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17475</td>\n","      <td>2</td>\n","      <td>26</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>67983.570000</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 371 columns</p>\n","</div>"],"text/plain":["   Unnamed: 0  var3  var15  imp_ent_var16_ult1  imp_op_var39_comer_ult1  \\\n","0       47739     2     29                 0.0                      0.0   \n","1        4212     2     38                 0.0                      0.0   \n","2       48967     2     23                 0.0                      0.0   \n","3       11077     2     23                 0.0                      0.0   \n","4       17475     2     26                 0.0                      0.0   \n","\n","   imp_op_var39_comer_ult3  imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n","0                      0.0                      0.0                      0.0   \n","1                      0.0                      0.0                      0.0   \n","2                      0.0                      0.0                      0.0   \n","3                      0.0                      0.0                      0.0   \n","4                      0.0                      0.0                      0.0   \n","\n","   imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  ...  \\\n","0                      0.0                      0.0  ...   \n","1                      0.0                      0.0  ...   \n","2                      0.0                      0.0  ...   \n","3                      0.0                      0.0  ...   \n","4                      0.0                      0.0  ...   \n","\n","   saldo_medio_var33_hace2  saldo_medio_var33_hace3  saldo_medio_var33_ult1  \\\n","0                      0.0                      0.0                     0.0   \n","1                      0.0                      0.0                     0.0   \n","2                      0.0                      0.0                     0.0   \n","3                      0.0                      0.0                     0.0   \n","4                      0.0                      0.0                     0.0   \n","\n","   saldo_medio_var33_ult3  saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\n","0                     0.0                      0.0                      0.0   \n","1                     0.0                      0.0                      0.0   \n","2                     0.0                      0.0                      0.0   \n","3                     0.0                      0.0                      0.0   \n","4                     0.0                      0.0                      0.0   \n","\n","   saldo_medio_var44_ult1  saldo_medio_var44_ult3          var38  TARGET  \n","0                     0.0                     0.0   46565.040000       0  \n","1                     0.0                     0.0   90736.770000       0  \n","2                     0.0                     0.0  117310.979016       0  \n","3                     0.0                     0.0  172107.720000       0  \n","4                     0.0                     0.0   67983.570000       0  \n","\n","[5 rows x 371 columns]"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","data = pd.read_csv('data/train_data.csv')\n","#data = pd.read_csv('train_data.csv')\n","data.head()"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["(66020, 371)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["data.shape"]},{"cell_type":"markdown","metadata":{},"source":["#### Target variable\n","\n","The last column, named `TARGET`, is the variable to be predicted. `TARGET=1` represents a dissatisfied customer. Inspect the target column with `.value_counts()`. \n","\n","What is the proportion of dissatisfied customers? Is the dataset balanced or imbalanced?"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["TARGET\n","0    0.960436\n","1    0.039564\n","Name: proportion, dtype: float64"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data['TARGET'].value_counts(normalize=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Note on dataset properties\n","\n","As you can see, the dataset is highly imbalanced: there are only 2.6k positive entries and 63.4k negative entries. It definitely should be addressed in the models by introducing class_weight parameter where possible (there are different ways it can be done - feel free check it out in sklearn documentation).\n","\n","If that is not possible to introduce class weights for the model due to the model type, be ready to the permanent majority class vote in the output. This can be addressed by tweaking the model parameters."]},{"cell_type":"markdown","metadata":{},"source":["Separate the data into features `X` and target `y`. Split the data into training and validation sets, with validation set of 5000 samples, with stratified split to keep the same level of imbalance.\n","\n","*Hint: you may use `train_test_split()` for stratified splits.*"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X = data.copy().drop(columns = 'TARGET')\n","y = data['TARGET']\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 5000, stratify = y)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(61020, 370) (5000, 370) (61020,) (5000,)\n"]}],"source":["print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Basic modelling pipeline\n","\n","Implement a basic modelling pipeline for a Decision Tree Classifier, fitting the training data and printing the training and validation accuracy."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'dtc__max_depth': 3, 'dtc__min_samples_split': 5}\n","0.9604555883316945\n","0.9604\n"]}],"source":["# fit the training data\n","# printing the training and validation accuracy\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score, make_scorer\n","from sklearn.model_selection import GridSearchCV\n","\n","pipeline_dt = Pipeline([('dtc',DecisionTreeClassifier())])\n","\n","pipelines = [pipeline_dt]\n","\n","for pipe in pipelines:\n","    #pipe.fit(X_train, y_train)\n","    parameters  = [{'dtc__max_depth': [3], \"dtc__min_samples_split\": [5]}] \n","    scoring = make_scorer(accuracy_score)\n","    gridCV = GridSearchCV(pipe, parameters, scoring = scoring, cv= 10)\n","    gridCV.fit(X_train, y_train)\n","    print(gridCV.best_params_)\n","    best_model = gridCV.best_estimator_\n","    print(accuracy_score(y_train, best_model.predict(X_train)))\n","    print(accuracy_score(y_val, best_model.predict(X_val)))\n"]},{"cell_type":"markdown","metadata":{},"source":["Note that the prediction score is quite high, even for this very simple model. Take a moment to think why this high score is not that significant.\n","\n","#### ROC curve metric\n","\n","Change your scoring metric to `roc_auc_score`, which calculates the area below the ROC curve of your **prediction probabilities**, instead of using the binary prediction decisions.\n","\n","*Hint: Use the probabilities for `y = True` (not `y = False`).*"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Accuracy: 0.9604555883316945\n","Validation Accuracy: 0.9604\n"]}],"source":["from sklearn.metrics import roc_auc_score\n","\n","for pipe in pipelines:\n","    parameters  = [{'dtc__max_depth': [3], \"dtc__min_samples_split\": [5]}] \n","    scoring = make_scorer(roc_auc_score)\n","    gridCV = GridSearchCV(pipe, parameters, scoring = scoring, cv= 10)\n","    gridCV.fit(X_train, y_train)\n","    best_model = gridCV.best_estimator_\n","\n","    # Evaluate the best model on the training data using probabilities for y=True\n","    train_probs = best_model.predict_proba(X_train)[:, 1]\n","    train_predictions = (train_probs > 0.5).astype(int)\n","    train_accuracy = accuracy_score(y_train, train_predictions)\n","    print(\"Training Accuracy:\", train_accuracy)\n","    \n","    # Evaluate the best model on the validation data using probabilities for y=True\n","    val_probs = best_model.predict_proba(X_val)[:, 1]\n","    val_predictions = (val_probs > 0.5).astype(int)\n","    val_accuracy = accuracy_score(y_val, val_predictions)\n","    print(\"Validation Accuracy:\", val_accuracy)"]},{"cell_type":"markdown","metadata":{},"source":["#### Baseline score for random predictions\n","\n","Calculate the ROC AUC for random uniform prediction probabilities. \n","\n","Is the Decision Tree better? Based on the training and validation scores, what is the problem with the Decision Tree model?\n","\n","*Hint: You can use `np.random.uniform`.*"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Predictions Accuracy: 0.4936\n"]}],"source":["import numpy as np\n","\n","random_probs = np.random.uniform(size = 5000)\n","random_predictions = (random_probs > 0.5).astype(int)\n","random_predictions_accuracy = accuracy_score(y_val, random_predictions)\n","print(\"Random Predictions Accuracy:\", random_predictions_accuracy)"]},{"cell_type":"markdown","metadata":{},"source":["Create a function named `test_model(model, X_train, y_train, X_test, y_test)` that performs the basic prediction pipeline, receiving as argument the model and data, fitting the training data, and returning the training and test prediction scores. Check that it works with the Decision Tree model."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def test_model(model, X_train=X_train, y_train=y_train, X_test= X_val, y_test= y_val):\n","    # set up the model\n","    dtc = model\n","    # fit the training data\n","    dtc.fit(X_train, y_train)\n","    # return the training and test prediction scores\n","    train_score = dtc.score(X_train, y_train)\n","    test_score = dtc.score(X_test, y_test)\n","    print('Train Score',train_score)\n","    print('Test Score',test_score)\n","    return train_score, test_score\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Score 1.0\n","Test Score 0.9246\n"]},{"data":{"text/plain":["(1.0, 0.9246)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["test_model(DecisionTreeClassifier())"]},{"cell_type":"markdown","metadata":{},"source":["## Optimizing decision trees "]},{"cell_type":"markdown","metadata":{},"source":["We can improve the prediction model by setting up the Decision Tree. Check the arguments available for the `DecisionTreeClassifier` class. \n","\n","Which arguments do you think could improve the validation score? Optimize your model by changing the meta-parameters. Inspect the most important meta-parameter by calculating the training and validation score for different values."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'dtc__criterion': 'gini', 'dtc__max_depth': 10, 'dtc__max_leaf_nodes': 90, 'dtc__min_samples_leaf': 2, 'dtc__min_samples_split': 3, 'dtc__splitter': 'best'}\n"]}],"source":["from sklearn.metrics import roc_auc_score\n","\n","for pipe in pipelines:\n","    parameters  = [{'dtc__criterion':['gini'],\n","                    'dtc__splitter':['best'],\n","                    'dtc__max_depth': [10],\n","                    'dtc__min_samples_split': [3],\n","                    'dtc__min_samples_leaf': [2],\n","                    'dtc__max_leaf_nodes': [90]\n","               }] \n","    scoring = make_scorer(roc_auc_score)\n","    gridCV = GridSearchCV(pipe, parameters, scoring = scoring, cv= 5)\n","    gridCV.fit(X_train, y_train)\n","    #best_model = gridCV.best_estimator_\n","    print(gridCV.best_params_)\n"]},{"cell_type":"markdown","metadata":{},"source":["To evaluate your models, we will test your data on a testing set. Load the test at `data/test_data.csv`."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["test_data = pd.read_csv('data/test_data.csv')\n","#test_data = pd.read_csv('test_data.csv')\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>var3</th>\n","      <th>var15</th>\n","      <th>imp_ent_var16_ult1</th>\n","      <th>imp_op_var39_comer_ult1</th>\n","      <th>imp_op_var39_comer_ult3</th>\n","      <th>imp_op_var40_comer_ult1</th>\n","      <th>imp_op_var40_comer_ult3</th>\n","      <th>imp_op_var40_efect_ult1</th>\n","      <th>imp_op_var40_efect_ult3</th>\n","      <th>imp_op_var40_ult1</th>\n","      <th>...</th>\n","      <th>saldo_medio_var29_ult3</th>\n","      <th>saldo_medio_var33_hace2</th>\n","      <th>saldo_medio_var33_hace3</th>\n","      <th>saldo_medio_var33_ult1</th>\n","      <th>saldo_medio_var33_ult3</th>\n","      <th>saldo_medio_var44_hace2</th>\n","      <th>saldo_medio_var44_hace3</th>\n","      <th>saldo_medio_var44_ult1</th>\n","      <th>saldo_medio_var44_ult3</th>\n","      <th>var38</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>48</td>\n","      <td>0.0</td>\n","      <td>203.46</td>\n","      <td>322.68</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>89541.870000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>28</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>117310.979016</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>23</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>71340.870000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>46</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>12.69</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>144333.600000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>34</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>117310.979016</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 369 columns</p>\n","</div>"],"text/plain":["   var3  var15  imp_ent_var16_ult1  imp_op_var39_comer_ult1  \\\n","0     2     48                 0.0                   203.46   \n","1     2     28                 0.0                     0.00   \n","2     2     23                 0.0                     0.00   \n","3     2     46                 0.0                     0.00   \n","4     2     34                 0.0                     0.00   \n","\n","   imp_op_var39_comer_ult3  imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n","0                   322.68                      0.0                      0.0   \n","1                     0.00                      0.0                      0.0   \n","2                     0.00                      0.0                      0.0   \n","3                    12.69                      0.0                      0.0   \n","4                     0.00                      0.0                      0.0   \n","\n","   imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  imp_op_var40_ult1  ...  \\\n","0                      0.0                      0.0                0.0  ...   \n","1                      0.0                      0.0                0.0  ...   \n","2                      0.0                      0.0                0.0  ...   \n","3                      0.0                      0.0                0.0  ...   \n","4                      0.0                      0.0                0.0  ...   \n","\n","   saldo_medio_var29_ult3  saldo_medio_var33_hace2  saldo_medio_var33_hace3  \\\n","0                     0.0                      0.0                      0.0   \n","1                     0.0                      0.0                      0.0   \n","2                     0.0                      0.0                      0.0   \n","3                     0.0                      0.0                      0.0   \n","4                     0.0                      0.0                      0.0   \n","\n","   saldo_medio_var33_ult1  saldo_medio_var33_ult3  saldo_medio_var44_hace2  \\\n","0                     0.0                     0.0                      0.0   \n","1                     0.0                     0.0                      0.0   \n","2                     0.0                     0.0                      0.0   \n","3                     0.0                     0.0                      0.0   \n","4                     0.0                     0.0                      0.0   \n","\n","   saldo_medio_var44_hace3  saldo_medio_var44_ult1  saldo_medio_var44_ult3  \\\n","0                      0.0                     0.0                     0.0   \n","1                      0.0                     0.0                     0.0   \n","2                      0.0                     0.0                     0.0   \n","3                      0.0                     0.0                     0.0   \n","4                      0.0                     0.0                     0.0   \n","\n","           var38  \n","0   89541.870000  \n","1  117310.979016  \n","2   71340.870000  \n","3  144333.600000  \n","4  117310.979016  \n","\n","[5 rows x 369 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["test_data.head()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["X_train = X_train.drop(['Unnamed: 0'],axis=1)\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["X_val = X_val.drop(['Unnamed: 0'],axis=1)"]},{"cell_type":"markdown","metadata":{},"source":[" Calculate the prediction probabilities for the test data for the best Decision Tree, saving them in a variable named `dtc_preds`. `dtc_preds` should be an numpy array a single dimension."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["best_dtc = gridCV.best_estimator_\n","best_dtc.fit(X_train, y_train)\n","dtc_preds = best_dtc.predict(test_data)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["numpy.ndarray"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["type(dtc_preds)"]},{"cell_type":"markdown","metadata":{},"source":["### Bagging and Random Forests"]},{"cell_type":"markdown","metadata":{},"source":["While Decision Trees are prone to overfitting, their ensemble can be powerful predictors. Random Forests are essentially Bagging ensembles of decision trees, using the average prediction of the multiple decision trees base models, each trained with a different set of data samples.\n","\n","You will create a Bagging model class, named `myBagging`, filling the class structure below.\n","\n","The `.fit()` method should fit each base model with a bootstrap sample of the data (with replacement), with data size proportional by the meta-parameter `subsample`. That is, if `subsample=0.5`, each base model should get half the total number of samples.\n","\n","The `.predict_proba()` method should estimate and average the prediction probabilities of the base models.\n","\n","*Hint: You can use the `resample()` function for creating bootstrap samples.*"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["from sklearn.utils import resample\n","\n","class myBagging:\n","    def __init__(self, base_models, subsample = 1.):\n","        self.n_models = len(base_models)\n","        self.base_models = base_models\n","        self.subsample = subsample\n","        \n","    def fit(self, X, y):\n","        '''Loop over base models, generate a bootstrap sample of the data with 'resample()',\n","           and fit them to the data.\n","           \n","           To access the variables inside the myBagging class, use the 'self.' prefix, \n","           i.e. self.base_models, self.n_models and self.subsample\n","        '''\n","        n_samples = int(len(X) * self.subsample)\n","        self.fitted_models = []\n","\n","        for base_model in self.base_models:\n","            X_bootstrap, y_bootstrap = resample(X, y, stratify= y, n_samples= n_samples)\n","            base_model.fit(X_bootstrap, y_bootstrap)\n","            self.fitted_models.append(base_model)\n","\n","    def predict_proba(self, X):\n","        '''Return the ensemble predictions, given by the average prediction probability over base models.\n","           It should be an array with the length of the dataset.'''\n","        predictions = np.array([model.predict_proba(X) for model in self.fitted_models])\n","        return np.mean(predictions, axis=0)\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["Run and score a Random Forest, with 10 base Decision Trees, with maximum depth 10 and subsample 0.5. Use your `myBagging` class and `test_model()`."]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["#set up model\n","base_model1 = DecisionTreeClassifier(max_depth=10)\n","# create instance\n","my_bagging = myBagging(base_models=[base_model1], subsample= 0.5)\n","# fit model\n","my_bagging.fit(X_train, y_train)\n","# predict proba\n","predictions_proba = my_bagging.predict_proba(test_data)\n","# predictions\n","predictions = np.argmax(predictions_proba, axis=1)\n","# evaluate accuracy\n","# accuracy = accuracy_score(y_test, predictions)\n","# print\n","# print('Accuracy:', accuracy)"]},{"cell_type":"markdown","metadata":{},"source":["### Extra-Trees\n","\n","Extremely Randomized Trees are decision trees in which, at each node split during training , only a fraction of the features is considered for the optimal split (e.g. for optimal Gini gain). This functionality is implemented on `sklearn` under the parameter `max_features`. \n","\n","Run and score a Extra-Trees version of your Random Forest, by changing the `max_features` parameter."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["369"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["X_train.shape[1]"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# from sklearn.ensemble import RandomForestClassifier\n","\n","# for i in range(1,X_train.shape[1],60):\n","#     rfc = RandomForestClassifier(max_features=i)\n","#     rfc.fit(X_train, y_train)\n","#     print('Max Features:',i,'\\tScore:',rfc.score(X_val, y_val))\n"]},{"cell_type":"markdown","metadata":{},"source":["### Sklearn comparison"]},{"cell_type":"markdown","metadata":{},"source":["For comparison, run and score the `sklearn` implementation, `RandomForestClassifier`."]},{"cell_type":"markdown","metadata":{},"source":["### Optimize your Random Forest"]},{"cell_type":"markdown","metadata":{},"source":["Optimize your Random Forest meta-parameters, both of the myBagging and Decision Trees, and make your predictions for the test data, saving the predictions under `rf_preds`."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\chris\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n"]},{"name":"stdout","output_type":"stream","text":["Best model accuracy: 0.7313339888561128\n","Best hyperparamters: {'bootstrap': True, 'class_weight': 'balanced', 'max_depth': 7, 'max_features': 'auto'}\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import GridSearchCV\n","\n","params = dict(\n","    max_depth=[7],\n","    #min_samples_split=[2, 5, 10],\n","    max_features=['sqrt'],\n","    #min_samples_leaf=[1, 2, 4],\n","    class_weight=['balanced'],\n","    #n_estimators=[100, 200, 300],\n","    bootstrap = [True]#,False]\n",")\n","\n","rfc1 = RandomForestClassifier(n_jobs= -1)\n","\n","gcv = GridSearchCV(estimator=rfc1, param_grid=params, n_jobs=-1, cv= 5)\n","gcv.fit(X_train, y_train)\n","\n","print(f\"Best model accuracy: {gcv.best_score_}\")\n","\n","print(f\"Best hyperparamters: {gcv.best_params_}\")\n","\n","best_model = gcv.best_estimator_\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["\n","rf_preds = best_model.predict(test_data)\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["array([0, 1, 1, ..., 1, 0, 0], dtype=int64)"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["rf_preds"]},{"cell_type":"markdown","metadata":{},"source":["Note that including more decision trees improve performance but increases the computational cost of training linearly. The `max_depth` and `max_features` arguments can heavily cut the training time, by reducing the tree size and number of features considered at each split."]},{"cell_type":"markdown","metadata":{},"source":["## Gradient Boosting"]},{"cell_type":"markdown","metadata":{},"source":["We will now implement a more sophisticated ensemble, Gradient Boosting, in which the base models are trained sequentially. Each new base model predicts what previous base models missed. \n","\n","As gradient boosting requires a continuous gradient, it can only use regression models for the base learner. \n","\n","For this exercise, we will perform regression directly on the 0-1 class labels, and treat the raw outputs as probabilities. \n","\n","We will try to setup the base models to optimise the MSE loss function against the class-labels, for which the gradient becomes simply the residual errors. \n","\n","When applied to probabilities, the MSE is known as the Brier score. \n","\n","Whilst performing this exercise, have a think about whether this is a robust approach. \n","\n","If not, what would you change either to your base-learners, meta-algorithm, or evaluation metrics to make this more robust?\n","\n","You will have a chance to implement your suggestions tomorrow!\n","\n","In the below structure, fill the `.fit()` and `.predict_proba()` functions. "]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["class myGradientBoosting:\n","    \n","    def __init__(self, base_models, learning_rate=0.5):\n","        self.n_models = len(base_models)\n","        self.models = base_models\n","        self.learning_rate = learning_rate\n","    \n","    def fit(self, x, y):\n","        ''' The `.fit()` function should loop over each base model \n","         fitting it to the residual of the ensemble predictions so far, for the MSE loss:\n","         \n","         predictions = 0\n","         for each base model:\n","             residual = y - predictions   \n","             fit base model and make new predictions\n","             predictions = predictions + learning_rate * new_prediction \n","        '''\n","        predictions = np.zeros(len(x))\n","        for i, model in enumerate(self.models):\n","            residual = y - predictions\n","            print(f\"Fitting model {i + 1}/{self.n_models}\")\n","            model.fit(x,residual)\n","            new_prediction = model.predict(x)\n","            predictions += self.learning_rate * new_prediction\n","            print(f\"Model {i + 1} fitted. Predictions updated.\")\n","                   \n","    def predict_proba(self, x):\n","        ''' Generate the ensemble prediction, by looping over each base model.\n","            Get their predictions and sum them, scaled by the learning rate.\n","        \n","            Trick: Regressor models return only one prediction (instead of two probabilities in the Classifiers).\n","                   To make your class compatible with test_model(), you can repeat the predictions, e.g.:\n","                   predictions.reshape(-1,1).repeat(2,axis=1)'''\n","        predictions = np.zeros(len(x))\n","        for i, model in enumerate(self.models):\n","            model_predictions = model.predict(x)\n","            predictions += self.learning_rate * model_predictions\n","            print(f\"Model {i + 1} predictions added.\")\n","        return predictions.reshape(-1,1).repeat(2,axis=1)\n","        \n"]},{"cell_type":"markdown","metadata":{},"source":["Run and score a Gradient Boosting model, with 20 base decision trees, with maximum depth 5, maximum feature 0.5 and learning rate 0.5. Use your `myGradientBoosting` class and `test_model()`. "]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting model 1/20\n","Model 1 fitted. Predictions updated.\n","Fitting model 2/20\n","Model 2 fitted. Predictions updated.\n","Fitting model 3/20\n","Model 3 fitted. Predictions updated.\n","Fitting model 4/20\n","Model 4 fitted. Predictions updated.\n","Fitting model 5/20\n","Model 5 fitted. Predictions updated.\n","Fitting model 6/20\n","Model 6 fitted. Predictions updated.\n","Fitting model 7/20\n","Model 7 fitted. Predictions updated.\n","Fitting model 8/20\n","Model 8 fitted. Predictions updated.\n","Fitting model 9/20\n","Model 9 fitted. Predictions updated.\n","Fitting model 10/20\n","Model 10 fitted. Predictions updated.\n","Fitting model 11/20\n","Model 11 fitted. Predictions updated.\n","Fitting model 12/20\n","Model 12 fitted. Predictions updated.\n","Fitting model 13/20\n","Model 13 fitted. Predictions updated.\n","Fitting model 14/20\n","Model 14 fitted. Predictions updated.\n","Fitting model 15/20\n","Model 15 fitted. Predictions updated.\n","Fitting model 16/20\n","Model 16 fitted. Predictions updated.\n","Fitting model 17/20\n","Model 17 fitted. Predictions updated.\n","Fitting model 18/20\n","Model 18 fitted. Predictions updated.\n","Fitting model 19/20\n","Model 19 fitted. Predictions updated.\n","Fitting model 20/20\n","Model 20 fitted. Predictions updated.\n","Model 1 predictions added.\n","Model 2 predictions added.\n","Model 3 predictions added.\n","Model 4 predictions added.\n","Model 5 predictions added.\n","Model 6 predictions added.\n","Model 7 predictions added.\n","Model 8 predictions added.\n","Model 9 predictions added.\n","Model 10 predictions added.\n","Model 11 predictions added.\n","Model 12 predictions added.\n","Model 13 predictions added.\n","Model 14 predictions added.\n","Model 15 predictions added.\n","Model 16 predictions added.\n","Model 17 predictions added.\n","Model 18 predictions added.\n","Model 19 predictions added.\n","Model 20 predictions added.\n","Predictions:\n","[[0.03366961 0.03366961]\n"," [0.01068065 0.01068065]\n"," [0.05476107 0.05476107]\n"," ...\n"," [0.01234324 0.01234324]\n"," [0.00423715 0.00423715]\n"," [0.21941316 0.21941316]]\n"]}],"source":["from sklearn.tree import DecisionTreeRegressor\n","#set up model\n","\n","#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","base_models = [DecisionTreeRegressor(max_depth=5, max_features=0.5, random_state=i) for i in range(20)]\n","\n","model = myGradientBoosting(base_models, learning_rate=0.5)\n","                           \n","model.fit(X_train, y_train)\n","\n","# Predict using the model\n","predictions_proba = model.predict_proba(X_val)\n","\n","print(\"Predictions:\")\n","print(predictions_proba)                          \n","                           "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["For comparison, run and score the `sklearn` implementation, `GradientBoostingClassifier`."]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.9606\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      1.00      0.98      4802\n","           1       0.67      0.01      0.02       198\n","\n","    accuracy                           0.96      5000\n","   macro avg       0.81      0.50      0.50      5000\n","weighted avg       0.95      0.96      0.94      5000\n","\n"]}],"source":["from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Initialize the GradientBoostingClassifier\n","gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n","\n","# Fit the model to the training data\n","gbc.fit(X_train, y_train)\n","\n","# Predict on the test data\n","y_pred = gbc.predict(X_val)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_val, y_pred)\n","report = classification_report(y_val, y_pred)\n","\n","print(f\"Accuracy: {accuracy}\")\n","print(\"Classification Report:\")\n","print(report)\n"]},{"cell_type":"markdown","metadata":{},"source":["Optimize your myGradientBoosting and decision tree meta-parameters, and make your predictions for the test data, saving the predictions under `gb_preds`."]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# X_train = X_train.drop(['Unnamed: 0'],axis=1)\n","# X_val = X_val.drop(['Unnamed: 0'],axis=1)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 1 candidates, totalling 5 fits\n","Best Parameters: {'learning_rate': 0.01, 'max_depth': 7, 'max_features': 0.5}\n","Best Cross-Validation Score: 0.9604883644706653\n"]}],"source":["# Set up the parameter grid to search over\n","param_grid = {\n","    #'n_estimators': [50, 100, 200],\n","    'learning_rate': [0.01],#, 0.1, 0.5],\n","    'max_depth': [7],#3, 5, 7],\n","    #'subsample': [0.7, 0.8, 0.9],\n","    'max_features': [0.5]#, 0.7], 1.0]\n","}\n","\n","# Initialize the GradientBoostingClassifier\n","gbc = GradientBoostingClassifier(random_state=42)\n","\n","# Set up GridSearchCV\n","grid_search = GridSearchCV(estimator=gbc, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=2)\n","\n","# Fit the grid search to the data\n","grid_search.fit(X_train, y_train)\n","\n","# Print the best parameters and best score\n","print(f\"Best Parameters: {grid_search.best_params_}\")\n","print(f\"Best Cross-Validation Score: {grid_search.best_score_}\")\n","\n","# Train the best model on the full training set\n","best_model = grid_search.best_estimator_\n","best_model.fit(X_train, y_train)\n","\n","# Predict on the test data\n","gb_preds = best_model.predict(test_data)\n","\n","# Evaluate the model\n","# accuracy = accuracy_score(y_test, y_pred)\n","# report = classification_report(y_test, y_pred)\n","\n","# print(f\"Test Accuracy: {accuracy}\")\n","# print(\"Classification Report:\")\n","# print(report)"]},{"cell_type":"markdown","metadata":{},"source":["Try to think about the difference between your implementation and the GradientBoostingClassifier.\n","\n","Are there any fundamental differences? If so, why?\n","\n","You could try looking at the distribution of your output probabilities for each model."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
