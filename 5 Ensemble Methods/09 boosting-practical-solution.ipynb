{"metadata": {"kernelspec": {"display_name": "Python 3.9.9 64-bit (microsoft store)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}, "vscode": {"interpreter": {"hash": "5739bf39c17db21ae60b8ce29dba7a6315d7ad17310a9a52cb1da7909ba2b1a6"}}}, "nbformat": 4, "nbformat_minor": 4, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Boosting with LightGBM\n", "\n", "In this practical we will use the `lightgbm` package to perform classification.\n", "\n", "We'll use a dataset of online behaviour, with the aim of classifying visitors that like to make, or not make, a purchase."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Classification: the data\n", "\n", "The first dataset is from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset).\n", "\n", "Each row summarises a visitor's history on an e-commerce site (12,330 visits in total).\n", "\n", "The features are related to the type of page visited (and duration), various analytics metrics (bounce/exit rates), OS/browser used, region, and so on.\n", "\n", "The binary variable we want to predict is \"Revenue\" - did the user eventually make a purchase?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import pandas as pd\n", "\n", "data = pd.read_csv('data/online_shoppers_intention.csv')\n", "\n", "data.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["data.info()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Preparing the data - I\n", "\n", "`lightgbm` requires data to be `int`, `float`, or `bool`. Categorical items will also need to be identified as such.\n", "\n", "From looking at the dataset above, we have two categorical columns which are `str` type. Some other columns are `int` but represent categorical data too, e.g. `Region`.\n", "\n", "Use the `.astype('category')` method available for `pandas` Series (each column in a DataFrame is a Series) to change these columns to the correct datatype."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "\n", "data[['Month', 'VisitorType', 'OperatingSystems', 'Browser', 'Region', 'TrafficType']] = data[['Month', 'VisitorType', 'OperatingSystems', 'Browser', 'Region', 'TrafficType']].astype('category')\n", "\n", "data.info()\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Preparing the data - II\n", "\n", "As is standard, we will split up the data using `sklearn.model_selection.train_test_split` to create a training set and a test set. We will keep the test set aside to evaluate the models on unseen data, as a measure of how generalisable they are.\n", "\n", "First, separate the predictive columns (the `X`) from the outcome variable (the `y`).\n", "\n", "Then, set up input/output arrays for train/test, at the default ratio of `train_test_split`.\n", "\n", "Call the variables `X_train`, `X_test`, `y_train`, and `y_test`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "# Your code here...\n", "\n", "X = data.drop('Revenue', axis=1)\n", "y = data.Revenue\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y)\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Preparing the data - III\n", "\n", "Normally we need to pre-process data for `sklearn` models. `lightgbm` comes with a `Dataset` class that will handle the data for us. It will use the column names of the `pandas` DataFrame to name the features.\n", "\n", "For more advanced use, it can also apply individual weights to specific items in the data.\n", "\n", "Set up two instances of `lgb.Dataset()` named `lgb_train` and `lgb_test`.\n", "\n", "You can do this by passing the appropriate `X_` and `y_` arrays you created above.\n", "\n", "For `lgb_test`, set the argument `reference=lgb_train`. This is so that when the model is pre-processing data (i.e. binning categorical values into discrete continuous values) it has access to the full range of possible data values, not just the ones that appear in the testing data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import lightgbm as lgb\n", "\n", "# Your code below\n", "\n", "lgb_train = lgb.Dataset(X_train, y_train)\n", "\n", "lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Setting parameters\n", "\n", "`lightgbm` is highly configurable - see [the documentation](https://lightgbm.readthedocs.io/en/latest/Parameters.html) for a full list.\n", "\n", "Some parameters are related to:\n", "\n", "- performance: Using the GPU or multiple machines to speed up training\n", "- task: regression, classification (binary or multi), ranking\n", "- boosting algorithm: gradient boosting decision trees? Random forests? Something else?\n", "- ensemble model parameters: learning rate, number of models in ensemble\n", "- model-specific parameters: depth of decision trees, number of nodes per level\n", "\n", "For now we will focus on parameters which are related to:\n", "\n", "* our binary classification task (the objective and metric used)\n", "* how boosting is done (the learning rate for gradient descent)\n", "* decision trees (number of leaves, depth)\n", "* ensemble (how many trees)\n", "\n", "The values in the dictionary are the default ones, besides `objective` and `metric`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["params = {\n", "    'boosting_type': 'gbdt',\n", "    'objective': 'binary',\n", "    'metric': 'binary_logloss',\n", "    'num_leaves': 31,\n", "    'learning_rate': 0.1,\n", "    'max_depth': -1,\n", "    'verbose': 0,\n", "    'seed': 0,\n", "}"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["* `max_depth: -1` means no limit in `max_depth`\n", "* `verbose: 0` allows you to see every evaluation result. When training for many epochs it might be worth setting this parameter to 5 or 10 rather than 0."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Training the model - I\n", "\n", "This is done by calling the `lgb.train()` function and passing it the parameters, the training set and a validation set.\n", "\n", "We will validate on the training set. By default it will run for 100 iterations of boosting.\n", "\n", "What do you observe as the model trains?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["model = lgb.train(params=params,\n", "                  train_set=lgb_train,\n", "                  valid_sets=[lgb_train],\n", "                 )\n", "\n", "# Your thoughts below...\n", "\n", "print(\"The logloss decreases over the 100 iterations by around 75%. We still don't know how the model performs in terms of the actual classification task though!\")\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Training the model - II\n", "\n", "`lightgbm` makes it easy to test on the unseen data at each iteration. Just add the `lgb_test` object to `valid_sets`.\n", "\n", "How does the loss on the unseen data compare?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["model = lgb.train(params=params,\n", "                  train_set=lgb_train,\n", "                  valid_sets=[lgb_train, lgb_test],\n", "                  valid_names=['seen', '\\tunseen']\n", "                 )\n", "\n", "# Your thoughts below...\n", "\n", "print(\"Log loss is around half as much on the unseen data, eventually. Around twice that of the seen data.\")\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Training the model - III\n", "\n", "Above we only used a measure of loss to evaluate the model. In classification tasks, we normally want to know the accuracy too.\n", "\n", "Update `params` so that `metric` is a list that also includes `\"auc\"` and `\"average_precision\"`.\n", "\n", "Run the model for 200 iterations by changing the value of `num_boost_round` when calling `lgb.train()`. Also, set `evals_result=metrics` - this will store the results of the training, for us to look at later on.\n", "\n", "What do you observe this time?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# A dictionary to store the training results...\n", "metrics = {}\n", "\n", "# Your code and thoughts below...\n", "\n", "params = {\n", "    'boosting_type': 'gbdt',\n", "    'objective': 'binary',\n", "    'metric': ['binary_logloss', 'auc', 'average_precision'],\n", "    'num_leaves': 31,\n", "    'learning_rate': 0.1,\n", "    'max_depth':-1,\n", "    'verbose': 0,\n", "    'seed': 0,\n", "}\n", "\n", "model = lgb.train(params=params,\n", "                  train_set=lgb_train,\n", "                  valid_sets=[lgb_train, lgb_test],\n", "                  valid_names=['seen', '\\tunseen'],\n", "                  num_boost_round=200,\n", "                  evals_result=metrics\n", "                 )\n", "print('='*150)\n", "print('Doubling the number of boosting rounds results in very low loss and very high metric scores for AUC/AP. It is very likely the model has overfitted to the training set!')\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Training the model - IV\n", "\n", "Run the cell below to plot the data you stored in `metrics`. It loops through the individual metrics and plots them at each iteration.\n", "\n", "What do you observe?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "\n", "f, a = plt.subplots(1,3, figsize=(16,5))\n", "\n", "for e, m in enumerate(['binary_logloss', 'auc', 'average_precision']):\n", "    lgb.plot_metric(metrics, metric=m, ax=a[e], title=f\"{m} during training\")\n", "    \n", "    \n", "plt.tight_layout()\n", "# Your thoughts below...\n", "\n", "print(\"The model is overfitting quite quickly - 200 iterations is way too much. Peak unseen performance is likely around 25 iterations.\")\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Investigating feature performance\n", "\n", "`lightgbm` has built-in methods for seeing which features were most predictive.\n", "\n", "There are two ways of measuring feature importance: \n", "\n", "* `split`: how many times a feature is used to make a decision in all the trees in the ensemble, except for leaf nodes. A feature which is used often to make decisions along the way is likely very informative.\n", "* `gain`: the sum of the information gain score from using a feature in a tree.\n", "\n", "These will each be normalised by the number of trees in the ensemble.\n", "\n", "A trained `lightgbm` model has a very handy `.plot_importance()` method. Before running the cells below, what do you predict the most/least useful features will be when trying to predict whether a visitor to a website will make a purchase?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["for imp_type in ['split', 'gain']:\n", "    ax = lgb.plot_importance(model,\n", "                             importance_type=imp_type,\n", "                             dpi=100,\n", "                             title=f\"Feature importance by {imp_type}\"\n", "                             )\n", "# Your thoughts below...\n", "\n", "print(\"Unsurprisingly, features which relate to human activity (duration spent on pages etc.) are more informative than which operating system or browser is being used.\")\n", "print(\"The feature related to how close the date is to a special day of the year was not very useful, which may be a surprise if you thought people would buy more around holidays etc.\")\n", "print(\"This information could help you decide which kind of data to focus on gathering in the future.\")\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Getting predictions for new data\n", "\n", "So far we have used only the data available, but in practice it is likely that more data will come in and you will want to evaluate that.\n", "\n", "In `lightgbm` you can save/load trained models and continue training them, if you have more labeled data.\n", "\n", "If you want predictions for unlabelled data, you can do that too. You can pass a DataFrame of just the `X` values.\n", "\n", "The model's `.predict()` method will return a score for each item. If it is >= 0.5 then it is in the positive class, otherwise the negative.\n", "\n", "The two examples below would be classified as leading to a sale."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["new_data_points = [\n", "    [2.7, 99, 0.2, 30.0, 60, 12445.0, 0.0, 0.0, 20.5, 0.8, 'Feb', 1, 3, 2, 2, 'Returning_Visitor', True],\n", "    [1.2, 99, 0.2, 1.0, 20, 942.0, 0.0, 0.0, 1.5, 0.28, 'Mar', 1, 1, 1, 1, 'Returning_Visitor', False],\n", "]\n", "\n", "new_data_points = pd.DataFrame(new_data_points, columns=data.columns[0:-1])\n", "\n", "new_data_points[['Month', 'VisitorType', 'OperatingSystems', 'Browser', 'Region', 'TrafficType']] = new_data_points[['Month', 'VisitorType', 'OperatingSystems', 'Browser', 'Region', 'TrafficType']].astype('category')\n", "\n", "model.predict(new_data_points)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Finding the best model hyperparameters\n", "\n", "As we saw, there are many hyperparameters to be tweaked - see [the documentation](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html) for more details.\n", "\n", "One method is to try each combination manually, but this is time-consuming.\n", "\n", "Instead, `lightgbm` can be used with `sklearn`'s  cross-validation methods. It will train multiple models on subsets of the data (and evaluate on the unseen remainder) for each combination of parameters and find the best combination.\n", "\n", "The model we trained had around 75% accuracy in the best case. `lightgbm` makes the following suggestions for improving accuracy:\n", "\n", "* Use large `max_bin` (may be slower)\n", "* Use small `learning_rate` with large `n_estimators`\n", "* Use large `num_leaves` (may cause over-fitting)\n", "* Try `dart` rather than `gbdt` for the boosting method\n", "\n", "We can try a few of these and some others.\n", "\n", "The `GridSearchCV` class from `sklearn` takes a model, with a dictionary of hyperparameter and values. Then you just fit/train it as usual, using the training dataset we created at the beginning, `X_train` and `y_train`.\n", "\n", "We use `lgb.LGBMClassifier()` as our model here, we assign it to the variable `classifier`.\n", "\n", "Below, create a `GridSearchCV` in the same way you would an `sklearn` model: assign it to a variable named `gcv`, pass it the `classifier` as your basic model without parameters set, and also pass it `params`.\n", "\n", "To speed things up, set `n_jobs=-1` to use all available CPU cores. Set `verbose=1` so you get updates as it proceeds - useful for making sure it is actually working!\n", "\n", "This is around 480 models, which should take approximately 2 minutes on a four core machine. If it is going too slowly for you, try trimming a few of the items from each of the params. Alternatively you can include a wider range of items by uncommenting some of the values below, this will take longer but will result in a higher accuracy. Remember you can interrupt and restart the kernel if it is taking too long."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.model_selection import GridSearchCV\n", "\n", "params = dict(\n", "    max_bin=[155, 255],#,355,455],\n", "    learning_rate=[0.001, 0.01],# 0.05, 0.1],\n", "    n_estimators=[20, 50],# 80, 100, 120, 150, 200],\n", "    num_leaves=[10, 20],#30, 40, 50],\n", "    boosting=['dart', 'gbdt'],\n", "    max_depth=[-1, 1, 2],#, 3, 4, 5],    \n", "    seed=[0],        \n", ")\n", "\n", "\n", "classifier = lgb.LGBMClassifier()\n", "\n", "# Your code below...\n", "\n", "gcv = GridSearchCV(estimator=classifier, param_grid=params, n_jobs=-1, verbose=1)\n", "\n", "gcv.fit(X_train, y_train)\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# What was the best model?\n", "\n", "`GridSearchCV` evaluated each possible model using the accuracy metric.\n", "\n", "The best model is stored inside `gcv` as `best_estimator_`. Its score is in `gcv.best_score_` and the actual hyperparameters used are in `gcv.best_params_`.\n", "\n", "(The score here is not the score on the training set, but the average score across subsets of the training set.)\n", "\n", "Take a look at these and then evaluate the best model on the test set `X_test` and `y_test` - the model in `gcv.best_estimator_` has a `.score()` method you can use.\n", "\n", "How does it compare to the model you trained before?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code and thoughts below...\n", "\n", "print(f\"Best model accuracy: {gcv.best_score_}\")\n", "\n", "print(f\"Best hyperparameters: {gcv.best_params_}\")\n", "\n", "best_model = gcv.best_estimator_\n", "\n", "print(f\"Accuracy on unseen data: {best_model.score(X_test, y_test)}\")\n", "\n", "print(\"Accuracy has gone up by around 9 points, to just over 84%.\")\n", "print(\"As per the documentation's recommendations, using a different boosting algorithm did help.\")\n", "print(\"In fact, further fine-tuning of the parameters with this boosting algorithm will yield an accuracy of just over 90%.\")\n", "print(\"The best model actually used fewer leaves (20) than the default (31) so it is a way to reduce overfitting (in line with the documentation warning).\")\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# A closer look at the best model - I\n", "\n", "The feature importances can be found in the `.feature_importances_` attribute of the model - check the `.best_model.importance_type` attribute to see if the model used `split` or `gain` for importances.\n", "\n", "The feature names are stored in `.feature_name_`.\n", "\n", "Create a DataFrame from the two lists (importances and names) and see which features are most important for this model.\n", "\n", "Have they changed with these new hyperparameters?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["best_model.importance_type"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code and thoughts below...\n", "\n", "f_imp = pd.DataFrame(dict(feature=best_model.feature_name_, importance=best_model.feature_importances_))\n", "\n", "f_imp.sort_values('importance', ascending=False)\n", "\n", "print('The feature importances are mostly the same.')\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# A closer look at the best model - II\n", "\n", "We used accuracy so far to evaluate performance, but it would be better to know the per-class performance.\n", "\n", "Get the best model's predictions for `X_test` using its `.predict()` method.\n", "\n", "Then, compare these to the true `y_test` classes using `sklearn.metrics.classification_report`.\n", "\n", "What do you observe?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import classification_report\n", "\n", "# Your code and thoughts below...\n", "\n", "best_model_pred = best_model.predict(X_test)\n", "\n", "print(classification_report(y_test, best_model_pred))\n", "\n", "print('='*53)\n", "\n", "print(\"The classes are not very balanced - far more people do not buy than do buy something.\")\n", "print(\"Performance on the most common class is very strong.\")\n", "print(\"The rarer class is never predicted by the model, so we might want to apply class weights.\")\n", "print(\"However, it may be that we want to target the negative class anyway - these are potential customers that could be converted into purchasers.\")\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Conclusion and next steps\n", "\n", "In this practical we went through the process of using `lightgbm` to classify customers, based on their behaviour on a website.\n", "\n", "We trained and evaluated a single model before fine-tuning the hyperparameters to increase performance by a large margin.\n", "\n", "We focused on classification but `lightgbm` also does regression. To take this practical further, you could try applying what you have learned to a regression dataset - see [the UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets.php) for possible datasets.\n", "\n", "Look into the `lgb.LGBMRegressor()` class, which is an `sklearn`-style class. Or use the `lgb.train()` method and change the `params` dictionary so that `objective` and `metric` are suitable for regression.\n", "\n", "Another thing to bear in mind is that `lightgbm` is very fast - we trained over 33k models on around 10k items in around 6 minutes. Try it on some big datasets."]}]}